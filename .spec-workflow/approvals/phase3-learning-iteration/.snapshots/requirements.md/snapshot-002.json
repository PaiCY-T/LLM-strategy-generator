{
  "id": "snapshot_1761865069202_fahxe2iqr",
  "approvalId": "approval_1761864946193_k3pmxkdyv",
  "approvalTitle": "Phase 3 Learning Iteration - Requirements Document",
  "version": 2,
  "timestamp": "2025-10-30T22:57:49.202Z",
  "trigger": "approved",
  "status": "pending",
  "content": "# Requirements Document\n\n## Introduction\n\nThe Phase 3 Learning Iteration feature implements a true **iterative learning system** where AI-generated strategies learn from previous execution results. Unlike Phase 1 (static validation) and Phase 2 (execution testing), Phase 3 enables the system to **improve strategies over multiple iterations** by feeding backtest performance metrics back to the LLM as learning signals.\n\n**Purpose**: Enable the autonomous learning loop to generate progressively better trading strategies by learning from execution history, using real LLM APIs to incorporate feedback from Sharpe ratios, returns, and failure patterns.\n\n**Value**: Transforms the system from a one-shot generator to an adaptive learning system that can discover high-performing strategies through iterative refinement, validating the core hypothesis that LLM-based evolution can improve quantitative trading strategies.\n\n## Alignment with Product Vision\n\nThis feature directly implements the **iterative learning system vision**:\n\n1. **Autonomous Evolution**: LLM learns from previous iterations without human intervention\n2. **Performance-Driven**: Sharpe ratio and returns guide strategy improvements\n3. **Failure Learning**: Execution errors inform constraint refinement\n4. **Champion Tracking**: Best strategies serve as reference points for future iterations\n5. **Scalability**: System can run hundreds of iterations to find optimal strategies\n\nThis is the **critical capability** that distinguishes this system from static strategy generators.\n\n## Requirements\n\n### Requirement 1: Iteration History Management\n\n**User Story:** As the learning system, I want to persist and retrieve iteration history, so that LLMs can learn from past successes and failures across multiple runs.\n\n#### Acceptance Criteria\n\n1. WHEN an iteration completes THEN the system SHALL save strategy code, execution result, metrics, and classification level to history\n2. WHEN history is saved THEN the system SHALL use JSONL format (one JSON object per line) for efficient append operations\n3. WHEN retrieving history THEN the system SHALL load the N most recent successful iterations (configurable, default N=5)\n4. WHEN loading history THEN the system SHALL include champion strategy prominently if it exists\n5. WHEN history file doesn't exist THEN the system SHALL create it and start with iteration 0\n6. WHEN history is corrupted THEN the system SHALL log error and continue with empty history (degraded mode)\n\n### Requirement 2: Feedback Generation\n\n**User Story:** As the LLM, I want clear, actionable feedback from previous iterations, so that I can understand what worked, what failed, and how to improve.\n\n#### Acceptance Criteria\n\n1. WHEN generating feedback THEN the system SHALL include:\n   - Previous strategy's Sharpe Ratio (if available)\n   - Classification level reached (Level 0/1/2/3)\n   - Execution outcome (success, timeout, error)\n   - Error category and message (if failed)\n2. WHEN strategy succeeded with positive Sharpe THEN feedback SHALL highlight successful patterns (e.g., \"factor combinations\", \"filters used\")\n3. WHEN strategy failed THEN feedback SHALL provide actionable guidance (e.g., \"avoid infinite loops\", \"check dataset availability\")\n4. WHEN multiple iterations exist THEN feedback SHALL summarize trend (e.g., \"improving Sharpe from 0.5 to 0.8\")\n5. WHEN champion exists THEN feedback SHALL reference champion's Sharpe as target to beat\n6. WHEN feedback exceeds 500 words THEN the system SHALL summarize to prevent prompt overflow\n\n### Requirement 3: LLM Integration for Learning\n\n**User Story:** As the system operator, I want the learning loop to use real LLM APIs (Google AI, OpenRouter), so that genuine learning occurs through natural language feedback.\n\n#### Acceptance Criteria\n\n1. WHEN generating a new strategy THEN the system SHALL call LLM API with iteration history as context\n2. WHEN LLM call succeeds THEN the system SHALL extract Python code from response\n3. WHEN LLM call fails THEN the system SHALL retry up to 3 times with exponential backoff\n4. WHEN all retries fail THEN the system SHALL fall back to Factor Graph mutation\n5. WHEN using Google AI THEN the system SHALL try Gemini first, fall back to OpenRouter if quota exceeded\n6. WHEN LLM returns invalid code THEN the system SHALL classify as generation failure and try again with stronger constraints\n7. WHEN LLM cost tracking is enabled THEN the system SHALL log token usage and estimated cost per iteration\n\n### Requirement 4: Champion Strategy Tracking\n\n**User Story:** As the learning system, I want to track the best-performing strategy (champion), so that I can use it as a reference point and prevent regression.\n\n#### Acceptance Criteria\n\n1. WHEN an iteration achieves higher Sharpe Ratio than current champion THEN the system SHALL update champion\n2. WHEN champion is updated THEN the system SHALL save strategy code, metrics, and timestamp to `champion.json`\n3. WHEN no champion exists THEN the system SHALL use first Level 3 strategy (Sharpe > 0) as initial champion\n4. WHEN generating feedback THEN the system SHALL include champion's Sharpe as improvement target\n5. WHEN champion has been stale for N iterations (default 20) THEN the system SHALL flag for review\n6. WHEN multiple strategies have equal Sharpe THEN the system SHALL prefer the one with lower Max Drawdown\n\n### Requirement 5: Iteration Control and Configuration\n\n**User Story:** As a system operator, I want to configure iteration count, LLM model, and learning parameters, so that I can control execution costs and experiment with different settings.\n\n#### Acceptance Criteria\n\n1. WHEN starting the loop THEN the system SHALL read configuration from YAML file (`config/learning_system.yaml`)\n2. WHEN configuration specifies max iterations THEN the system SHALL stop after reaching that limit\n3. WHEN configuration specifies LLM model THEN the system SHALL use that model for all generations\n4. WHEN configuration enables innovation mode THEN the system SHALL use LLM-based generation (vs pure Factor Graph)\n5. WHEN configuration specifies innovation rate THEN the system SHALL use LLM for X% of iterations, Factor Graph for (100-X)%\n6. WHEN iteration limit is reached THEN the system SHALL save final results and generate summary report\n\n### Requirement 6: Learning Loop Integration\n\n**User Story:** As the autonomous loop, I want to integrate Phase 2 execution with Phase 3 learning seamlessly, so that execution results automatically feed back into strategy generation.\n\n#### Acceptance Criteria\n\n1. WHEN loop starts iteration N THEN the system SHALL:\n   - Load history from previous N-1 iterations\n   - Generate feedback from history\n   - Call LLM or Factor Graph with feedback\n   - Execute generated strategy (Phase 2)\n   - Extract metrics and classify (Phase 2)\n   - Save to history for next iteration\n2. WHEN execution fails THEN the system SHALL continue to next iteration (not crash entire loop)\n3. WHEN generating strategy THEN the system SHALL use timeout to prevent LLM API hangs (default 60 seconds)\n4. WHEN loop completes THEN the system SHALL generate summary with champion, success rates, and Sharpe progression\n5. WHEN loop is interrupted THEN the system SHALL save progress and allow resumption from last completed iteration\n\n## Non-Functional Requirements\n\n### Code Architecture and Modularity\n\n- **Single Responsibility**: Separate history management, feedback generation, LLM integration, and champion tracking\n- **Modular Design**: Create reusable components that work with existing Phase 2 executor\n- **Clear Interfaces**: Define dataclasses for IterationRecord, FeedbackContext, ChampionStrategy\n- **Dependency Management**: Minimize new dependencies, reuse existing LLM client code\n\n### Performance\n\n- **PERF-1**: History loading must complete in <1 second for 1000 iterations\n- **PERF-2**: LLM API call (with retries) must complete within 60 seconds or timeout\n- **PERF-3**: Feedback generation must complete in <2 seconds\n- **PERF-4**: Champion update check must complete in <0.5 seconds\n\n### Reliability\n\n- **REL-1**: History file corruption must not crash the system (degrade gracefully)\n- **REL-2**: LLM API failures must fall back to Factor Graph (no iteration lost)\n- **REL-3**: Iteration failures must not prevent subsequent iterations from running\n- **REL-4**: Loop interruption must save state cleanly for resumption\n\n### Security\n\n- **SEC-1**: LLM API keys must be loaded from environment variables, never hardcoded\n- **SEC-2**: History files must not contain API keys or credentials\n- **SEC-3**: Generated strategies must be validated before execution (Phase 2 validation)\n- **SEC-4**: LLM prompts must not include sensitive user data\n\n### Usability\n\n- **USE-1**: Progress must show iteration number, success rate, and current champion Sharpe\n- **USE-2**: Feedback must be human-readable for debugging (not just machine-optimized)\n- **USE-3**: Configuration errors must provide clear guidance (e.g., \"Invalid model name: gpt-6, use gpt-5 or gemini-2.5-flash\")\n- **USE-4**: Final summary must be actionable (e.g., \"Champion Sharpe: 1.2, recommend production deployment\")\n\n## Success Metrics\n\n**Primary Success Criteria**:\n- Achieve measurable Sharpe Ratio improvement over 20 iterations (e.g., from 0.5 to 1.0+)\n- Maintain â‰¥70% execution success rate (Level 1+) across iterations\n- LLM-based learning outperforms pure Factor Graph mutation (A/B test)\n\n**Secondary Success Criteria**:\n- Champion strategy remains stable for last 10 iterations (convergence)\n- Execution errors decrease over time (learning from failures)\n- Average Sharpe Ratio increases monotonically or with minor fluctuations\n\n**Exit Criteria for Phase 3**:\n- Complete 20+ iteration run successfully\n- Demonstrate clear learning (Sharpe improvement or error reduction)\n- Generate comprehensive results report with champion strategy\n- System ready for 100+ iteration production runs\n\n## Dependencies\n\n**Required Before Phase 3**:\n- Phase 2 backtest execution framework completed\n- `BacktestExecutor`, `MetricsExtractor`, `SuccessClassifier` operational\n- Generated strategies from Phase 1 available (for initial testing)\n\n**External Dependencies**:\n- Google AI API (Gemini) with valid API key\n- OpenRouter API with valid API key (fallback)\n- Finlab session authenticated\n- `artifacts/data/innovations.jsonl` for history storage (will be created)\n- `champion.json` for champion tracking (will be created)\n\n## Risks and Mitigation\n\n**Risk 1**: LLM generates strategies that always fail\n- **Mitigation**: Strong prompt engineering with examples, fallback to Factor Graph\n\n**Risk 2**: LLM API costs exceed budget\n- **Mitigation**: Token usage tracking, configurable innovation rate, model selection\n\n**Risk 3**: No learning observed (Sharpe stays flat)\n- **Mitigation**: Increase iteration count to 50+, try stronger models (GPT-5, Claude Opus)\n\n**Risk 4**: History file grows too large (>100MB)\n- **Mitigation**: Implement history rotation (keep last 1000 iterations), compress old data\n\n## Future Enhancements (Out of Scope for Phase 3)\n\n- Multi-objective optimization (Sharpe + Drawdown + Win Rate)\n- Ensemble strategies (combine top 5 champions)\n- Transfer learning (use champion from one market as seed for another)\n- Real-time learning dashboard with Sharpe progression charts\n- Automated hyperparameter tuning for LLM prompts\n",
  "fileStats": {
    "size": 11403,
    "lines": 203,
    "lastModified": "2025-10-30T22:55:38.449Z"
  },
  "comments": []
}