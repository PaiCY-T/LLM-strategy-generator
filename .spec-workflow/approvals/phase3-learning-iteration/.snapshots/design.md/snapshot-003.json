{
  "id": "snapshot_1761866131257_3vqi9zhcd",
  "approvalId": "approval_1761865890440_qleqcczlg",
  "approvalTitle": "Phase 3 Learning Iteration - Design Document (Revised)",
  "version": 3,
  "timestamp": "2025-10-30T23:15:31.257Z",
  "trigger": "approved",
  "status": "pending",
  "content": "# Design Document\n\n## Overview\n\nPhase 3 Learning Iteration implements the **autonomous learning loop** that enables AI-generated strategies to learn from execution results. The system creates a feedback cycle: execute strategy → extract metrics → generate feedback → call LLM with feedback → generate improved strategy.\n\n**Core Innovation**: Unlike static generators, this system uses **real LLM APIs** (Gemini, OpenRouter) with natural language feedback from previous iterations, enabling genuine learning through iteration history.\n\n**Architecture Pattern**: Pipeline with persistent state (JSONL history file + champion.json)\n\n```\nIteration N:\n  Load History → Generate Feedback → Call LLM → Execute Strategy → Extract Metrics → Update Champion → Save to History\n                                                    ↓\n                                              (Phase 2 Executor)\n```\n\n## Steering Document Alignment\n\n### Technical Standards (tech.md)\n\n*Following Python best practices:*\n- **Modular Design**: Separate history, feedback, LLM client, champion tracking, main loop\n- **Type Safety**: Python dataclasses with type hints for all data structures\n- **Error Handling**: Graceful degradation (LLM fail → Factor Graph, corrupt history → empty start)\n- **Logging**: Structured logging for debugging and monitoring\n\n### Project Structure (structure.md)\n\n*Following existing patterns:*\n- Core learning logic in `artifacts/working/modules/` (alongside existing autonomous_loop.py)\n- History files in `artifacts/data/innovations.jsonl`\n- Champion tracking in project root `champion.json`\n- Configuration in `config/learning_system.yaml`\n\n## Code Reuse Analysis\n\n### Existing Components to Leverage\n\n- **`artifacts/working/modules/autonomous_loop.py`**: Current monolithic loop - **WILL BE REFACTORED** into smaller modules (see Component 5)\n- **`artifacts/working/modules/poc_claude_test.py`**: LLM API client (Gemini + OpenRouter) - reuse directly\n- **`artifacts/working/modules/history.py`**: Basic history structure - extend for full functionality\n- **Phase 2 Components**: `BacktestExecutor`, `MetricsExtractor`, `SuccessClassifier` - integrate directly\n- **`src/backtest/metrics.py`**: Sharpe extraction - already integrated in Phase 2\n\n### Refactoring Strategy for autonomous_loop.py\n\n**Current Problem**: `autonomous_loop.py` has become a monolithic file with ~2000+ lines, mixing concerns (orchestration, LLM calls, metrics, logging, monitoring).\n\n**Refactoring Plan**:\n1. **Extract** iteration logic into `iteration_executor.py` (new)\n2. **Extract** feedback generation into `feedback_generator.py` (new)\n3. **Extract** champion tracking into `champion_tracker.py` (new)\n4. **Extract** LLM client wrapper into `llm_client.py` (new)\n5. **Keep** `autonomous_loop.py` as lightweight entry point (~200 lines) that:\n   - Loads configuration\n   - Initializes all components\n   - Runs main loop calling other components\n   - Generates final summary\n\n**Benefits**:\n- Each file <300 lines, single responsibility\n- Easier testing (mock individual components)\n- Clearer separation of concerns\n- Maintainable and extensible\n\n### Integration Points\n\n- **Phase 2 Executor**: Call `BacktestExecutor.execute_strategy()` for each iteration\n- **History Storage**: JSONL format (one JSON per line) in `artifacts/data/innovations.jsonl`\n- **LLM APIs**: Google AI (primary), OpenRouter (fallback) via `poc_claude_test.generate_strategy()`\n- **Configuration**: YAML config loading via existing patterns\n\n## Architecture\n\nThe system follows a **stateful pipeline architecture** with 5 core components:\n\n```mermaid\ngraph TB\n    A[LearningLoop] --> B[IterationHistory]\n    A --> C[FeedbackGenerator]\n    A --> D[LLMClient]\n    A --> E[ChampionTracker]\n    A --> F[BacktestExecutor Phase 2]\n\n    B --> G[innovations.jsonl]\n    E --> H[champion.json]\n    D --> I[Google AI API]\n    D --> J[OpenRouter API]\n    F --> K[MetricsExtractor]\n    K --> E\n```\n\n### Modular Design Principles\n\n- **Single File Responsibility**: Each component in separate file\n- **Component Isolation**: IterationHistory knows nothing about LLM, FeedbackGenerator knows nothing about execution\n- **Service Layer Separation**:\n  - **Storage layer**: IterationHistory (JSONL read/write)\n  - **Intelligence layer**: FeedbackGenerator, LLMClient\n  - **Control layer**: LearningLoop (orchestration)\n  - **Execution layer**: Phase 2 components (already built)\n- **Utility Modularity**: Share retry logic, timeout handling as utilities\n\n## Components and Interfaces\n\n### Component 1: IterationHistory\n- **Purpose:** Persist and retrieve iteration records from JSONL file\n- **File:** `artifacts/working/modules/history.py` (extend existing)\n- **Interfaces:**\n  ```python\n  @dataclass\n  class IterationRecord:\n      iteration: int\n      strategy_code: str\n      execution_success: bool\n      sharpe_ratio: Optional[float]\n      total_return: Optional[float]\n      max_drawdown: Optional[float]\n      classification_level: int  # 0-3\n      error_message: Optional[str]\n      error_category: Optional[str]\n      execution_time: float\n      timestamp: str\n      model_used: str\n\n  class IterationHistory:\n      def __init__(history_file: str = \"artifacts/data/innovations.jsonl\"):\n          \"\"\"Initialize with history file path\"\"\"\n\n      def load_recent(n: int = 5) -> List[IterationRecord]:\n          \"\"\"Load N most recent iterations\"\"\"\n\n      def save_iteration(record: IterationRecord) -> None:\n          \"\"\"Append iteration to JSONL file\"\"\"\n\n      def get_all() -> List[IterationRecord]:\n          \"\"\"Load all iterations (use sparingly, can be slow)\"\"\"\n\n      def get_successful() -> List[IterationRecord]:\n          \"\"\"Get all iterations with classification_level >= 1\"\"\"\n\n      def get_best_sharpe() -> Optional[IterationRecord]:\n          \"\"\"Get iteration with highest Sharpe Ratio\"\"\"\n  ```\n- **Dependencies:**\n  - Python `json` module\n  - File I/O\n- **Reuses:** Existing `history.py` structure\n- **Error Handling:** Corrupted JSONL → skip bad lines, log warning, continue\n\n### Component 2: FeedbackGenerator\n- **Purpose:** Generate natural language feedback from iteration history for LLM\n- **File:** `artifacts/working/modules/feedback_generator.py` (new)\n- **Interfaces:**\n  ```python\n  @dataclass\n  class FeedbackContext:\n      recent_iterations: List[IterationRecord]\n      champion_sharpe: Optional[float]\n      current_iteration: int\n      total_iterations: int\n\n  class FeedbackGenerator:\n      def generate_feedback(context: FeedbackContext) -> str:\n          \"\"\"Generate feedback text for LLM (max 500 words)\"\"\"\n\n      def _summarize_success(record: IterationRecord) -> str:\n          \"\"\"Summarize successful iteration\"\"\"\n\n      def _summarize_failure(record: IterationRecord) -> str:\n          \"\"\"Summarize failed iteration with actionable guidance\"\"\"\n\n      def _generate_trend_analysis(iterations: List[IterationRecord]) -> str:\n          \"\"\"Analyze Sharpe trend over iterations\"\"\"\n  ```\n- **Dependencies:**\n  - IterationRecord dataclass\n  - String formatting\n- **Reuses:** None (new component)\n- **Feedback Format:**\n  ```\n  Iteration 5/20\n\n  Recent Performance:\n  - Iteration 4: Sharpe 0.8, Level 3 ✓ (improved from 0.5)\n  - Iteration 3: Failed (timeout) - avoid infinite loops\n  - Iteration 2: Sharpe 0.5, Level 3 ✓\n\n  Champion: Sharpe 0.8 (Iteration 4)\n  Target: Beat 0.8 to become new champion\n\n  Recommendations:\n  - Continue using adjusted data (etl:adj_close)\n  - Factor combinations in Iter 4 worked well (momentum + quality + value)\n  - Avoid computationally expensive rolling operations\n  ```\n\n### Component 3: LLMClient (Wrapper)\n- **Purpose:** Unified interface for LLM API calls with retry and fallback\n- **File:** `artifacts/working/modules/llm_client.py` (new wrapper)\n- **Interfaces:**\n  ```python\n  @dataclass\n  class LLMRequest:\n      iteration_num: int\n      feedback: str\n      model: str  # 'gemini-2.5-flash', 'gpt-5', etc.\n      timeout: int = 60\n\n  @dataclass\n  class LLMResponse:\n      success: bool\n      strategy_code: str\n      error: Optional[str]\n      tokens_used: Optional[int]\n      cost_estimate: Optional[float]\n      generation_time: float\n\n  class LLMClient:\n      def generate_strategy(request: LLMRequest) -> LLMResponse:\n          \"\"\"Call LLM API with retry and timeout\"\"\"\n\n      def _call_with_retry(request: LLMRequest, max_retries: int = 3) -> LLMResponse:\n          \"\"\"Retry with exponential backoff\"\"\"\n\n      def _extract_code(llm_response: str) -> str:\n          \"\"\"Extract Python code from LLM response\"\"\"\n  ```\n- **Dependencies:**\n  - **Reuses:** `poc_claude_test.generate_strategy()` - wraps it with retry logic\n  - Google AI SDK, OpenRouter SDK\n  - `tenacity` for retry (or implement manual exponential backoff)\n- **Fallback Strategy:**\n  1. Try Google AI (Gemini)\n  2. If quota exceeded → OpenRouter with same model\n  3. If all fail → return error, caller falls back to Factor Graph\n\n### Component 4: ChampionTracker\n- **Purpose:** Track best-performing strategy and detect staleness\n- **File:** `artifacts/working/modules/champion_tracker.py` (new)\n- **Interfaces:**\n  ```python\n  @dataclass\n  class ChampionStrategy:\n      iteration: int\n      strategy_code: str\n      sharpe_ratio: float\n      total_return: float\n      max_drawdown: float\n      timestamp: str\n      iterations_since_update: int\n\n  class ChampionTracker:\n      def __init__(champion_file: str = \"champion.json\"):\n          \"\"\"Initialize with champion file path\"\"\"\n\n      def load_champion() -> Optional[ChampionStrategy]:\n          \"\"\"Load current champion from JSON file\"\"\"\n\n      def update_champion(iteration_record: IterationRecord) -> bool:\n          \"\"\"Update champion if Sharpe is better, return True if updated\"\"\"\n\n      def is_stale(threshold: int = 20) -> bool:\n          \"\"\"Check if champion hasn't been updated for N iterations\"\"\"\n\n      def get_sharpe_target() -> float:\n          \"\"\"Get champion Sharpe as target to beat\"\"\"\n  ```\n- **Dependencies:**\n  - Python `json` module\n  - File I/O\n- **Reuses:** None (new component)\n- **Update Logic:**\n  - If `new_sharpe > champion_sharpe`: update\n  - If `new_sharpe == champion_sharpe` and `new_drawdown < champion_drawdown`: update\n  - Increment `iterations_since_update` on each iteration\n\n### Component 5: LearningLoop (Lightweight Main Controller)\n- **Purpose:** Lightweight orchestrator that delegates to specialized components\n- **File:** `artifacts/working/modules/autonomous_loop.py` (**REFACTORED** from monolithic to thin controller)\n- **Size:** Target ~200 lines (down from 2000+)\n- **Interfaces:**\n  ```python\n  @dataclass\n  class LoopConfig:\n      max_iterations: int\n      model: str\n      innovation_rate: float  # 0.0-1.0, % of iterations using LLM\n      history_file: str\n      champion_file: str\n      config_file: str\n\n  class LearningLoop:\n      \"\"\"Lightweight orchestrator - delegates to specialized components\"\"\"\n\n      def __init__(config: LoopConfig):\n          \"\"\"Initialize by loading config and creating component instances\"\"\"\n          self.config = config\n          # Initialize components (all in separate files):\n          self.history = IterationHistory(config.history_file)\n          self.feedback_gen = FeedbackGenerator()\n          self.llm_client = LLMClient()\n          self.champion = ChampionTracker(config.champion_file)\n          self.iteration_executor = IterationExecutor()  # NEW component\n\n      def run() -> LoopSummary:\n          \"\"\"Execute full learning loop - simple orchestration logic\"\"\"\n          for i in range(self.config.max_iterations):\n              record = self.iteration_executor.execute_iteration(\n                  iteration_num=i,\n                  history=self.history,\n                  feedback_gen=self.feedback_gen,\n                  llm_client=self.llm_client,\n                  champion=self.champion,\n                  config=self.config\n              )\n              self.history.save_iteration(record)\n          return self.generate_summary()\n\n      def generate_summary() -> LoopSummary:\n          \"\"\"Generate final summary from history and champion\"\"\"\n  ```\n- **Dependencies:**\n  - All Phase 3 components (history, feedback, llm, champion, iteration_executor)\n  - Configuration loading (YAML)\n- **Refactoring Notes:**\n  - Move all iteration logic to `IterationExecutor` (new Component 6)\n  - Move all LLM-specific logic to `LLMClient` (Component 3)\n  - Move all feedback logic to `FeedbackGenerator` (Component 2)\n  - Keep only: config loading, component initialization, main loop, summary generation\n\n### Component 6: IterationExecutor (Extracted from autonomous_loop.py)\n- **Purpose:** Execute a single iteration (was inside autonomous_loop.py, now extracted)\n- **File:** `artifacts/working/modules/iteration_executor.py` (NEW - extracted from autonomous_loop.py)\n- **Size:** Target ~250 lines\n- **Interfaces:**\n  ```python\n  class IterationExecutor:\n      \"\"\"Executes single iteration - extracted from autonomous_loop.py\"\"\"\n\n      def __init__():\n          # Initialize Phase 2 components:\n          self.backtest_executor = BacktestExecutor()\n          self.metrics_extractor = MetricsExtractor()\n          self.classifier = SuccessClassifier()\n\n      def execute_iteration(\n          iteration_num: int,\n          history: IterationHistory,\n          feedback_gen: FeedbackGenerator,\n          llm_client: LLMClient,\n          champion: ChampionTracker,\n          config: LoopConfig\n      ) -> IterationRecord:\n          \"\"\"Execute single iteration - 10-step process\"\"\"\n          # 1. Load recent history\n          # 2. Generate feedback\n          # 3. Decide: LLM or Factor Graph\n          # 4. Generate strategy (call LLM or Factor Graph)\n          # 5. Execute strategy (Phase 2)\n          # 6. Extract metrics (Phase 2)\n          # 7. Classify success level (Phase 2)\n          # 8. Update champion if better\n          # 9. Create IterationRecord\n          # 10. Return record (caller saves to history)\n  ```\n- **Dependencies:**\n  - Phase 2 components (executor, metrics, classifier)\n  - Phase 3 components (passed as parameters)\n- **Purpose of Extraction:** Separate iteration logic from loop orchestration, enable easier testing\n\n## Data Models\n\n### IterationRecord\n```python\n@dataclass\nclass IterationRecord:\n    iteration: int                    # Iteration number (0-indexed)\n    strategy_code: str                # Generated Python code\n    execution_success: bool           # Did execution complete?\n    sharpe_ratio: Optional[float]     # None if execution failed\n    total_return: Optional[float]     # None if execution failed\n    max_drawdown: Optional[float]     # None if execution failed\n    classification_level: int         # 0-3 (Phase 2 classification)\n    error_message: Optional[str]      # If execution failed\n    error_category: Optional[str]     # timeout | data_missing | calculation | other\n    execution_time: float             # Time in seconds\n    timestamp: str                    # ISO 8601 format\n    model_used: str                   # 'gemini-2.5-flash' | 'factor_graph' | etc.\n```\n\n### ChampionStrategy\n```python\n@dataclass\nclass ChampionStrategy:\n    iteration: int                    # Which iteration became champion\n    strategy_code: str                # Full Python code\n    sharpe_ratio: float               # Champion's Sharpe\n    total_return: float               # Champion's return %\n    max_drawdown: float               # Champion's drawdown %\n    timestamp: str                    # When it became champion\n    iterations_since_update: int      # How long it's been champion (staleness)\n```\n\n### FeedbackContext\n```python\n@dataclass\nclass FeedbackContext:\n    recent_iterations: List[IterationRecord]  # Last 5 iterations\n    champion_sharpe: Optional[float]          # Current champion's Sharpe\n    current_iteration: int                    # Current iteration number (e.g., 5)\n    total_iterations: int                     # Total planned (e.g., 20)\n```\n\n### LoopSummary\n```python\n@dataclass\nclass LoopSummary:\n    total_iterations: int                     # Total executed\n    successful_iterations: int                # Level 1+ count\n    level_3_count: int                        # Positive Sharpe count\n    champion: Optional[ChampionStrategy]      # Best strategy\n    avg_sharpe: float                         # Average of successful iterations\n    sharpe_progression: List[float]           # Sharpe over time (for plotting)\n    execution_success_rate: float             # % that executed successfully\n    llm_usage_count: int                      # How many used LLM vs Factor Graph\n    total_execution_time: float               # Total time in seconds\n    recommendation: str                       # \"Ready for production\" | \"Needs more iterations\" | etc.\n```\n\n## Error Handling\n\n### Error Scenarios\n\n1. **LLM API Timeout (60 seconds exceeded)**\n   - **Handling:**\n     - Set timeout on API call using `requests.timeout` or `asyncio.wait_for`\n     - Catch `TimeoutError` exception\n     - Fall back to Factor Graph mutation\n   - **User Impact:** Iteration uses Factor Graph instead of LLM, logged as fallback\n\n2. **LLM API Quota Exceeded**\n   - **Handling:**\n     - Catch quota error from Google AI (`ResourceExhausted`)\n     - Automatically retry with OpenRouter\n     - If OpenRouter also fails, fall back to Factor Graph\n   - **User Impact:** Seamless fallback, logged as \"Google AI quota exceeded, used OpenRouter\"\n\n3. **History File Corrupted**\n   - **Handling:**\n     - Try to load JSONL line by line\n     - Skip malformed lines, log warning\n     - If file completely unreadable, start with empty history\n   - **User Impact:** Some history lost but system continues (degraded mode)\n\n4. **Champion File Missing or Corrupt**\n   - **Handling:**\n     - If file doesn't exist, create new champion from first Level 3 strategy\n     - If file corrupted, log error and use no champion (start fresh)\n   - **User Impact:** No champion reference for feedback, but learning continues\n\n5. **LLM Returns Invalid Code**\n   - **Handling:**\n     - Try to extract code from response (regex for ```python blocks)\n     - Run static validation (Phase 1 validator)\n     - If invalid, log error and retry with stronger prompt constraints\n     - After 3 retries, fall back to Factor Graph\n   - **User Impact:** Wasted LLM calls, but iteration still completes with Factor Graph\n\n6. **Execution Failure (from Phase 2)**\n   - **Handling:**\n     - Already handled by Phase 2 executor\n     - Save failure to history with error details\n     - Continue to next iteration\n   - **User Impact:** Iteration marked as failed, feedback generated for next iteration to avoid same error\n\n## Testing Strategy\n\n### Unit Testing\n\n**File:** `tests/learning/test_iteration_history.py`\n- Test JSONL save and load\n- Test handling of corrupted lines\n- Test recent iterations filtering\n\n**File:** `tests/learning/test_feedback_generator.py`\n- Test feedback generation with various scenarios (all success, all failure, mixed)\n- Test trend analysis\n- Test word limit (max 500 words)\n\n**File:** `tests/learning/test_champion_tracker.py`\n- Test champion update logic (better Sharpe, equal Sharpe with better drawdown)\n- Test staleness detection\n- Test JSON save/load\n\n**File:** `tests/learning/test_llm_client.py`\n- Mock LLM API calls\n- Test retry logic with exponential backoff\n- Test fallback from Google AI to OpenRouter\n- Test code extraction from various response formats\n\n### Integration Testing\n\n**File:** `tests/integration/test_learning_loop.py`\n- Test 3-iteration learning loop with mock components\n- Verify history saves correctly after each iteration\n- Verify champion updates when better Sharpe found\n- Test graceful degradation (LLM fail → Factor Graph)\n\n### End-to-End Testing\n\n**Test Scenario 1: 5-Iteration Learning Test**\n- Run 5 real iterations with Gemini API\n- Verify Sharpe improvement or at least execution success\n- Check champion tracking works\n- Validate all components integrate correctly\n\n**Test Scenario 2: LLM Fallback Test**\n- Mock Google AI to fail (quota exceeded)\n- Verify OpenRouter is called\n- Mock both to fail\n- Verify Factor Graph fallback works\n\n## Implementation Notes\n\n### Configuration File (config/learning_system.yaml)\n\n```yaml\nlearning_loop:\n  max_iterations: 20\n  model: \"gemini-2.5-flash\"  # or \"gpt-5\", \"anthropic/claude-opus-4\", etc.\n  innovation_rate: 0.8  # 80% LLM, 20% Factor Graph\n  history_file: \"artifacts/data/innovations.jsonl\"\n  champion_file: \"champion.json\"\n\nllm:\n  timeout_seconds: 60\n  max_retries: 3\n  retry_delay_base: 2  # Exponential backoff base (2, 4, 8 seconds)\n\nexecution:\n  timeout_seconds: 420  # 7 minutes per strategy (Phase 2)\n\nfeedback:\n  max_recent_iterations: 5\n  max_words: 500\n  include_code_snippets: false  # Too verbose for prompt\n\nchampion:\n  staleness_threshold: 20  # Flag if no update for 20 iterations\n```\n\n### LLM Prompt Structure\n\nThe feedback will be injected into LLM prompt:\n\n```\nYou are an expert quantitative trading strategy developer.\n\nPrevious Iteration Results:\n{feedback_text}\n\nTask: Generate an improved trading strategy that:\n1. Uses adjusted price data (etl:adj_close) NOT raw data\n2. Aims to achieve Sharpe Ratio > {champion_sharpe}\n3. Learns from previous failures (avoid timeouts, missing data)\n4. Executes within 7 minutes\n\nGenerate only Python code, no explanations.\n```\n\n### Iteration Loop Pseudocode\n\n```python\ndef run_iteration(iteration_num):\n    # 1. Load history\n    recent = history.load_recent(n=5)\n\n    # 2. Generate feedback\n    feedback_ctx = FeedbackContext(\n        recent_iterations=recent,\n        champion_sharpe=champion_tracker.get_sharpe_target(),\n        current_iteration=iteration_num,\n        total_iterations=config.max_iterations\n    )\n    feedback = feedback_gen.generate_feedback(feedback_ctx)\n\n    # 3. Decide: LLM or Factor Graph?\n    use_llm = random.random() < config.innovation_rate\n\n    # 4. Generate strategy\n    if use_llm:\n        llm_request = LLMRequest(iteration_num, feedback, config.model)\n        llm_response = llm_client.generate_strategy(llm_request)\n        if llm_response.success:\n            code = llm_response.strategy_code\n            model_used = config.model\n        else:\n            # Fallback to Factor Graph\n            code = factor_graph.mutate()\n            model_used = \"factor_graph_fallback\"\n    else:\n        code = factor_graph.mutate()\n        model_used = \"factor_graph\"\n\n    # 5-7. Execute, extract metrics, classify (Phase 2)\n    exec_result = executor.execute_strategy(code)\n    metrics = metrics_extractor.extract_metrics(exec_result.report)\n    level = classifier.classify(exec_result, metrics)\n\n    # 8. Update champion\n    if metrics.sharpe_ratio and metrics.sharpe_ratio > champion_tracker.get_sharpe_target():\n        champion_tracker.update_champion(iteration_record)\n\n    # 9. Save to history\n    record = IterationRecord(\n        iteration=iteration_num,\n        strategy_code=code,\n        execution_success=exec_result.success,\n        sharpe_ratio=metrics.sharpe_ratio,\n        ...\n    )\n    history.save_iteration(record)\n\n    return record\n```\n\n### File Organization\n\n```\nfinlab/\n├── artifacts/\n│   ├── data/\n│   │   └── innovations.jsonl                # Iteration history (JSONL)\n│   └── working/\n│       └── modules/\n│           ├── autonomous_loop.py            # LearningLoop (REFACTORED - now ~200 lines, thin orchestrator)\n│           ├── iteration_executor.py         # IterationExecutor (NEW - extracted from autonomous_loop.py)\n│           ├── history.py                    # IterationHistory (extended)\n│           ├── feedback_generator.py         # FeedbackGenerator (new)\n│           ├── llm_client.py                 # LLMClient (new wrapper)\n│           └── champion_tracker.py           # ChampionTracker (new)\n├── config/\n│   └── learning_system.yaml                 # Configuration\n├── champion.json                             # Current champion (generated)\n├── tests/\n│   ├── learning/\n│   │   ├── test_iteration_history.py\n│   │   ├── test_feedback_generator.py\n│   │   ├── test_champion_tracker.py\n│   │   ├── test_llm_client.py\n│   │   └── test_iteration_executor.py       # NEW - test extracted component\n│   └── integration/\n│       └── test_learning_loop.py\n└── run_learning_loop.py                     # Main entry point (new)\n```\n\n**Refactoring Impact**:\n- **Before**: `autonomous_loop.py` ~2000+ lines (monolithic)\n- **After**: 6 focused files, each <300 lines:\n  - `autonomous_loop.py`: ~200 lines (orchestration only)\n  - `iteration_executor.py`: ~250 lines (iteration logic)\n  - `history.py`: ~150 lines (JSONL I/O)\n  - `feedback_generator.py`: ~200 lines (feedback generation)\n  - `llm_client.py`: ~150 lines (LLM wrapper)\n  - `champion_tracker.py`: ~100 lines (champion tracking)\n- **Total**: ~1050 lines across 6 files vs 2000+ in 1 file\n\n## Performance Optimizations\n\n- **History Loading**: Only load recent N iterations (default 5), not entire file\n- **JSONL Format**: Allows O(1) append, efficient for streaming writes\n- **Lazy Champion Load**: Load champion.json only when needed, cache in memory\n- **Feedback Caching**: Cache generated feedback for retry scenarios (same iteration, different LLM call)\n\n## Security Considerations\n\n- **API Keys**: Load from `GOOGLE_API_KEY` and `OPENROUTER_API_KEY` environment variables\n- **Prompt Injection**: Sanitize any user-provided feedback context (though not applicable here)\n- **Code Execution**: Leverage Phase 2 sandboxing (restricted globals in exec())\n- **File Permissions**: Champion.json and innovations.jsonl should be write-protected in production\n\n## Success Criteria\n\n- All 5 components implemented and unit tested\n- Integration test passes with 3 mock iterations\n- 5-iteration real test shows learning (Sharpe improvement or error reduction)\n- Champion tracking works correctly\n- LLM fallback to Factor Graph works seamlessly\n- Configuration can control all key parameters (model, iteration count, innovation rate)\n",
  "fileStats": {
    "size": 26176,
    "lines": 692,
    "lastModified": "2025-10-30T23:10:41.663Z"
  },
  "comments": []
}