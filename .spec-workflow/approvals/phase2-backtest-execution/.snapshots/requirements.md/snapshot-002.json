{
  "id": "snapshot_1761862386515_tm9dl2ca9",
  "approvalId": "approval_1761861672032_by9t5izyj",
  "approvalTitle": "Phase 2 Backtest Execution - Requirements",
  "version": 2,
  "timestamp": "2025-10-30T22:13:06.515Z",
  "trigger": "revision_requested",
  "status": "pending",
  "content": "# Requirements Document\n\n## Introduction\n\nThe Phase 2 Backtest Execution feature validates that AI-generated trading strategies actually execute successfully with real finlab data and produce measurable performance metrics. Currently, we have only validated that strategies pass static code validation (correct dataset keys), but we have NOT tested whether they execute without runtime errors or produce valid backtest results. This feature is critical to determine the real success rate of our strategy generation system and unblock Phase 3 learning capabilities.\n\n**Purpose**: Execute 20 previously generated strategies with real finlab data and measure their success rates across three levels: execution completion, valid metrics extraction, and positive performance.\n\n**Value**: Provides factual evidence of system effectiveness, identifies execution bottlenecks, establishes baseline performance metrics for Phase 3 learning, and validates the end-to-end pipeline.\n\n## Alignment with Product Vision\n\nThis feature directly supports the iterative learning system vision by:\n1. **Validation**: Proving that generated strategies are not just syntactically valid but executionally viable\n2. **Measurement**: Establishing quantitative metrics (Sharpe ratio, returns, drawdown) for learning feedback\n3. **Quality Control**: Identifying failure patterns that need to be addressed before scaling to 1000+ iterations\n4. **Learning Foundation**: Providing the performance data needed to implement Phase 3 feedback loops\n\n## Requirements\n\n### Requirement 1: Strategy Execution Framework\n\n**User Story:** As a system developer, I want to execute generated strategies in a safe sandbox environment, so that I can measure their real-world performance without risking system stability.\n\n#### Acceptance Criteria\n\n1. WHEN a strategy code string is provided THEN the system SHALL execute it within a 5-minute timeout limit\n2. IF a strategy fails to complete within timeout THEN the system SHALL classify it as \"timeout error\" and continue with next strategy\n3. WHEN executing strategy code THEN the system SHALL provide access to finlab data context (data.get, data.indicator, sim functions)\n4. IF a runtime error occurs THEN the system SHALL catch the exception, log the error details, and classify the failure type\n5. WHEN all 20 strategies complete THEN the system SHALL generate a comprehensive results report with success/failure breakdown\n\n### Requirement 2: Performance Metrics Collection\n\n**User Story:** As a system analyst, I want to extract standardized performance metrics from backtest results, so that I can compare strategies objectively and identify successful patterns.\n\n#### Acceptance Criteria\n\n1. WHEN a strategy executes successfully THEN the system SHALL extract Sharpe Ratio from the sim() report\n2. WHEN a strategy executes successfully THEN the system SHALL extract Total Return percentage\n3. WHEN a strategy executes successfully THEN the system SHALL extract Maximum Drawdown percentage\n4. WHEN a strategy executes successfully THEN the system SHALL extract Win Rate (if available)\n5. IF metrics are NaN or missing THEN the system SHALL classify as \"invalid metrics\" failure\n6. WHEN metrics are valid THEN the system SHALL store them in structured JSON format with strategy ID\n\n### Requirement 3: Three-Level Success Classification\n\n**User Story:** As a quality engineer, I want strategies classified into three success levels, so that I can understand the quality distribution and set appropriate targets for Phase 3.\n\n#### Acceptance Criteria\n\n1. WHEN a strategy completes without runtime errors THEN it SHALL be classified as \"Level 1: Execution Success\"\n2. WHEN a strategy has valid metrics (Sharpe exists and is not NaN) THEN it SHALL be classified as \"Level 2: Valid Metrics\"\n3. WHEN a strategy has Sharpe Ratio > 0 THEN it SHALL be classified as \"Level 3: Positive Performance\"\n4. WHEN classifying strategies THEN the system SHALL count and report percentages for each level\n5. IF Level 1 success rate < 60% THEN the system SHALL flag for investigation before Phase 3\n6. IF Level 3 success rate < 40% THEN the system SHALL recommend prompt template improvements\n\n### Requirement 4: Error Handling and Classification\n\n**User Story:** As a system debugger, I want detailed error logs with categorization, so that I can identify and fix common failure patterns.\n\n#### Acceptance Criteria\n\n1. WHEN a runtime error occurs THEN the system SHALL capture the full stack trace\n2. WHEN an error is caught THEN the system SHALL classify it into categories: timeout, missing data, calculation error, syntax error, or other\n3. WHEN multiple strategies fail with similar errors THEN the system SHALL group them and report the pattern\n4. WHEN generating the report THEN the system SHALL include error frequency distribution\n5. IF a critical error pattern affects >20% of strategies THEN the system SHALL highlight it as \"requires immediate fix\"\n\n### Requirement 5: Results Reporting and Analysis\n\n**User Story:** As a project stakeholder, I want a comprehensive results report with visualizations, so that I can make informed decisions about Phase 3 readiness.\n\n#### Acceptance Criteria\n\n1. WHEN all strategies finish THEN the system SHALL generate a JSON results file with all metrics\n2. WHEN generating the report THEN the system SHALL include success rate for each level (L1, L2, L3)\n3. WHEN generating the report THEN the system SHALL include average Sharpe ratio for successful strategies\n4. WHEN generating the report THEN the system SHALL include execution time statistics (min, max, average)\n5. IF web search is available THEN the system SHALL compare results to industry benchmarks for quantitative strategies\n\n## Non-Functional Requirements\n\n### Code Architecture and Modularity\n- **Single Responsibility Principle**: Separate execution engine, metrics extractor, classifier, and reporter into distinct modules\n- **Modular Design**: Create reusable components that can be used in Phase 3 learning loop\n- **Dependency Management**: Minimize dependencies on specific finlab versions, use stable API patterns\n- **Clear Interfaces**: Define clean contracts between executor and metrics extractor (e.g., ExecutionResult dataclass)\n\n### Performance\n- **PERF-1**: Each strategy execution must complete within 5 minutes or timeout\n- **PERF-2**: Total test execution (20 strategies) should complete within 100 minutes maximum\n- **PERF-3**: Metrics extraction must complete within 5 seconds per strategy\n- **PERF-4**: Results report generation must complete within 10 seconds\n\n### Security\n- **SEC-1**: Code execution must use Python exec() with restricted globals/locals context\n- **SEC-2**: No file I/O operations should be allowed in executed strategies (sandbox restriction)\n- **SEC-3**: Network access should be restricted during strategy execution\n- **SEC-4**: Finlab data access should use existing authenticated sessions, no credential passing\n\n### Reliability\n- **REL-1**: System must handle all 20 strategies without crashing the test runner\n- **REL-2**: Failure of one strategy must not affect execution of subsequent strategies\n- **REL-3**: Results must be persisted to disk before process termination\n- **REL-4**: Re-running the test should produce deterministic results (given same data)\n\n### Usability\n- **USE-1**: Progress must be visible during execution (e.g., \"Processing strategy 5/20...\")\n- **USE-2**: Results report must be human-readable markdown in addition to JSON\n- **USE-3**: Error messages must be actionable (e.g., \"Missing dataset: etl:adj_close - Run data sync\")\n- **USE-4**: Dashboard integration should show real-time progress (optional enhancement)\n\n## Success Metrics\n\n**Primary Success Criteria**:\n- Achieve ≥ 60% Level 1 success rate (execution completes)\n- Achieve ≥ 40% Level 3 success rate (positive Sharpe ratio)\n\n**Secondary Success Criteria**:\n- Identify and document all error patterns\n- Provide baseline metrics for Phase 3 design\n- Complete test execution within reasonable time (< 100 minutes)\n\n**Exit Criteria for Phase 2**:\n- All 20 strategies tested\n- Results report generated and reviewed\n- Decision made on Phase 3 readiness based on success rates\n",
  "fileStats": {
    "size": 8210,
    "lines": 130,
    "lastModified": "2025-10-30T22:00:58.501Z"
  },
  "comments": []
}