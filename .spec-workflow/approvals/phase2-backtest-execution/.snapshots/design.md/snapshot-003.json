{
  "id": "snapshot_1761868290215_b96vcxo0k",
  "approvalId": "approval_1761868231602_jfl3v71lz",
  "approvalTitle": "Phase 2 Design Document (Revised - All Issues Fixed)",
  "version": 3,
  "timestamp": "2025-10-30T23:51:30.215Z",
  "trigger": "approved",
  "status": "pending",
  "content": "# Design Document\n\n## Overview\n\nThe Phase 2 Backtest Execution system provides a robust framework for executing AI-generated trading strategies with real finlab data and measuring their actual performance. Unlike Phase 1 (static validation), this system performs **real strategy execution** with data loading, computation, and backtest simulation to determine true success rates.\n\n**Core Problem**: Previously we only validated that strategies use correct dataset keys (static check). Now we need to verify they actually **run without errors** and **produce valid performance metrics**.\n\n**Solution**: Modular execution pipeline with timeout protection, metrics extraction, three-level classification, and comprehensive error reporting.\n\n## Steering Document Alignment\n\n### Technical Standards (tech.md)\n\n*Note: No formal tech.md exists yet for this project. Following Python best practices:*\n- **Modular Design**: Separate concerns into focused classes/modules\n- **Type Safety**: Use Python type hints for all public interfaces\n- **Error Handling**: Explicit exception handling with detailed logging\n- **Testing**: Unit tests for each component, integration tests for full pipeline\n\n### Project Structure (structure.md)\n\n*Note: No formal structure.md exists yet. Following existing patterns:*\n- Place test scripts in project root (e.g., `run_phase2_backtest_execution.py`)\n- Core execution logic in `src/backtest/` (new module)\n- Metrics utilities in `src/backtest/metrics.py` (exists, extend it)\n- Results output to project root as JSON/markdown files\n\n## Code Reuse Analysis\n\n### Existing Components to Leverage\n\n- **`src/backtest/metrics.py`**: Contains `MetricsExtractor` class - will extend for Sharpe/return extraction\n- **`artifacts/working/modules/poc_claude_test.py`**: Strategy generation logic - will reuse for loading generated code\n- **`artifacts/working/modules/static_validator.py`**: Static validation - will call before execution\n- **Timeout Utilities**: Python's `signal` module or `multiprocessing.Pool.apply_async()` with timeout\n- **JSON Logging**: Use Python's `json` module for structured results\n\n### Integration Points\n\n- **Generated Strategy Files**: Read from `generated_strategy_fixed_iter*.py` files (20 files from Phase 1)\n- **Finlab Data Access**: Must have finlab session authenticated, use existing `data.get()` API\n- **Sandbox Environment**: Execute in isolated Python namespace using `exec()` with restricted globals\n- **Results Storage**: Write to `phase2_backtest_results.json` and `PHASE2_BACKTEST_REPORT.md`\n\n## Architecture\n\nThe system follows a **pipeline architecture** with 5 sequential stages:\n\n```mermaid\ngraph TD\n    A[Phase2TestRunner] --> B[BacktestExecutor]\n    B --> C[MetricsExtractor]\n    C --> D[SuccessClassifier]\n    D --> E[ErrorClassifier]\n    E --> F[ResultsReporter]\n\n    B -->|timeout| E\n    B -->|runtime error| E\n    C -->|invalid metrics| D\n\n    F --> G[JSON Results]\n    F --> H[Markdown Report]\n```\n\n### Modular Design Principles\n\n- **Single File Responsibility**: Each component in separate file under `src/backtest/`\n- **Component Isolation**: `BacktestExecutor` knows nothing about metrics extraction\n- **Service Layer Separation**:\n  - Execution layer: `BacktestExecutor`\n  - Analysis layer: `MetricsExtractor`, `SuccessClassifier`, `ErrorClassifier`\n  - Presentation layer: `ResultsReporter`\n- **Utility Modularity**: Share timeout logic, error patterns as separate utilities\n\n## Components and Interfaces\n\n### Component 1: BacktestExecutor\n- **Purpose:** Execute strategy code with finlab data context and timeout protection\n- **File:** `src/backtest/executor.py`\n- **Interfaces:**\n  ```python\n  class ExecutionResult:\n      success: bool\n      code: str\n      report: Optional[Any]  # finlab sim() report object\n      error: Optional[str]\n      error_type: Optional[str]  # 'timeout' | 'runtime' | 'data_missing' | 'other'\n      execution_time: float\n\n  class BacktestExecutor:\n      def execute_strategy(code: str, timeout_seconds: int = 420) -> ExecutionResult:\n          \"\"\"Execute strategy code with timeout protection\"\"\"\n  ```\n- **Dependencies:**\n  - Python `signal` module (timeout)\n  - Finlab `data`, `sim` objects (must be in globals)\n- **Reuses:** Existing finlab session, no new dependencies\n\n### Component 2: MetricsExtractor\n- **Purpose:** Extract Sharpe Ratio, Total Return, Max Drawdown, Win Rate from sim() report\n- **File:** `src/backtest/metrics.py` (extend existing)\n- **Interfaces:**\n  ```python\n  class StrategyMetrics:\n      sharpe_ratio: Optional[float]\n      total_return: Optional[float]\n      max_drawdown: Optional[float]\n      win_rate: Optional[float]\n      has_valid_metrics: bool  # True if Sharpe exists and not NaN\n\n  class MetricsExtractor:\n      def extract_metrics(report: Any) -> StrategyMetrics:\n          \"\"\"Extract metrics from finlab sim() report object\"\"\"\n  ```\n- **Dependencies:**\n  - Finlab report object structure\n  - `pandas` for NaN checking\n- **Reuses:** Existing `src/backtest/metrics.py` structure\n\n### Component 3: SuccessClassifier\n- **Purpose:** Classify strategies into Level 1/2/3 based on execution and metrics\n- **File:** `src/backtest/classifier.py`\n- **Interfaces:**\n  ```python\n  class SuccessLevel(Enum):\n      LEVEL_0_FAILED = 0       # Execution failed\n      LEVEL_1_EXECUTION = 1     # Execution success\n      LEVEL_2_VALID_METRICS = 2 # Has valid metrics\n      LEVEL_3_POSITIVE_PERF = 3 # Sharpe > 0\n\n  class SuccessClassifier:\n      def classify(execution_result: ExecutionResult, metrics: StrategyMetrics) -> SuccessLevel:\n          \"\"\"Classify success level based on execution and metrics\"\"\"\n  ```\n- **Dependencies:**\n  - `ExecutionResult` from executor\n  - `StrategyMetrics` from extractor\n- **Reuses:** None (new component)\n\n### Component 4: ErrorClassifier\n- **Purpose:** Categorize errors into timeout, data missing, calculation errors, etc.\n- **File:** `src/backtest/error_classifier.py`\n- **Interfaces:**\n  ```python\n  class ErrorCategory(Enum):\n      TIMEOUT = \"timeout\"\n      MISSING_DATA = \"missing_data\"\n      CALCULATION_ERROR = \"calculation_error\"\n      SYNTAX_ERROR = \"syntax_error\"\n      OTHER = \"other\"\n\n  class ErrorClassifier:\n      def classify_error(error_message: str) -> ErrorCategory:\n          \"\"\"Classify error based on message patterns\"\"\"\n\n      def group_errors(results: List[ExecutionResult]) -> Dict[ErrorCategory, List[str]]:\n          \"\"\"Group errors by category for reporting\"\"\"\n  ```\n- **Dependencies:**\n  - Regular expressions for pattern matching\n- **Reuses:** None (new component)\n\n### Component 5: ResultsReporter\n- **Purpose:** Generate JSON and markdown reports with statistics\n- **File:** `src/backtest/reporter.py`\n- **Interfaces:**\n  ```python\n  class ReportData:\n      total_strategies: int\n      level_1_count: int\n      level_2_count: int\n      level_3_count: int\n      level_1_rate: float\n      level_2_rate: float\n      level_3_rate: float\n      avg_sharpe: Optional[float]\n      avg_execution_time: float\n      error_distribution: Dict[ErrorCategory, int]\n      strategy_results: List[Dict]  # Individual strategy details\n\n  class ResultsReporter:\n      def generate_json_report(data: ReportData, output_path: str) -> None:\n          \"\"\"Generate structured JSON results file\"\"\"\n\n      def generate_markdown_report(data: ReportData, output_path: str) -> None:\n          \"\"\"Generate human-readable markdown report\"\"\"\n  ```\n- **Dependencies:**\n  - Python `json` module\n  - Python string formatting\n- **Reuses:** None (new component)\n\n### Component 6: Phase2TestRunner (Main Controller)\n- **Purpose:** Orchestrate the entire test execution pipeline for 20 strategies\n- **File:** `run_phase2_backtest_execution.py` (project root)\n- **Interfaces:**\n  ```python\n  class Phase2TestRunner:\n      def __init__(strategy_files: List[str], timeout: int = 420):\n          \"\"\"Initialize with list of strategy files to test\"\"\"\n\n      def run_all() -> ReportData:\n          \"\"\"Execute all strategies and generate report\"\"\"\n\n      def run_single(strategy_file: str) -> Tuple[ExecutionResult, StrategyMetrics, SuccessLevel]:\n          \"\"\"Execute single strategy (for testing)\"\"\"\n  ```\n- **Dependencies:**\n  - All 5 components above\n  - Progress logging\n- **Reuses:** Existing generated strategy files from Phase 1\n\n## Data Models\n\n### ExecutionResult\n```python\n@dataclass\nclass ExecutionResult:\n    success: bool              # Did execution complete without errors?\n    code: str                  # Strategy code that was executed\n    report: Optional[Any]      # finlab sim() report object (if success)\n    error: Optional[str]       # Error message (if failure)\n    error_type: Optional[str]  # 'timeout' | 'runtime' | 'data_missing' | 'other'\n    execution_time: float      # Time taken in seconds\n    strategy_file: str         # Source file name\n```\n\n### StrategyMetrics\n```python\n@dataclass\nclass StrategyMetrics:\n    sharpe_ratio: Optional[float]     # Sharpe Ratio (None if NaN or missing)\n    total_return: Optional[float]     # Total return % (None if NaN or missing)\n    max_drawdown: Optional[float]     # Max drawdown % (None if NaN or missing)\n    win_rate: Optional[float]         # Win rate % (None if unavailable)\n    has_valid_metrics: bool           # True if sharpe_ratio is not None/NaN\n```\n\n### ReportData\n```python\n@dataclass\nclass ReportData:\n    total_strategies: int                     # Total tested (should be 20)\n    level_1_count: int                        # Execution success count\n    level_2_count: int                        # Valid metrics count\n    level_3_count: int                        # Positive performance count\n    level_1_rate: float                       # L1 / total (target ≥ 0.6)\n    level_2_rate: float                       # L2 / total\n    level_3_rate: float                       # L3 / total (target ≥ 0.4)\n    avg_sharpe: Optional[float]               # Avg Sharpe for L2+ strategies\n    avg_execution_time: float                 # Avg time per strategy\n    min_execution_time: float                 # Fastest execution\n    max_execution_time: float                 # Slowest execution\n    error_distribution: Dict[str, int]        # ErrorCategory -> count\n    strategy_results: List[Dict]              # Individual results\n    test_start_time: str                      # ISO timestamp\n    test_end_time: str                        # ISO timestamp\n    total_duration: float                     # Total test duration in seconds\n```\n\n## Error Handling\n\n### Error Scenarios\n\n1. **Timeout Error (7 minutes exceeded)**\n   - **Handling:**\n     - Use `multiprocessing.Process` with `join(timeout=420)` for cross-platform timeout\n     - Terminate process if still alive after timeout\n     - Mark as `error_type='timeout'`\n   - **User Impact:** Strategy marked as Level 0 (failed), counted in timeout category\n\n2. **Missing Dataset Error**\n   - **Handling:**\n     - Catch exceptions containing \"KeyError\" or dataset name\n     - Classify as `error_type='data_missing'`\n     - Extract missing dataset key from error message\n   - **User Impact:** Strategy marked as Level 0, error logged with missing key\n\n3. **Calculation Error (division by zero, invalid operations)**\n   - **Handling:**\n     - Catch `ValueError`, `ZeroDivisionError`, `TypeError`\n     - Classify as `error_type='calculation_error'`\n   - **User Impact:** Strategy marked as Level 0, error details logged\n\n4. **Syntax Error (should not happen after Phase 1, but defensive)**\n   - **Handling:**\n     - Catch `SyntaxError` during `exec()`\n     - Classify as `error_type='syntax_error'`\n   - **User Impact:** Strategy marked as Level 0, immediate investigation required\n\n5. **Invalid Metrics (NaN Sharpe Ratio)**\n   - **Handling:**\n     - Check `pd.isna(sharpe_ratio)` after extraction\n     - Mark `has_valid_metrics=False`\n   - **User Impact:** Strategy marked as Level 1 (execution success) but NOT Level 2\n\n6. **Report Object Not Found**\n   - **Handling:**\n     - Check if `report` variable exists in execution namespace\n     - If missing, mark as execution failure\n   - **User Impact:** Strategy marked as Level 0, logged as \"no report generated\"\n\n## Testing Strategy\n\n### Unit Testing\n\n**File:** `tests/backtest/test_executor.py`\n- Test timeout mechanism with intentional infinite loop\n- Test error classification with mock exceptions\n- Test metrics extraction with mock finlab reports\n\n**File:** `tests/backtest/test_classifier.py`\n- Test Level 1/2/3 classification logic\n- Test edge cases (Sharpe = 0, Sharpe = NaN, negative Sharpe)\n\n**File:** `tests/backtest/test_error_classifier.py`\n- Test regex patterns for error categorization\n- Test error grouping with multiple similar errors\n\n### Integration Testing\n\n**File:** `tests/integration/test_phase2_execution.py`\n- Test end-to-end execution with 3 mock strategies:\n  1. Simple valid strategy (should reach Level 3)\n  2. Strategy with timeout (infinite loop)\n  3. Strategy with missing dataset\n- Verify JSON report generation\n- Verify markdown report generation\n\n### End-to-End Testing\n\n**Test Scenario 1: Subset Validation**\n- Run on 3 real strategies from Phase 1 (`generated_strategy_fixed_iter0.py`, `iter1.py`, `iter2.py`)\n- Verify execution completes within expected time\n- Verify results match manual validation\n\n**Test Scenario 2: Full 20-Strategy Run**\n- Execute all 20 strategies from Phase 1\n- Verify total time < 140 minutes (20 * 7 min timeout)\n- Verify success rates meet targets (L1 ≥ 60%, L3 ≥ 40%)\n- Review error distribution for patterns\n\n## Implementation Notes\n\n### Execution Environment Setup\n\nThe `BacktestExecutor` must provide finlab context using multiprocessing for reliable cross-platform timeout:\n```python\nfrom multiprocessing import Process, Queue\nimport time\n\ndef _execute_in_process(code: str, result_queue: Queue):\n    \"\"\"Execute strategy code in isolated process\"\"\"\n    try:\n        from finlab import data\n        from finlab.backtest import sim\n        import pandas as pd\n        import numpy as np\n\n        execution_globals = {\n            'data': data,\n            'sim': sim,\n            'pd': pd,\n            'np': np\n        }\n\n        exec(code, execution_globals)\n        report = execution_globals.get('report')\n        result_queue.put(('success', report))\n    except Exception as e:\n        result_queue.put(('error', str(e), type(e).__name__))\n\ndef execute_strategy(code: str, timeout_seconds: int = 420) -> ExecutionResult:\n    \"\"\"Execute strategy with multiprocessing-based timeout\"\"\"\n    result_queue = Queue()\n    process = Process(target=_execute_in_process, args=(code, result_queue))\n\n    start_time = time.time()\n    process.start()\n    process.join(timeout=timeout_seconds)\n\n    if process.is_alive():\n        # Timeout occurred\n        process.terminate()\n        process.join()  # Wait for termination\n        return ExecutionResult(\n            success=False,\n            error_type='timeout',\n            error=f'Strategy execution exceeded {timeout_seconds}s timeout',\n            execution_time=timeout_seconds\n        )\n\n    # Process completed within timeout\n    if not result_queue.empty():\n        result = result_queue.get()\n        if result[0] == 'success':\n            return ExecutionResult(\n                success=True,\n                report=result[1],\n                execution_time=time.time() - start_time\n            )\n        else:\n            # Execution error\n            return ExecutionResult(\n                success=False,\n                error=result[1],\n                error_type=classify_error(result[2]),\n                execution_time=time.time() - start_time\n            )\n```\n\n**Benefits of multiprocessing approach:**\n- **Cross-platform**: Works reliably on Windows, macOS, and Linux\n- **True isolation**: Process-level isolation prevents memory leaks and resource issues\n- **Reliable timeout**: Effective even for I/O blocking and C extensions\n- **Thread-safe**: No signal handler conflicts with multi-threaded code\n\n### Progress Visibility\n\nUse simple print statements during execution:\n```python\nprint(f\"[{i+1}/20] Executing {strategy_file}...\")\nprint(f\"  → Execution time: {exec_time:.1f}s\")\nprint(f\"  → Level: {level.name}\")\n```\n\n### File Organization\n\n```\nfinlab/\n├── src/\n│   └── backtest/\n│       ├── __init__.py\n│       ├── executor.py          # BacktestExecutor\n│       ├── metrics.py           # MetricsExtractor (extend existing)\n│       ├── classifier.py        # SuccessClassifier\n│       ├── error_classifier.py  # ErrorClassifier\n│       └── reporter.py          # ResultsReporter\n├── tests/\n│   ├── backtest/\n│   │   ├── test_executor.py\n│   │   ├── test_classifier.py\n│   │   └── test_error_classifier.py\n│   └── integration/\n│       └── test_phase2_execution.py\n├── run_phase2_backtest_execution.py  # Main test runner\n├── phase2_backtest_results.json      # Output (generated)\n└── PHASE2_BACKTEST_REPORT.md         # Output (generated)\n```\n\n## Security Considerations\n\n- **Sandboxing:** Use restricted `globals` dict in `exec()` - no `__import__`, `open`, `os`, `sys`\n- **Timeout Protection:** Hard 7-minute limit per strategy to prevent runaway execution\n- **Resource Limits:** Consider adding memory limits (optional enhancement)\n- **Authenticated Access:** Finlab data access uses existing session, no credential passing\n\n## Performance Optimizations\n\n- **Parallel Execution (Future Enhancement):** Could execute strategies in parallel using `multiprocessing.Pool` to reduce total time from 140 min to ~30 min (with 5 workers)\n- **Caching:** Finlab data is cached by finlab library, no additional caching needed\n- **Early Exit:** If error pattern affects >50% of strategies, consider alerting user to stop and fix\n\n## Success Criteria\n\n- All 5 components implemented and unit tested\n- Integration test passes with 3 mock strategies\n- Full 20-strategy execution completes successfully\n- JSON and markdown reports generated correctly\n- Success rates measured and documented (even if below targets)\n",
  "fileStats": {
    "size": 18106,
    "lines": 472,
    "lastModified": "2025-10-30T23:40:12.762Z"
  },
  "comments": []
}