{
  "id": "snapshot_1761863162954_ouh5djsq9",
  "approvalId": "approval_1761863162800_fuikqij5i",
  "approvalTitle": "Phase 2 Backtest Execution - Tasks Document",
  "version": 1,
  "timestamp": "2025-10-30T22:26:02.954Z",
  "trigger": "initial",
  "status": "pending",
  "content": "# Tasks Document\n\n## Phase 1: Core Execution Infrastructure\n\n- [ ] 1.1 Create BacktestExecutor class\n  - File: src/backtest/executor.py\n  - Implement timeout protection using signal.alarm()\n  - Create ExecutionResult dataclass for return values\n  - Add finlab context setup (data, sim globals)\n  - Purpose: Provide safe strategy execution with timeout\n  - _Leverage: Python signal module, existing finlab session_\n  - _Requirements: 1 (Strategy Execution Framework)_\n  - _Prompt: Role: Python Backend Developer with expertise in process management and timeout handling | Task: Create BacktestExecutor class following Requirement 1, implementing timeout protection using signal.alarm(), safe code execution with exec(), and proper finlab context setup (data.get, sim) | Restrictions: Must use signal.alarm() for timeout (not threading), execute in restricted globals, catch all exceptions with detailed error messages, ensure cleanup in finally block | Success: Executor handles timeouts correctly, execution errors are caught with stack traces, finlab context is properly provided, ExecutionResult dataclass contains all required fields_\n\n- [ ] 1.2 Add timeout mechanism testing\n  - File: tests/backtest/test_executor.py\n  - Write unit tests for timeout scenarios\n  - Test infinite loop detection (while True)\n  - Verify cleanup after timeout\n  - Purpose: Ensure timeout protection works reliably\n  - _Leverage: pytest framework, mock strategies_\n  - _Requirements: 1 (Timeout handling)_\n  - _Prompt: Role: QA Engineer with expertise in Python testing and timeout scenarios | Task: Create comprehensive timeout tests for BacktestExecutor covering edge cases like infinite loops, long computations, and nested function calls | Restrictions: Tests must complete quickly (use short timeouts like 2s for testing), must not leave zombie processes, verify signal handler cleanup | Success: All timeout scenarios are tested, tests complete in <10 seconds, no resource leaks detected_\n\n- [ ] 1.3 Add error classification patterns\n  - File: src/backtest/error_classifier.py\n  - Define ErrorCategory enum (timeout, data_missing, calculation, syntax, other)\n  - Implement regex patterns for error detection\n  - Create classify_error() and group_errors() methods\n  - Purpose: Categorize errors for better debugging\n  - _Leverage: Python re module, error message patterns_\n  - _Requirements: 4 (Error Handling and Classification)_\n  - _Prompt: Role: Python Developer with expertise in error handling and regex pattern matching | Task: Implement ErrorClassifier following Requirement 4, using regex patterns to detect error categories (KeyError for missing data, ZeroDivisionError for calculation errors, TimeoutError for timeouts) | Restrictions: Must handle Chinese error messages (finlab uses 中文), patterns must be robust to different error formats, classify unknown errors as 'other' | Success: Classifier correctly identifies all 5 error categories, handles both English and Chinese messages, provides actionable error grouping_\n\n## Phase 2: Metrics Extraction\n\n- [ ] 2.1 Extend MetricsExtractor class\n  - File: src/backtest/metrics.py (extend existing)\n  - Add extract_metrics() method for finlab reports\n  - Create StrategyMetrics dataclass\n  - Handle NaN values with pd.isna() checks\n  - Purpose: Extract Sharpe, Return, Drawdown, Win Rate\n  - _Leverage: Existing src/backtest/metrics.py structure, pandas_\n  - _Requirements: 2 (Performance Metrics Collection)_\n  - _Prompt: Role: Quant Developer with expertise in financial metrics and pandas | Task: Extend existing MetricsExtractor in src/backtest/metrics.py following Requirement 2, extracting Sharpe Ratio, Total Return, Max Drawdown, and Win Rate from finlab sim() reports, handling NaN with pandas | Restrictions: Must not modify existing metrics code, handle missing metrics gracefully (return None), verify Sharpe is numeric and not NaN before marking valid | Success: Extractor handles all report formats, NaN detection works correctly, StrategyMetrics dataclass has proper type hints, metrics match finlab report values_\n\n- [ ] 2.2 Add metrics extraction tests\n  - File: tests/backtest/test_metrics.py\n  - Create mock finlab report objects\n  - Test NaN handling and edge cases\n  - Verify metrics accuracy\n  - Purpose: Ensure reliable metrics extraction\n  - _Leverage: pytest, mock finlab reports_\n  - _Requirements: 2 (Metrics validation)_\n  - _Prompt: Role: QA Engineer with expertise in financial data testing | Task: Create unit tests for MetricsExtractor covering normal cases, NaN handling, missing metrics, and edge cases like Sharpe=0, negative returns | Restrictions: Must create realistic mock report objects matching finlab structure, test all 4 metrics independently, verify has_valid_metrics flag logic | Success: All metrics extraction scenarios tested, edge cases covered (NaN, None, missing fields), tests validate correct StrategyMetrics creation_\n\n## Phase 3: Success Classification\n\n- [ ] 3.1 Create SuccessClassifier\n  - File: src/backtest/classifier.py\n  - Define SuccessLevel enum (LEVEL_0 to LEVEL_3)\n  - Implement classify() method with clear logic\n  - Add validation for success level requirements\n  - Purpose: Classify strategies into 4 levels\n  - _Leverage: ExecutionResult, StrategyMetrics dataclasses_\n  - _Requirements: 3 (Three-Level Success Classification)_\n  - _Prompt: Role: Python Developer with expertise in classification systems | Task: Create SuccessClassifier following Requirement 3, implementing clear classification logic: Level 0 (failed), Level 1 (execution success), Level 2 (valid Sharpe), Level 3 (Sharpe > 0) | Restrictions: Classification must be deterministic and unambiguous, must check conditions in order (execution → valid metrics → positive performance), document classification logic clearly | Success: Classifier correctly assigns all 4 levels, logic is easy to understand and maintain, edge cases like Sharpe=0 or Sharpe=NaN are handled correctly_\n\n- [ ] 3.2 Add classification tests\n  - File: tests/backtest/test_classifier.py\n  - Test all 4 classification levels\n  - Test edge cases (Sharpe=0, Sharpe=NaN, negative)\n  - Verify classification consistency\n  - Purpose: Ensure correct level assignment\n  - _Leverage: pytest, mock ExecutionResult and StrategyMetrics_\n  - _Requirements: 3 (Classification validation)_\n  - _Prompt: Role: QA Engineer with expertise in classification testing | Task: Create comprehensive tests for SuccessClassifier covering all 4 levels and edge cases (execution failed, valid metrics but Sharpe=NaN, Sharpe=0, negative Sharpe, positive Sharpe) | Restrictions: Must test all classification paths, verify level assignment is correct for each scenario, test with realistic combinations of success/failure | Success: All 4 levels tested with multiple scenarios each, edge cases documented and verified, classification logic validated_\n\n## Phase 4: Results Reporting\n\n- [ ] 4.1 Create ResultsReporter class\n  - File: src/backtest/reporter.py\n  - Implement generate_json_report() for structured output\n  - Implement generate_markdown_report() for human reading\n  - Add statistics calculation (rates, averages)\n  - Purpose: Generate comprehensive test reports\n  - _Leverage: Python json module, string formatting_\n  - _Requirements: 5 (Results Reporting and Analysis)_\n  - _Prompt: Role: Python Developer with expertise in report generation and data formatting | Task: Create ResultsReporter following Requirement 5, generating both JSON (machine-readable) and Markdown (human-readable) reports with success rates, average Sharpe, execution times, and error distribution | Restrictions: JSON must be valid and parseable, Markdown must be well-formatted with tables and sections, calculate percentages correctly (avoid division by zero), include timestamps | Success: Reports are clear and informative, JSON is valid, Markdown is readable, statistics are accurate, reports include all required metrics_\n\n- [ ] 4.2 Add report generation tests\n  - File: tests/backtest/test_reporter.py\n  - Test JSON format validity\n  - Test markdown rendering\n  - Verify statistics accuracy\n  - Purpose: Ensure report quality and correctness\n  - _Leverage: pytest, json.loads() for validation_\n  - _Requirements: 5 (Report validation)_\n  - _Prompt: Role: QA Engineer with expertise in data validation and report testing | Task: Create tests for ResultsReporter validating JSON structure (parseable, contains all fields), Markdown formatting (readable, contains tables), and statistics accuracy (rates, averages calculated correctly) | Restrictions: Must validate JSON against schema, test with various result combinations (all success, all failure, mixed), verify edge cases like 0 strategies or all timeouts | Success: JSON validation passes, Markdown is properly formatted, statistics match manual calculations, reports handle edge cases gracefully_\n\n## Phase 5: Main Test Runner\n\n- [ ] 5.1 Create Phase2TestRunner\n  - File: run_phase2_backtest_execution.py (project root)\n  - Implement main orchestration logic\n  - Add progress logging during execution\n  - Integrate all 5 components (executor, extractor, classifier, error_classifier, reporter)\n  - Purpose: Execute 20 strategies end-to-end\n  - _Leverage: All components from Phases 1-4, existing generated_strategy_fixed_iter*.py files_\n  - _Requirements: All (end-to-end execution)_\n  - _Prompt: Role: Senior Python Developer with expertise in system integration and orchestration | Task: Create Phase2TestRunner orchestrating all components (BacktestExecutor, MetricsExtractor, SuccessClassifier, ErrorClassifier, ResultsReporter) to execute 20 strategies from generated_strategy_fixed_iter*.py files | Restrictions: Must show progress (e.g., \"Processing 5/20...\"), handle individual strategy failures gracefully (continue to next), log all results to JSON before exiting, ensure finlab session is authenticated | Success: Runner executes all 20 strategies without crashing, progress is visible, results are saved even if some strategies fail, error handling is robust_\n\n- [ ] 5.2 Add runner integration tests\n  - File: tests/integration/test_phase2_execution.py\n  - Create 3 mock strategies (valid, timeout, error)\n  - Test full pipeline execution\n  - Verify report generation\n  - Purpose: Validate end-to-end workflow\n  - _Leverage: pytest, mock finlab session_\n  - _Requirements: All (integration validation)_\n  - _Prompt: Role: Integration Test Engineer with expertise in E2E testing | Task: Create integration test for Phase2TestRunner using 3 mock strategies: one valid (should reach Level 3), one with infinite loop (timeout), one with missing data (error), verify all components work together correctly | Restrictions: Must mock finlab session to avoid real data dependency, test should complete in <30 seconds, verify JSON and Markdown reports are generated | Success: Integration test validates full pipeline, all 3 strategies classified correctly, reports generated successfully, test is fast and reliable_\n\n## Phase 6: Pre-Execution Setup\n\n- [ ] 6.1 Verify generated strategies exist\n  - Files: generated_strategy_fixed_iter0.py through iter19.py\n  - Count files and verify content\n  - Ensure all use adjusted data (etl:adj_close)\n  - Purpose: Confirm test prerequisites\n  - _Leverage: bash ls, grep commands_\n  - _Requirements: Prerequisites for Requirement 1_\n  - _Prompt: Role: DevOps Engineer with bash scripting expertise | Task: Create verification script to check all 20 generated_strategy_fixed_iter*.py files exist, contain valid Python code, and use adjusted data (grep for 'etl:adj_close'), report any files using forbidden raw data | Restrictions: Script must be non-destructive (read-only), report clear errors if files missing, check for both positive (adjusted data) and negative (raw data) patterns | Success: Script verifies all 20 files exist, all use adjusted data, clear report generated, script exits with error code if verification fails_\n\n- [ ] 6.2 Verify finlab session authentication\n  - Create authentication check script\n  - Test data.get() call works\n  - Verify sim() function available\n  - Purpose: Ensure finlab environment ready\n  - _Leverage: finlab API, existing session_\n  - _Requirements: Prerequisites for Requirement 1_\n  - _Prompt: Role: DevOps Engineer with Python and API authentication expertise | Task: Create script to verify finlab session is authenticated by testing data.get('etl:adj_close') returns data and sim() function is callable | Restrictions: Must not modify data or execute strategies, handle authentication failures gracefully with clear error messages, verify both data access and sim function | Success: Script confirms finlab session works, provides clear success/failure message, helps diagnose authentication issues if any_\n\n## Phase 7: Execution and Validation\n\n- [ ] 7.1 Run 3-strategy pilot test\n  - Execute test on iter0, iter1, iter2 only\n  - Verify execution times are reasonable\n  - Check report accuracy manually\n  - Purpose: Validate system before full run\n  - _Leverage: Phase2TestRunner with --limit flag_\n  - _Requirements: Testing strategy validation_\n  - _Prompt: Role: QA Engineer with expertise in pilot testing | Task: Execute Phase2TestRunner on first 3 strategies (iter0-2) to validate the system works correctly before full 20-strategy run, manually verify metrics match expected values | Restrictions: Must complete in <30 minutes (3 * 7min + overhead), verify each classification level is correct, check for any unexpected errors | Success: Pilot test completes successfully, all 3 strategies classified correctly, reports generated accurately, no critical issues found_\n\n- [ ] 7.2 Run full 20-strategy execution\n  - Execute all 20 strategies with Phase2TestRunner\n  - Monitor progress and log output\n  - Save results to phase2_backtest_results.json\n  - Purpose: Complete Phase 2 backtest validation\n  - _Leverage: Phase2TestRunner, all components_\n  - _Requirements: All requirements_\n  - _Prompt: Role: Test Execution Engineer | Task: Execute full Phase2TestRunner on all 20 generated strategies, monitor progress, ensure results are saved to JSON and Markdown files, verify test completes within 140-minute timeout | Restrictions: Must not interrupt test once started, monitor for any crashes or hangs, ensure disk space for logs, verify finlab session remains authenticated throughout | Success: All 20 strategies tested, results saved to JSON and Markdown, test completes within time limit, success rates calculated and reported_\n\n- [ ] 7.3 Analyze results and generate summary\n  - Review phase2_backtest_results.json\n  - Calculate success rate against targets (L1 ≥60%, L3 ≥40%)\n  - Identify error patterns requiring fixes\n  - Create PHASE2_EXECUTION_COMPLETE.md summary\n  - Purpose: Document Phase 2 completion and decision\n  - _Leverage: Python json module, manual analysis_\n  - _Requirements: Success Metrics evaluation_\n  - _Prompt: Role: Data Analyst with expertise in test result analysis | Task: Analyze phase2_backtest_results.json, compare success rates against targets (Level 1 ≥60%, Level 3 ≥40%), identify top error patterns, create executive summary with recommendations for Phase 3 readiness | Restrictions: Must provide objective analysis (even if targets not met), document all error patterns with frequencies, make clear go/no-go recommendation for Phase 3 | Success: Clear summary generated showing actual vs target rates, error patterns documented with examples, actionable recommendations provided, decision on Phase 3 readiness made_\n\n## Phase 8: Documentation and Cleanup\n\n- [ ] 8.1 Document execution framework\n  - Update README.md with Phase 2 execution section\n  - Add usage examples for BacktestExecutor\n  - Document metrics extraction process\n  - Purpose: Enable future use of execution framework\n  - _Leverage: Existing README structure_\n  - _Requirements: Usability (USE-3)_\n  - _Prompt: Role: Technical Writer with Python documentation expertise | Task: Document Phase 2 execution framework in README.md, including how to use BacktestExecutor independently, metrics extraction process, and success classification logic with examples | Restrictions: Must follow existing README format, include code examples that work, document all public APIs, keep explanations clear and concise | Success: Documentation is clear and comprehensive, examples are runnable, users can understand and use the framework independently_\n\n- [ ] 8.2 Add API documentation\n  - Generate docstrings for all public methods\n  - Create API reference documentation\n  - Add type hints to all functions\n  - Purpose: Improve code maintainability\n  - _Leverage: Python type hints, docstring conventions_\n  - _Requirements: Code quality best practices_\n  - _Prompt: Role: Python Developer with expertise in documentation and type systems | Task: Add comprehensive docstrings to all public methods in executor, metrics, classifier, error_classifier, and reporter modules, following Google/NumPy docstring format, add type hints to all function signatures | Restrictions: Must document all parameters, return values, and exceptions, type hints must pass mypy validation, docstrings must be consistent in format | Success: All public APIs documented with clear docstrings, type hints added and validated, API reference can be auto-generated from docstrings_\n\n- [ ] 8.3 Code review and optimization\n  - Review all code for best practices\n  - Optimize execution performance if needed\n  - Add logging for debugging\n  - Purpose: Ensure production quality\n  - _Leverage: Python logging module, profiling tools_\n  - _Requirements: Code quality and performance_\n  - _Prompt: Role: Senior Python Developer with code review and optimization expertise | Task: Review all Phase 2 code for best practices (error handling, resource cleanup, code organization), optimize performance bottlenecks, add structured logging for debugging using Python logging module | Restrictions: Must not break existing functionality, optimizations must be measurable (profile first), logging must be appropriate level (INFO for progress, DEBUG for details) | Success: Code follows Python best practices, performance is acceptable (no unnecessary delays), logging helps debugging without spam, code review findings documented and resolved_\n\n## Success Criteria Checklist\n\n- [ ] All 20 strategies execute without crashing the test runner (REL-1)\n- [ ] Execution completes within 140 minutes total (PERF-2)\n- [ ] Success rates measured: L1, L2, L3 (even if below targets)\n- [ ] Error patterns identified and documented\n- [ ] JSON and Markdown reports generated successfully\n- [ ] Decision made on Phase 3 readiness based on results\n- [ ] Documentation complete and clear\n- [ ] All tests passing (unit + integration)\n",
  "fileStats": {
    "size": 18764,
    "lines": 212,
    "lastModified": "2025-10-30T22:25:49.118Z"
  },
  "comments": []
}