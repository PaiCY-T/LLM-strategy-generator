{
  "id": "snapshot_1761661474878_5a4shqsu7",
  "approvalId": "approval_1761661474732_luy9jezw7",
  "approvalTitle": "Docker Sandbox Integration Testing - Tasks",
  "version": 1,
  "timestamp": "2025-10-28T14:24:34.878Z",
  "trigger": "initial",
  "status": "pending",
  "content": "# Tasks Document: Docker Sandbox Integration Testing\n\n## Overview\n\n**Spec**: docker-sandbox-integration-testing\n**Total Tasks**: 13 tasks across 4 phases\n**Timeline**: 10 working days\n**Dependencies**: Docker Sandbox implementation complete (91%, 2,529 lines)\n\n## Phase 1: Basic Functionality Tests (Days 1-2)\n\n- [ ] 1.1 Create Docker lifecycle tests\n  - **File**: `tests/sandbox/test_docker_lifecycle.py` (NEW)\n  - **Purpose**: Test container start, execution, stop, cleanup lifecycle\n  - **Test Cases**:\n    - Container starts within 10 seconds\n    - Strategy executes successfully\n    - Container terminates within 5 seconds\n    - Concurrent execution (5 containers)\n  - **_Leverage**: `src/sandbox/docker_executor.py` (613 lines, already complete)\n  - **_Requirements**: Requirement 1 (Basic Docker Sandbox Functionality)\n  - **_Prompt**: Implement the task for spec docker-sandbox-integration-testing, first run spec-workflow-guide to get the workflow guide then implement the task: Role: QA Engineer with expertise in Docker testing and pytest | Task: Create comprehensive lifecycle tests in tests/sandbox/test_docker_lifecycle.py covering Requirement 1, leveraging existing DockerExecutor from src/sandbox/docker_executor.py to test container start/stop/cleanup operations | Restrictions: Do not modify docker_executor.py code, only test existing functionality; must test both success and failure scenarios; ensure test isolation with proper setup/teardown | Success: All 5 test cases pass, tests run independently, container cleanup verified, timing assertions validate <10s startup and <5s cleanup; edit tasks.md to mark [-] when starting and [x] when complete\n\n- [ ] 1.2 Create resource limit enforcement tests\n  - **File**: `tests/sandbox/test_resource_limits.py` (NEW)\n  - **Purpose**: Validate CPU, Memory, Disk limits enforced at container level\n  - **Test Cases**:\n    - Memory limit (2GB) triggers OOMKilled\n    - CPU timeout (300s) triggers Timeout\n    - Disk limit (1GB) triggers restriction\n    - Violations logged correctly\n  - **_Leverage**: `src/sandbox/docker_config.py` (329 lines, limit configuration), `src/sandbox/container_monitor.py` (619 lines, resource tracking)\n  - **_Requirements**: Requirement 2 (Resource Limits Enforcement)\n  - **_Prompt**: Implement the task for spec docker-sandbox-integration-testing, first run spec-workflow-guide to get the workflow guide then implement the task: Role: DevOps Engineer with expertise in Docker resource management and stress testing | Task: Create resource limit tests in tests/sandbox/test_resource_limits.py covering Requirement 2, using DockerConfig for limit configuration and ContainerMonitor for validation | Restrictions: Must submit intentionally malicious strategies to test limits; do not bypass resource limits in tests; ensure tests clean up even on OOMKilled scenarios | Success: All 4 test cases pass, memory/CPU/disk limits enforced correctly, violations logged with proper metadata, test cleanup prevents resource leaks; edit tasks.md to mark [-] when starting and [x] when complete\n\n- [ ] 1.3 Create Seccomp security tests\n  - **File**: `tests/sandbox/test_seccomp_security.py` (NEW)\n  - **Purpose**: Validate dangerous syscalls blocked by Seccomp profile\n  - **Test Cases**:\n    - File I/O (open, read, write) blocked\n    - Network (socket, connect) blocked\n    - Process manipulation (fork, exec) blocked\n    - Time manipulation (settimeofday) blocked\n    - Allowed syscalls (getpid) permitted\n  - **_Leverage**: `config/seccomp_profile.json` (Seccomp whitelist), `src/sandbox/runtime_monitor.py` (584 lines, security monitoring)\n  - **_Requirements**: Requirement 3 (Seccomp Security Profile)\n  - **_Prompt**: Implement the task for spec docker-sandbox-integration-testing, first run spec-workflow-guide to get the workflow guide then implement the task: Role: Security Engineer with expertise in Linux syscalls and container security | Task: Create Seccomp security tests in tests/sandbox/test_seccomp_security.py covering Requirement 3, leveraging existing Seccomp profile and RuntimeMonitor for violation detection | Restrictions: Must test actual syscall attempts (not mocked); do not modify Seccomp profile during tests; ensure malicious test strategies cannot escape sandbox | Success: All 5 test cases pass, dangerous syscalls blocked with EPERM, allowed syscalls succeed, violations logged to RuntimeMonitor with strategy ID and syscall name; edit tasks.md to mark [-] when starting and [x] when complete\n\n## Phase 2: Integration (Days 3-5)\n\n- [ ] 2.1 Implement SandboxExecutionWrapper in autonomous loop\n  - **Files**:\n    - `artifacts/working/modules/autonomous_loop.py` (MODIFY, +40 lines)\n  - **Purpose**: Integrate Docker Sandbox execution with automatic fallback\n  - **Changes**:\n    - Add `SandboxExecutionWrapper` class (~40 lines)\n    - Methods: `__init__`, `execute_strategy`, `_sandbox_execution`, `_direct_execution`, `get_fallback_stats`\n    - Wrap existing execution logic (do not replace)\n  - **_Leverage**: `src/sandbox/docker_executor.py`, existing execution pipeline in `autonomous_loop.py`\n  - **_Requirements**: Requirement 4 (Integration with Autonomous Loop)\n  - **_Prompt**: Implement the task for spec docker-sandbox-integration-testing, first run spec-workflow-guide to get the workflow guide then implement the task: Role: Senior Python Developer with expertise in wrapper patterns and error handling | Task: Implement SandboxExecutionWrapper in autonomous_loop.py covering Requirement 4, wrapping existing execution logic with Docker Sandbox support and automatic fallback to AST-only on failures | Restrictions: Must not modify existing execution path when sandbox disabled; keep integration code minimal (<50 lines); ensure zero regression with sandbox.enabled: false; do not remove AST validation layer | Success: Wrapper class implemented with all methods, sandbox routes to DockerExecutor when enabled, automatic fallback on timeout/error, metadata tracking (sandbox_used, fallback), backward compatible with existing tests; edit tasks.md to mark [-] when starting and [x] when complete\n\n- [ ] 2.2 Create integration tests for sandbox wrapper\n  - **File**: `tests/integration/test_sandbox_integration.py` (NEW)\n  - **Purpose**: Test autonomous loop integration with sandbox enabled/disabled\n  - **Test Cases**:\n    - Sandbox enabled → routes to DockerExecutor\n    - Sandbox disabled → routes to direct execution\n    - Timeout triggers fallback to AST-only\n    - Docker error triggers fallback\n    - Metadata tracking works correctly\n  - **_Leverage**: `SandboxExecutionWrapper` from autonomous_loop.py, existing test fixtures\n  - **_Requirements**: Requirement 4 (Integration with Autonomous Loop)\n  - **_Prompt**: Implement the task for spec docker-sandbox-integration-testing, first run spec-workflow-guide to get the workflow guide then implement the task: Role: Integration Test Engineer with expertise in pytest and mocking | Task: Create integration tests in tests/integration/test_sandbox_integration.py covering Requirement 4, testing SandboxExecutionWrapper routing and fallback logic | Restrictions: Must test both sandbox enabled and disabled paths; use mocking for DockerExecutor to control timeout/error scenarios; do not test Docker internals, only wrapper behavior | Success: All 5 test cases pass, routing logic validated, fallback mechanism tested under failure conditions, metadata assertions verify sandbox_used and fallback flags; edit tasks.md to mark [-] when starting and [x] when complete\n\n- [ ] 2.3 Create 5-iteration smoke test\n  - **File**: `tests/integration/test_sandbox_e2e.py` (NEW)\n  - **Purpose**: End-to-end validation with 5 full iterations\n  - **Test Cases**:\n    - 5 iterations complete successfully with sandbox enabled\n    - Success rate: 100%\n    - Results returned correctly\n    - Monitoring integration validated\n  - **_Leverage**: Existing iteration execution infrastructure, Turtle/Momentum templates as strategy code\n  - **_Requirements**: Requirement 4 (Integration), Requirement 5 (Performance Benchmarking, initial data)\n  - **_Prompt**: Implement the task for spec docker-sandbox-integration-testing, first run spec-workflow-guide to get the workflow guide then implement the task: Role: QA Automation Engineer with expertise in end-to-end testing and system validation | Task: Create 5-iteration smoke test in tests/integration/test_sandbox_e2e.py covering Requirements 4 and 5, running real autonomous evolution cycles with Docker Sandbox enabled | Restrictions: Must use real strategy code (Turtle/Momentum templates); must complete all 5 iterations without manual intervention; test must run in <15 minutes to be practical | Success: All 5 iterations complete successfully, 100% success rate achieved, results returned with valid metrics (Sharpe, Calmar), monitoring metrics collected, iteration metadata includes sandbox_used flags; edit tasks.md to mark [-] when starting and [x] when complete\n\n## Phase 3: Performance Benchmarking (Days 6-8)\n\n- [ ] 3.1 Create performance benchmark test suite\n  - **File**: `tests/performance/test_sandbox_performance.py` (NEW)\n  - **Purpose**: Measure sandbox overhead and compare vs. AST-only\n  - **Test Cases**:\n    - Run 20 iterations with sandbox.enabled: false (baseline)\n    - Run 20 iterations with sandbox.enabled: true\n    - Calculate overhead percentage\n    - Verify success rate parity (both 100%)\n  - **_Leverage**: Existing iteration timing infrastructure, `src/monitoring/metrics_collector.py`\n  - **_Requirements**: Requirement 5 (Performance Benchmarking)\n  - **_Prompt**: Implement the task for spec docker-sandbox-integration-testing, first run spec-workflow-guide to get the workflow guide then implement the task: Role: Performance Engineer with expertise in benchmarking and statistical analysis | Task: Create performance benchmark suite in tests/performance/test_sandbox_performance.py covering Requirement 5, measuring iteration time overhead of Docker Sandbox vs. AST-only execution | Restrictions: Must run sufficient iterations (20) for statistical significance; must control for external factors (same machine, same strategies); must validate both modes achieve 100% success rate | Success: Benchmark collects timing data for both modes, calculates overhead percentage using formula (sandbox_avg - ast_avg) / ast_avg * 100%, success rate parity verified, results logged with mean/std/min/max for both modes; edit tasks.md to mark [-] when starting and [x] when complete\n\n- [ ] 3.2 Run comprehensive performance validation (20-iteration test)\n  - **Files**:\n    - **Execute**: `python3 -m pytest tests/performance/test_sandbox_performance.py -v`\n    - **Output**: Save results to `DOCKER_SANDBOX_PERFORMANCE_REPORT.md`\n  - **Purpose**: Execute 20-iteration validation test and collect empirical data\n  - **Data to Collect**:\n    - AST-only: Mean iteration time, std dev\n    - Docker Sandbox: Mean iteration time, std dev\n    - Overhead percentage\n    - Success rate (both modes)\n  - **_Leverage**: Test suite from Task 3.1, existing monitoring system\n  - **_Requirements**: Requirement 5 (Performance Benchmarking)\n  - **_Prompt**: Implement the task for spec docker-sandbox-integration-testing, first run spec-workflow-guide to get the workflow guide then implement the task: Role: QA Lead with expertise in test execution and reporting | Task: Execute 20-iteration performance validation test covering Requirement 5, running test_sandbox_performance.py and documenting results in DOCKER_SANDBOX_PERFORMANCE_REPORT.md | Restrictions: Must run tests on same machine without interference; must wait for completion without interruption; must document exact machine specs (CPU, RAM) for reproducibility | Success: Test completes successfully (20 iterations each mode), performance report generated with detailed metrics (mean/std/overhead%), graphs or tables comparing both modes, machine specifications documented; edit tasks.md to mark [-] when starting and [x] when complete\n\n- [ ] 3.3 Analyze performance results and identify bottlenecks\n  - **Files**:\n    - **Read**: `DOCKER_SANDBOX_PERFORMANCE_REPORT.md` (from Task 3.2)\n    - **Create**: `DOCKER_SANDBOX_BOTTLENECK_ANALYSIS.md` (NEW)\n  - **Purpose**: Analyze performance data to identify specific bottlenecks\n  - **Analysis**:\n    - Container startup time breakdown\n    - Data loading overhead (Taiwan stock ~10M points)\n    - Execution time vs. cleanup time\n    - Comparison with 13-26s AST-only baseline\n  - **_Leverage**: Monitoring data from `src/monitoring/container_monitor.py`, performance report from Task 3.2\n  - **_Requirements**: Requirement 5 (Performance Benchmarking)\n  - **_Prompt**: Implement the task for spec docker-sandbox-integration-testing, first run spec-workflow-guide to get the workflow guide then implement the task: Role: Performance Analyst with expertise in profiling and bottleneck identification | Task: Analyze performance results covering Requirement 5, reading data from DOCKER_SANDBOX_PERFORMANCE_REPORT.md and creating bottleneck analysis in DOCKER_SANDBOX_BOTTLENECK_ANALYSIS.md | Restrictions: Must identify specific bottlenecks with evidence; must compare against documented 13-26s AST-only baseline; must provide actionable recommendations if overhead is high | Success: Bottleneck analysis document created with detailed breakdown of startup/execution/cleanup times, comparison with baseline documented, specific bottlenecks identified (if any), recommendations provided for optimization (if overhead >50%); edit tasks.md to mark [-] when starting and [x] when complete\n\n## Phase 4: Decision & Documentation (Days 9-10)\n\n- [ ] 4.1 Apply decision framework based on test results\n  - **Files**:\n    - **Read**: `DOCKER_SANDBOX_PERFORMANCE_REPORT.md`, functional test results\n    - **Create**: `DOCKER_SANDBOX_DECISION_REPORT.md` (NEW)\n  - **Purpose**: Apply Requirement 6 decision framework to determine enable/disable\n  - **Decision Matrix**:\n    ```\n    | Functional Tests | Overhead | Decision |\n    |-----------------|----------|----------|\n    | ✅ PASS | < 50% | ✅ Enable by default |\n    | ✅ PASS | 50-100% | ⚠️ Optional feature |\n    | ✅ PASS | > 100% | ❌ Document only |\n    | ❌ FAIL | Any | ❌ Do not use |\n    ```\n  - **_Leverage**: Performance data from Phase 3, functional test results from Phase 1-2\n  - **_Requirements**: Requirement 6 (Decision Framework)\n  - **_Prompt**: Implement the task for spec docker-sandbox-integration-testing, first run spec-workflow-guide to get the workflow guide then implement the task: Role: Technical Product Manager with expertise in data-driven decision making | Task: Apply decision framework covering Requirement 6, analyzing all test results and performance data to determine Docker Sandbox deployment strategy in DOCKER_SANDBOX_DECISION_REPORT.md | Restrictions: Must follow decision matrix exactly as specified in Requirement 6; must provide clear rationale citing specific test results; must document risks and trade-offs for chosen decision | Success: Decision report created with clear recommendation (enable by default / optional / document only / do not use), rationale documented with specific test data citations, risks and trade-offs documented, decision aligns with matrix criteria; edit tasks.md to mark [-] when starting and [x] when complete\n\n- [ ] 4.2 Update configuration based on decision\n  - **Files**:\n    - `config/learning_system.yaml` (MODIFY, sandbox section)\n    - `README.md` (MODIFY, add sandbox activation instructions if optional)\n  - **Purpose**: Update configuration and documentation based on decision\n  - **Changes**:\n    - If \"enable by default\": Set `sandbox.enabled: true`\n    - If \"optional feature\": Keep `sandbox.enabled: false`, add activation docs to README\n    - If \"document only\": Keep disabled, add warning comment to config\n  - **_Leverage**: Decision from Task 4.1, existing config structure in `config/learning_system.yaml`\n  - **_Requirements**: Requirement 6 (Decision Framework)\n  - **_Prompt**: Implement the task for spec docker-sandbox-integration-testing, first run spec-workflow-guide to get the workflow guide then implement the task: Role: DevOps Engineer with expertise in configuration management and documentation | Task: Update configuration and documentation covering Requirement 6, implementing decision from DOCKER_SANDBOX_DECISION_REPORT.md by modifying config/learning_system.yaml and updating README.md | Restrictions: Must follow decision exactly as specified in decision report; if optional feature, must provide clear activation instructions; must not break existing functionality | Success: Config file updated with appropriate sandbox.enabled value and comments, README updated with activation instructions (if optional), changes are backward compatible, clear documentation of decision rationale in config comments; edit tasks.md to mark [-] when starting and [x] when complete\n\n- [ ] 4.3 Update STATUS.md with decision rationale\n  - **Files**:\n    - `STATUS.md` (MODIFY, Docker Sandbox section)\n    - `.spec-workflow/specs/docker-sandbox-integration-testing/STATUS.md` (CREATE)\n  - **Purpose**: Document decision rationale and implementation evidence\n  - **Content**:\n    - Test results summary (functional + performance)\n    - Decision and rationale\n    - Overhead data\n    - Future optimization recommendations (if needed)\n  - **_Leverage**: All test results and decision report from Phase 3-4\n  - **_Requirements**: Requirement 6 (Decision Framework)\n  - **_Prompt**: Implement the task for spec docker-sandbox-integration-testing, first run spec-workflow-guide to get the workflow guide then implement the task: Role: Technical Writer with expertise in technical documentation and architecture decision records | Task: Document decision rationale covering Requirement 6, updating STATUS.md with comprehensive decision record and creating spec STATUS.md in .spec-workflow/specs/docker-sandbox-integration-testing/ | Restrictions: Must include all test results (functional + performance); must explain decision rationale clearly; must document any known limitations or future work | Success: STATUS.md updated with Docker Sandbox decision section including test results, decision, rationale, and overhead data; spec STATUS.md created with implementation evidence and completion status; documentation is clear and actionable for future developers; edit tasks.md to mark [-] when starting and [x] when complete\n\n- [ ] 4.4 Create activation and troubleshooting guide\n  - **Files**:\n    - `docs/DOCKER_SANDBOX_GUIDE.md` (CREATE)\n  - **Purpose**: Provide user-facing guide for enabling/disabling and troubleshooting\n  - **Sections**:\n    - **Overview**: What is Docker Sandbox and why use it\n    - **Prerequisites**: Docker Desktop installation, system requirements\n    - **Activation**: How to enable (`sandbox.enabled: true`)\n    - **Verification**: How to verify it's working\n    - **Troubleshooting**: Common issues (Docker not running, timeout, OOMKilled)\n    - **Performance**: Expected overhead and optimization tips\n  - **_Leverage**: Decision report, bottleneck analysis, existing troubleshooting patterns from project\n  - **_Requirements**: All requirements (comprehensive guide)\n  - **_Prompt**: Implement the task for spec docker-sandbox-integration-testing, first run spec-workflow-guide to get the workflow guide then implement the task: Role: Technical Writer with expertise in user documentation and troubleshooting guides | Task: Create comprehensive Docker Sandbox guide in docs/DOCKER_SANDBOX_GUIDE.md covering all requirements, providing clear activation instructions and troubleshooting for common issues | Restrictions: Must be user-friendly for non-experts; must include concrete examples and commands; must address Windows-specific issues (multiprocessing); must link to relevant config files and test results | Success: Guide created with all sections (overview, prerequisites, activation, verification, troubleshooting, performance), clear step-by-step instructions with commands, troubleshooting section covers common errors with solutions, guide is complete and ready for end users; edit tasks.md to mark [-] when starting and [x] when complete\n\n## Summary\n\n### Task Breakdown by Phase\n\n| Phase | Tasks | Days | Purpose |\n|-------|-------|------|---------|\n| **Phase 1** | 1.1-1.3 | 2 | Basic Functionality Tests |\n| **Phase 2** | 2.1-2.3 | 3 | Integration with Autonomous Loop |\n| **Phase 3** | 3.1-3.3 | 3 | Performance Benchmarking |\n| **Phase 4** | 4.1-4.4 | 2 | Decision & Documentation |\n| **Total** | **13 tasks** | **10 days** | **Complete validation and integration** |\n\n### Dependencies\n\n```\nPhase 1 (1.1, 1.2, 1.3) ─┐\n                          ├─> Phase 2 (2.1) ─> Phase 2 (2.2) ─> Phase 2 (2.3) ─┐\n                          │                                                      │\n                          └──────────────────────────────────────────────────────┤\n                                                                                 │\nPhase 3 (3.1) ─> Phase 3 (3.2) ─> Phase 3 (3.3) ────────────────────────────────┤\n                                                                                 │\n                                                                                 v\nPhase 4 (4.1) ─> Phase 4 (4.2) ─> Phase 4 (4.3) ─> Phase 4 (4.4)\n```\n\n**Critical Path**: 1.1-1.3 → 2.1 → 2.2 → 2.3 → 3.1 → 3.2 → 3.3 → 4.1 → 4.2 → 4.3 → 4.4\n\n### Success Metrics\n\n**Functional Validation**:\n- ✅ All Phase 1 tests pass (Req 1-3)\n- ✅ All Phase 2 tests pass (Req 4)\n- ✅ 5-iteration smoke test: 100% success rate\n\n**Performance Validation**:\n- ✅ 20-iteration test completed (both modes)\n- ✅ Overhead percentage calculated\n- ✅ Bottlenecks identified (if any)\n\n**Decision & Documentation**:\n- ✅ Decision framework applied\n- ✅ Configuration updated appropriately\n- ✅ Comprehensive documentation created\n\n### Files Created/Modified\n\n**New Test Files** (7):\n- `tests/sandbox/test_docker_lifecycle.py`\n- `tests/sandbox/test_resource_limits.py`\n- `tests/sandbox/test_seccomp_security.py`\n- `tests/integration/test_sandbox_integration.py`\n- `tests/integration/test_sandbox_e2e.py`\n- `tests/performance/test_sandbox_performance.py`\n\n**Modified Files** (3):\n- `artifacts/working/modules/autonomous_loop.py` (+40 lines)\n- `config/learning_system.yaml` (update sandbox.enabled)\n- `STATUS.md` (add decision record)\n- `README.md` (add activation instructions if optional)\n\n**New Documentation** (5):\n- `DOCKER_SANDBOX_PERFORMANCE_REPORT.md`\n- `DOCKER_SANDBOX_BOTTLENECK_ANALYSIS.md`\n- `DOCKER_SANDBOX_DECISION_REPORT.md`\n- `.spec-workflow/specs/docker-sandbox-integration-testing/STATUS.md`\n- `docs/DOCKER_SANDBOX_GUIDE.md`\n\n### Risk Mitigation\n\n| Risk | Task Addressing | Mitigation |\n|------|----------------|------------|\n| Windows multiprocessing overhead >100% | 3.2, 4.1 | Decision framework allows \"optional feature\" |\n| Docker daemon errors | 2.2, 2.3 | Automatic fallback tested |\n| Regression in AST-only mode | 2.2 | Backward compatibility tests |\n| Taiwan stock data load timeout | 3.3 | Bottleneck analysis identifies specific issue |\n\n### Implementation Notes\n\n**For Each Task**:\n1. Edit `tasks.md`: Change `[ ]` to `[-]` when starting\n2. Read the `_Prompt` field for guidance\n3. Follow `_Leverage` fields to use existing code\n4. Implement according to `_Requirements`\n5. Verify `Success` criteria met\n6. Edit `tasks.md`: Change `[-]` to `[x]` when complete\n\n**Starting Point**: Begin with Task 1.1 (Docker lifecycle tests)\n",
  "fileStats": {
    "size": 23813,
    "lines": 280,
    "lastModified": "2025-10-28T14:22:10.243Z"
  },
  "comments": []
}