{
  "id": "snapshot_1762040196349_pf6lg9r8w",
  "approvalId": "approval_1762040196210_htww1zpmz",
  "approvalTitle": "Design Document - Bug Fixes & Test Framework (No Refactoring)",
  "version": 1,
  "timestamp": "2025-11-01T23:36:36.349Z",
  "trigger": "initial",
  "status": "pending",
  "content": "# Design Document\n\n## Overview\n\nThis design establishes a systematic approach to fix 4 critical integration bugs and create a test framework to prevent future integration failures. The implementation follows a **test-first, minimal-change** philosophy: fix only what's broken, add tests to verify fixes, avoid any refactoring work.\n\n**Scope**: Bug fixes (~50 lines) + test framework (characterization, unit, integration tests)\n**Out of Scope**: Refactoring, architectural changes, code reorganization (see Requirement 7)\n\n**Note**: This spec focuses exclusively on stabilization. Refactoring will be addressed in a separate future spec (`autonomous-loop-refactoring`) after all 8 completion criteria are met.\n\n## Steering Document Alignment\n\nThis project does not currently have steering documents (product.md, tech.md, structure.md). The design follows observed project conventions:\n\n### Observed Technical Patterns\n- **Testing Framework**: pytest with fixtures in `tests/` directory\n- **Mocking Strategy**: unittest.mock for external dependencies\n- **Logging**: Python logging module with structured messages\n- **Docker Integration**: docker-py library for container execution\n- **Configuration**: YAML files in `config/` directory\n\n### Observed Project Structure\n```\nfinlab/\n├── src/\n│   ├── sandbox/          # Docker execution (Bug #1 location)\n│   ├── innovation/       # LLM generation (Bug #2 location)\n│   └── config/           # Configuration (Bug #3 location)\n├── artifacts/working/modules/\n│   └── autonomous_loop.py  # Main orchestrator (Bug #4 location)\n├── tests/\n│   ├── unit/            # Unit tests (new)\n│   ├── integration/     # Integration tests (existing + new)\n│   └── conftest.py      # Shared fixtures\n└── config/              # YAML configurations\n```\n\n## Code Reuse Analysis\n\n### Existing Components to Leverage\n- **pytest framework**: Already used for integration tests in `tests/integration/`\n- **Docker executor**: Existing `src/sandbox/docker_executor.py` - fix, don't rewrite\n- **LLM generator**: Existing `src/innovation/llm_strategy_generator.py` - add validation only\n- **Autonomous loop**: Existing `artifacts/working/modules/autonomous_loop.py` - minimal fixes\n- **Mock fixtures**: Can reuse mock data patterns from manual test in `/tmp/docker_test_strategy.py`\n\n### Integration Points\n- **Docker API**: Existing integration via docker-py library\n- **LLM APIs**: Existing integration via provider-specific clients (Google, OpenRouter)\n- **File I/O**: Strategy code written to temp files before Docker execution\n- **Logging**: Existing logging infrastructure - add diagnostic messages only\n\n## Architecture\n\n### Design Principles for Bug Fixes\n\n1. **Minimal Change Principle**: Change only the lines necessary to fix each bug\n2. **Test-First Approach**: Write failing test, then implement fix to make test pass\n3. **Characterization Test**: Establish baseline behavior before any changes\n4. **No Refactoring**: Strictly prohibited until Requirement 7 criteria are met (separate spec)\n\n### Integration Boundary Map\n\n```mermaid\ngraph LR\n    A[LLM Generator] -->|Generated Code| B[Autonomous Loop]\n    B -->|Assembled Code| C[Docker Executor]\n    C -->|Execution Result| D[Metrics Extractor]\n    D -->|Metrics| B\n\n    style A fill:#f9f,stroke:#333\n    style B fill:#ff9,stroke:#333\n    style C fill:#9ff,stroke:#333\n    style D fill:#9f9,stroke:#333\n\n    A -.->|Bug #2: API Routing| A\n    B -.->|Bug #1: F-string| B\n    B -.->|Bug #4: Exception| B\n    C -.->|Bug #3: Import| C\n```\n\n**Critical Boundaries**:\n1. **Boundary A→B**: LLM generation → Code assembly (Bug #2: API routing validation)\n2. **Boundary B→C**: Code assembly → Docker execution (Bug #1: F-string evaluation)\n3. **Boundary C→D**: Docker execution → Metrics extraction (Bug #3: Missing module)\n4. **Boundary D→B**: Exception handling → State update (Bug #4: State propagation)\n\n## Components and Interfaces\n\n### Component 1: F-String Template Evaluator\n\n**Purpose**: Ensure f-string templates are evaluated before Docker injection\n\n**Location**: `artifacts/working/modules/autonomous_loop.py:344`\n\n**Current Code**:\n```python\ncomplete_code = data_setup + \"\\n\" + code + \"\\n\" + metrics_extraction\n```\n\n**Investigation Needed**: Verify if `data_setup` f-string is evaluated properly. The template contains `{{}}` escaping which should become `{}` after evaluation.\n\n**Potential Fix** (if needed):\n```python\n# Ensure f-string template is fully evaluated before Docker injection\ncomplete_code = data_setup + \"\\n\" + code + \"\\n\" + metrics_extraction\n\n# Add diagnostic logging\nself.logger.debug(f\"Complete code (first 500 chars): {complete_code[:500]}\")\n\n# Verify no {{}} remains (should all be {})\nif '{{' in complete_code or '}}' in complete_code:\n    self.logger.warning(\"F-string template may not be fully evaluated\")\n```\n\n**Interfaces**:\n- Input: `data_setup` (str), `code` (str), `metrics_extraction` (str)\n- Output: `complete_code` (str) with all `{{}}` resolved to `{}`\n\n**Dependencies**: None (pure string operation)\n\n**Test Strategy**: Characterization test to verify current behavior, then fix if needed\n\n### Component 2: LLM API Routing Validator\n\n**Purpose**: Validate model name matches provider before API call\n\n**Location**: `src/innovation/llm_strategy_generator.py` (new function)\n\n**Implementation**:\n```python\ndef _validate_model_provider_match(provider: str, model: str) -> None:\n    \"\"\"\n    Validate model name matches provider capabilities.\n\n    Args:\n        provider: LLM provider name ('google', 'openrouter', 'anthropic')\n        model: Model identifier (e.g., 'gemini-2.5-flash', 'anthropic/claude-3.5-sonnet')\n\n    Raises:\n        ValueError: If model/provider combination is invalid\n\n    Examples:\n        >>> _validate_model_provider_match('google', 'gemini-2.5-flash')  # OK\n        >>> _validate_model_provider_match('google', 'anthropic/claude-3.5-sonnet')\n        ValueError: Model 'anthropic/claude-3.5-sonnet' cannot be used with provider 'google'...\n    \"\"\"\n    if provider == 'google':\n        if model.startswith('anthropic/'):\n            raise ValueError(\n                f\"Model '{model}' cannot be used with provider '{provider}'. \"\n                f\"Anthropic models require provider='openrouter'.\"\n            )\n        if not model.startswith('gemini'):\n            raise ValueError(\n                f\"Google provider requires gemini-* models, got: '{model}'\"\n            )\n\n    # Can add more provider validations as needed\n    # For now, only validate Google provider (the failing case)\n```\n\n**Integration Point**: Call this function in LLM generator `__init__()` method\n\n**Interfaces**:\n- Input: `provider` (str), `model` (str)\n- Output: None (raises ValueError on mismatch)\n\n**Dependencies**: None (pure validation logic)\n\n**Test Strategy**: Unit tests with valid/invalid combinations\n\n### Component 3: ExperimentConfig Module\n\n**Purpose**: Provide minimal configuration snapshot capability\n\n**Location**: `src/config/experiment_config.py` (new file)\n\n**Implementation**:\n```python\n\"\"\"Configuration snapshot for experiment reproducibility.\"\"\"\nfrom dataclasses import dataclass, asdict\nfrom typing import Dict, Any, Optional\n\n\n@dataclass\nclass ExperimentConfig:\n    \"\"\"\n    Captures configuration state for a single experiment iteration.\n\n    This is a minimal implementation to fix import errors.\n    Future enhancement may add validation, versioning, etc.\n\n    Attributes:\n        iteration: Iteration number\n        config_snapshot: Dictionary containing full config state\n        timestamp: ISO 8601 timestamp when config was captured\n    \"\"\"\n\n    iteration: int\n    config_snapshot: Dict[str, Any]\n    timestamp: Optional[str] = None\n\n    @classmethod\n    def from_dict(cls, config_dict: Dict[str, Any]) -> 'ExperimentConfig':\n        \"\"\"\n        Create ExperimentConfig from dictionary.\n\n        Args:\n            config_dict: Dictionary with iteration, config_snapshot, timestamp keys\n\n        Returns:\n            ExperimentConfig instance\n        \"\"\"\n        return cls(**config_dict)\n\n    def to_dict(self) -> Dict[str, Any]:\n        \"\"\"\n        Convert ExperimentConfig to dictionary.\n\n        Returns:\n            Dictionary representation suitable for JSON serialization\n        \"\"\"\n        return asdict(self)\n```\n\n**Interfaces**:\n- `from_dict(config_dict)`: Create instance from dictionary\n- `to_dict()`: Serialize instance to dictionary\n\n**Dependencies**:\n- `dataclasses` (Python standard library)\n- `typing` (Python standard library)\n\n**Test Strategy**: Unit tests for serialization/deserialization round-trip\n\n### Component 4: Exception State Propagator\n\n**Purpose**: Ensure exceptions update state to trigger diversity fallback\n\n**Location**: `artifacts/working/modules/autonomous_loop.py:106-113`\n\n**Current Code**:\n```python\nexcept Exception as e:\n    self.logger.error(f\"Error during iteration {self.iteration}: {e}\")\n    # self.last_result is NOT set here - BUG!\n```\n\n**Fixed Code**:\n```python\nexcept Exception as e:\n    self.logger.error(f\"Error during iteration {self.iteration}: {e}\")\n    self.last_result = False  # Trigger diversity fallback on next iteration\n    self.logger.info(\"Setting last_result=False to enable diversity fallback\")\n```\n\n**Interfaces**:\n- Modifies: `self.last_result` (bool)\n- Logs: Error message and state change notification\n\n**Dependencies**:\n- `self.logger` (existing logging instance)\n- `self.last_result` (existing state variable)\n\n**Test Strategy**: Integration test that triggers exception and verifies state change\n\n## Data Models\n\n### MetricsResult (existing)\n```python\n{\n    'success': bool,\n    'signal': Dict[str, float] | None,\n    'error': str | None,\n    'execution_time': float,\n    'container_id': str,\n    'logs': str\n}\n```\n\n### ExperimentConfig (new)\n```python\n{\n    'iteration': int,\n    'config_snapshot': Dict[str, Any],\n    'timestamp': str | None\n}\n```\n\n### LLMConfig (existing, needs validation)\n```python\n{\n    'provider': str,  # 'google' | 'openrouter' | 'anthropic'\n    'model': str,     # Model name (must match provider)\n    'temperature': float,\n    # ... other fields\n}\n```\n\n## Error Handling\n\n### Error Scenario 1: F-String Not Evaluated\n\n**Description**: F-string template contains `{{}}` which becomes `{}` in Python code sent to Docker, causing SyntaxError\n\n**Current Behavior**: Container exits with code 1, no diagnostic info\n\n**Fixed Behavior**:\n1. Add diagnostic logging before file write: log first 500 chars of `complete_code`\n2. Verify `{{}}` is resolved to `{}`\n3. If not resolved, investigate why f-string evaluation failed\n\n**User Impact**:\n- Before: Silent failure, requires manual debugging\n- After: Clear log message showing code sent to Docker\n\n### Error Scenario 2: Wrong Provider for Model\n\n**Description**: Config specifies `provider='google'` with `model='anthropic/claude-3.5-sonnet'`\n\n**Current Behavior**: 404 error from Google API, fallback to Factor Graph (diversity never activates)\n\n**Fixed Behavior**:\n1. Validate at LLM initialization\n2. Raise clear ValueError with correction guidance\n3. Fail fast before API call\n\n**User Impact**:\n- Before: Cryptic 404 error, unclear cause\n- After: Clear error: \"Anthropic models require provider='openrouter'\"\n\n### Error Scenario 3: Missing Configuration Module\n\n**Description**: Import `from src.config.experiment_config import ExperimentConfig` fails\n\n**Current Behavior**: Warning logged every iteration, config snapshot not saved\n\n**Fixed Behavior**:\n1. Create minimal module\n2. Import succeeds\n3. Config snapshots saved correctly\n\n**User Impact**:\n- Before: Warning spam in logs\n- After: Clean logs, reproducible experiments\n\n### Error Scenario 4: Exception Doesn't Trigger Diversity\n\n**Description**: Docker execution exception doesn't update `last_result`, so diversity fallback never activates\n\n**Current Behavior**: Error logged, next iteration uses normal (non-diversity) LLM\n\n**Fixed Behavior**:\n1. Set `last_result = False` in exception handler\n2. Log state change explicitly\n3. Next iteration triggers diversity fallback\n\n**User Impact**:\n- Before: Diversity prompting never activates (0%)\n- After: Diversity activates after failures (target >30%)\n\n## Testing Strategy\n\n### Characterization Test (Critical First Step)\n\n**Purpose**: Establish baseline behavior before ANY changes\n\n**Test File**: `tests/integration/test_characterization_baseline.py`\n\n**Approach**:\n```python\ndef test_current_system_behavior():\n    \"\"\"\n    Characterization test: Document current behavior (even if wrong).\n\n    This test captures the ACTUAL behavior before bug fixes.\n    After fixes, this test will FAIL, which is expected.\n    We then update it to verify correct behavior.\n    \"\"\"\n    # Run one iteration with mocked LLM and Docker\n    # Document:\n    # 1. Does f-string evaluation happen? (check for {{}} in file)\n    # 2. Does LLM API validation happen? (try wrong provider)\n    # 3. Does config module import succeed?\n    # 4. Does exception set last_result=False?\n\n    # Assert current behavior (may be wrong, that's OK)\n    # This gives us a safety net for future refactoring\n```\n\n### Unit Testing\n\n**Test File**: `tests/unit/test_bug_fixes.py`\n\n**Bug #2 - LLM API Validation**:\n```python\ndef test_validate_google_provider_rejects_anthropic_model():\n    \"\"\"Bug #2: Validate model/provider matching\"\"\"\n    with pytest.raises(ValueError, match=\"Anthropic models require\"):\n        _validate_model_provider_match('google', 'anthropic/claude-3.5-sonnet')\n\n\ndef test_validate_google_provider_accepts_gemini_model():\n    \"\"\"Bug #2: Validate Google provider accepts gemini models\"\"\"\n    _validate_model_provider_match('google', 'gemini-2.5-flash')  # Should not raise\n```\n\n**Bug #3 - ExperimentConfig**:\n```python\ndef test_experiment_config_serialization():\n    \"\"\"Bug #3: Verify ExperimentConfig module works\"\"\"\n    config = ExperimentConfig(\n        iteration=1,\n        config_snapshot={'test': 'data'},\n        timestamp='2025-11-02T12:00:00'\n    )\n\n    # Verify serialization round-trip\n    config_dict = config.to_dict()\n    restored = ExperimentConfig.from_dict(config_dict)\n    assert restored.iteration == 1\n    assert restored.config_snapshot == {'test': 'data'}\n```\n\n### Integration Testing\n\n**Test File**: `tests/integration/test_docker_integration_boundaries.py`\n\n**Bug #1 - F-String Evaluation**:\n```python\ndef test_f_string_template_evaluated_before_docker(tmp_path, mock_docker_client):\n    \"\"\"\n    Bug #1: Verify f-string template is evaluated before Docker execution.\n\n    This test verifies complete_code contains '{}' not '{{}}' when written to file.\n    \"\"\"\n    # Mock autonomous loop with real data_setup template\n    # Execute one iteration\n    # Read the file written for Docker\n    # Assert: file contains 'STOCK_{:04d}'.format(i) not 'STOCK_{{:04d}}'.format(i)\n    # Assert: no SyntaxError when exec() is called on file content\n```\n\n**Bug #4 - Exception State Propagation**:\n```python\ndef test_exception_triggers_diversity_fallback(mock_docker_client, mock_llm_client):\n    \"\"\"\n    Bug #4: Verify exceptions update last_result to trigger diversity.\n\n    This test simulates Docker execution failure and verifies diversity activates.\n    \"\"\"\n    # Mock Docker to raise exception\n    mock_docker_client.containers.run.side_effect = Exception(\"Mock failure\")\n\n    # Run iteration 1 (should fail)\n    loop = AutonomousLoop(config)\n    loop.run_iteration()\n\n    # Verify: last_result == False\n    assert loop.last_result is False\n\n    # Run iteration 2 (should use diversity LLM)\n    # Verify: LLM call uses diversity model\n```\n\n### End-to-End Testing\n\n**Test File**: `tests/integration/test_docker_integration_e2e.py`\n\n**Full System Test**:\n```python\ndef test_end_to_end_docker_execution_with_real_strategy():\n    \"\"\"\n    E2E test: Verify complete flow from LLM generation to metrics extraction.\n\n    Uses real strategy code (not LLM-generated, use fixed template).\n    Mocks: LLM API, Docker execution\n    Real: Code assembly, f-string evaluation, file writing\n    \"\"\"\n    # 1. Mock LLM to return known-good strategy code\n    # 2. Run autonomous loop iteration\n    # 3. Verify: complete_code has no {{}}\n    # 4. Verify: Docker receives syntactically valid Python\n    # 5. Verify: Metrics extraction works (if Docker succeeds)\n    # 6. Verify: Diagnostic logs present at each boundary\n```\n\n### Test Execution Order\n\n```\n1. Characterization test (establishes baseline)\n   └─> EXPECTED: May fail (system is broken)\n\n2. Unit tests (test fixes in isolation)\n   └─> EXPECTED: Fail initially (bugs not fixed yet)\n\n3. Implement bug fixes (minimal changes)\n   └─> GOAL: Make unit tests pass\n\n4. Integration tests (test boundaries)\n   └─> EXPECTED: Fail initially, pass after fixes\n\n5. E2E test (test full flow)\n   └─> EXPECTED: Pass after all fixes\n\n6. Update characterization test to verify correct behavior\n   └─> GOAL: Document new baseline\n```\n\n### Test Coverage Requirements\n\n- **Unit Tests**: 100% coverage of bug fix code (4 fixes, ~10 unit tests)\n- **Integration Tests**: 100% coverage of integration boundaries (4 boundaries, ~4 integration tests)\n- **E2E Tests**: At least 1 full system test verifying all fixes work together\n- **Characterization Test**: 1 test establishing baseline before/after\n\n**Total Test Suite**: ~15-20 tests, execution time <30 seconds\n\n### Diagnostic Instrumentation\n\n**Logging Additions** (all at DEBUG level to avoid log spam):\n\n1. **LLM Initialization** (`src/innovation/llm_strategy_generator.py`):\n   ```python\n   self.logger.debug(f\"LLM initialized: provider={provider}, model={model}\")\n   ```\n\n2. **Code Assembly** (`artifacts/working/modules/autonomous_loop.py:344`):\n   ```python\n   self.logger.debug(f\"Complete code (first 500 chars): {complete_code[:500]}\")\n   ```\n\n3. **Docker Execution Result** (`src/sandbox/docker_executor.py`):\n   ```python\n   self.logger.debug(f\"Docker result: {result}\")\n   ```\n\n4. **Exception Handling** (`artifacts/working/modules/autonomous_loop.py:108`):\n   ```python\n   self.logger.info(\"Setting last_result=False to enable diversity fallback\")\n   ```\n\n## Implementation Constraints\n\n### Scope Limitations (from Requirement 7)\n\n**DO NOT**:\n- Refactor any files\n- Reorganize code structure\n- Extract new classes/modules (except ExperimentConfig which is required)\n- Change architectural patterns\n- Optimize code that is already working\n\n**DO**:\n- Fix bugs with minimal line changes\n- Add tests for bug fixes\n- Add diagnostic logging only\n- Create missing module (ExperimentConfig) with minimal implementation\n\n### Work Estimate\n\n- **Bug Fix #1** (F-string): 2 lines + 10 lines logging = 12 lines, 30 min\n- **Bug Fix #2** (Validation): 15 lines validation function + 2 lines call = 17 lines, 30 min\n- **Bug Fix #3** (Module): New file ~25 lines, 30 min\n- **Bug Fix #4** (Exception): 2 lines + 1 line logging = 3 lines, 15 min\n- **Characterization Test**: ~50 lines, 1 hour\n- **Unit Tests**: ~100 lines, 1 hour\n- **Integration Tests**: ~150 lines, 1.5 hours\n- **E2E Test**: ~75 lines, 1 hour\n\n**Total**: ~57 lines of bug fixes, ~375 lines of tests, **5.5 hours**\n\n## Success Criteria\n\n### Immediate Success (after implementation)\n\n- ✅ All unit tests pass (4 bugs covered)\n- ✅ All integration tests pass (4 boundaries covered)\n- ✅ E2E test passes (full flow works)\n- ✅ Characterization test updated and passes\n\n### System-Level Success (after deployment)\n\n- ✅ Docker execution success rate >80% (currently 0%)\n- ✅ Diversity-aware prompting activates ≥30% of eligible iterations (currently 0%)\n- ✅ No regression in direct-execution mode (verify with existing tests)\n- ✅ Configuration snapshots saved successfully (no import warnings)\n\n### Spec Completion Criteria (Requirement 7)\n\nAll 8 conditions from Requirement 7 must be met to close this spec:\n- ✅ All 4 critical bugs fixed\n- ✅ Test framework established and integrated into CI\n- ✅ Diagnostic instrumentation in place\n- ✅ Characterization test passes\n- ✅ System execution success rate >80% for 30+ iterations (verified via metrics)\n- ✅ Diversity-aware prompting activates ≥30% of eligible iterations (verified via logs)\n- ✅ No regression in direct-execution mode\n- ✅ Maintenance difficulties observed and documented\n\n**After ALL criteria met**: Evaluate whether to initiate `autonomous-loop-refactoring` spec.\n\n## Post-Stabilization Handoff\n\nThis design focuses exclusively on stabilization. After this spec is complete and all success criteria are met, a separate specification (`autonomous-loop-refactoring`) may be created to address technical debt.\n\n**Evidence to Gather During Implementation**:\n- Document any maintenance difficulties encountered during bug fixes\n- Note any performance bottlenecks discovered\n- Capture developer feedback on code comprehension challenges\n- Track which modules required the most changes\n\nThis evidence will inform the scope and priority of future refactoring work.\n",
  "fileStats": {
    "size": 20990,
    "lines": 616,
    "lastModified": "2025-11-01T23:36:20.778Z"
  },
  "comments": []
}