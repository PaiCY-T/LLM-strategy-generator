{
  "id": "snapshot_1762037111948_e3g9kvuqg",
  "approvalId": "approval_1762037111805_3magsdp3j",
  "approvalTitle": "Requirements Document - Docker Integration Test Framework",
  "version": 1,
  "timestamp": "2025-11-01T22:45:11.948Z",
  "trigger": "initial",
  "status": "pending",
  "content": "# Requirements Document\n\n## Introduction\n\nThe Docker Integration Test Framework addresses critical integration failures in the autonomous learning system where components work correctly in isolation but fail when integrated together. The system currently experiences **100% Docker execution failure** and **diversity-aware prompting never activates** due to missing validation at integration boundaries.\n\nThis feature establishes systematic boundary validation through a three-tier testing strategy (unit, integration, E2E) and provides diagnostic instrumentation to catch integration issues early. The framework will prevent future integration failures and unblock the Stage 2 LLM innovation capability.\n\n## Alignment with Product Vision\n\nThis feature directly supports Stage 2 (LLM + Population) activation by:\n\n1. **Unblocking LLM Innovation**: Fixes 4 critical bugs preventing diversity-aware prompting from activating\n2. **Enabling Structural Innovation**: Allows LLM-generated strategies to execute successfully in Docker sandbox (currently 0% → target 90%)\n3. **Maintaining System Reliability**: Establishes testing infrastructure to prevent regression during Stage 2 activation\n4. **Accelerating Development**: Reduces debugging time through systematic boundary validation and diagnostic logging\n\n**Current Blocker**: Without this framework, the validated 90% LLM generation success rate (2025-10-30) cannot be realized because all generated strategies fail during Docker execution.\n\n## Requirements\n\n### Requirement 1: Fix Critical Docker Execution Failure\n\n**User Story:** As a system operator, I want all Docker-executed strategies to run without SyntaxError, so that the autonomous learning loop can evaluate LLM-generated strategies.\n\n#### Acceptance Criteria\n\n1. WHEN the system assembles code for Docker execution THEN the system SHALL evaluate f-string templates before file write\n2. WHEN Docker receives the complete code THEN the code SHALL contain no `{{}}` double-brace syntax\n3. WHEN Docker executes the code THEN the exit code SHALL be 0 for valid strategies\n4. WHEN execution completes successfully THEN the system SHALL extract metrics from the container\n\n**Priority**: CRITICAL (hard blocker for all Docker executions)\n**Current State**: 100% failure rate (exit code 1)\n**Target State**: >80% execution success rate\n\n### Requirement 2: Fix LLM API Routing Configuration\n\n**User Story:** As a system operator, I want LLM model names to route to correct provider endpoints, so that diversity-aware prompting can activate.\n\n#### Acceptance Criteria\n\n1. WHEN config specifies provider='google' and model='anthropic/claude-3.5-sonnet' THEN the system SHALL raise ValueError\n2. WHEN config specifies provider='google' THEN the system SHALL only accept gemini-* models\n3. WHEN config specifies provider='openrouter' THEN the system SHALL accept anthropic/* models\n4. WHEN LLM initialization fails validation THEN the system SHALL log clear error message with correct provider\n\n**Priority**: CRITICAL (blocks diversity-aware prompting)\n**Current State**: anthropic model sent to Google API → 404 error → fallback to Factor Graph\n**Target State**: Correct routing for all model/provider combinations\n\n### Requirement 3: Establish Integration Boundary Validation\n\n**User Story:** As a developer, I want automated tests at every integration boundary, so that data flows are verified before production.\n\n#### Acceptance Criteria\n\n1. WHEN code flows from LLM generation to Docker execution THEN integration tests SHALL verify code validity\n2. WHEN Docker executes code THEN integration tests SHALL verify metrics extraction works\n3. WHEN LLM config is parsed THEN integration tests SHALL verify API routing is correct\n4. IF any integration boundary test fails THEN the system SHALL prevent deployment\n\n**Priority**: HIGH (prevents future integration failures)\n**Current State**: ZERO checkpoint validation\n**Target State**: 100% integration boundary coverage\n\n### Requirement 4: Add Diagnostic Instrumentation\n\n**User Story:** As a developer, I want diagnostic logging at integration boundaries, so that I can debug integration failures quickly.\n\n#### Acceptance Criteria\n\n1. WHEN LLM is initialized THEN the system SHALL log provider name and model being used\n2. WHEN code is assembled for Docker THEN the system SHALL log first 500 chars of complete code\n3. WHEN Docker execution completes THEN the system SHALL log full result structure\n4. WHEN integration tests run THEN tests SHALL verify diagnostic logs are generated\n\n**Priority**: MEDIUM (improves debugging efficiency)\n**Current State**: No diagnostic logging at boundaries\n**Target State**: All boundaries instrumented with debug-level logs\n\n### Requirement 5: Fix Exception Handling State Propagation\n\n**User Story:** As a system operator, I want exceptions to trigger diversity fallback, so that the system can recover from LLM failures.\n\n#### Acceptance Criteria\n\n1. WHEN Docker execution raises an exception THEN the system SHALL set last_result = False\n2. WHEN last_result is False THEN the next iteration SHALL use diversity LLM model\n3. WHEN diversity fallback activates THEN the system SHALL log the fallback event\n4. IF fallback succeeds THEN the system SHALL continue autonomous loop\n\n**Priority**: MEDIUM (enables recovery mechanism)\n**Current State**: Exceptions don't update state → diversity never triggers\n**Target State**: Automatic diversity fallback on failures\n\n### Requirement 6: Create Missing Configuration Module\n\n**User Story:** As a system operator, I want configuration snapshots to be captured, so that experiments are reproducible.\n\n#### Acceptance Criteria\n\n1. WHEN the system imports src.config.experiment_config THEN the import SHALL succeed\n2. WHEN a configuration snapshot is needed THEN ExperimentConfig.from_dict() SHALL create an instance\n3. WHEN configuration is serialized THEN ExperimentConfig.to_dict() SHALL return a dictionary\n4. IF the module doesn't exist THEN unit tests SHALL fail\n\n**Priority**: LOW (non-fatal but breaks config tracking)\n**Current State**: Module doesn't exist → import fails every iteration\n**Target State**: Module exists with minimal implementation\n\n## Non-Functional Requirements\n\n### Code Architecture and Modularity\n- **Test Isolation**: All integration tests must use mocks for external dependencies (LLM APIs, Docker execution)\n- **Hermetic Testing**: Tests must not depend on external state or network access\n- **Clear Test Organization**: Separate unit tests (`tests/unit/`) from integration tests (`tests/integration/`)\n- **Fixture Reusability**: Use pytest fixtures for common test setup (temp directories, mock configs)\n\n### Performance\n- **Test Execution Speed**: Unit tests SHALL complete in <1 second each\n- **Integration Test Speed**: Integration tests SHALL complete in <5 seconds each\n- **CI/CD Impact**: Full test suite SHALL complete in <2 minutes\n- **Production Impact**: Checkpoint validation SHALL add <50ms overhead per iteration\n\n### Security\n- **No Secrets in Tests**: Test configurations SHALL NOT contain real API keys\n- **Sandbox Validation**: Docker integration tests SHALL verify read-only filesystem is enforced\n- **Code Injection Prevention**: Tests SHALL verify no arbitrary code execution in templates\n\n### Reliability\n- **Test Flakiness**: Zero tolerance for flaky tests (must be deterministic)\n- **Backward Compatibility**: Bug fixes SHALL NOT break existing direct-execution mode\n- **Regression Prevention**: All fixed bugs SHALL have permanent unit tests\n- **Coverage**: Integration boundary tests SHALL cover 100% of identified boundaries\n\n### Usability\n- **Clear Error Messages**: Test failures SHALL identify exact integration point that failed\n- **Test Documentation**: Each test SHALL have docstring explaining what it validates\n- **Debugging Support**: Failed tests SHALL output diagnostic information (logs, intermediate state)\n- **Developer Experience**: Running tests SHALL require zero manual setup (all mocked)\n",
  "fileStats": {
    "size": 8008,
    "lines": 142,
    "lastModified": "2025-11-01T22:44:50.606Z"
  },
  "comments": []
}