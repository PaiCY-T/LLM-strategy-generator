{
  "id": "snapshot_1762040588632_g52vi82lm",
  "approvalId": "approval_1762040576340_o6jly5gnj",
  "approvalTitle": "Tasks Document - Test-First Implementation (20 Tasks, 6 Phases)",
  "version": 2,
  "timestamp": "2025-11-01T23:43:08.632Z",
  "trigger": "approved",
  "status": "pending",
  "content": "# Tasks Document\n\n## Execution Strategy\n\n**Test-First Approach**: Write failing tests first, then implement fixes to make tests pass.\n\n**Order of Execution**:\n1. **Phase 1**: Characterization Test (establish baseline)\n2. **Phase 2**: Unit Tests (test fixes in isolation)\n3. **Phase 3**: Bug Fixes (make unit tests pass)\n4. **Phase 4**: Integration Tests (test boundaries)\n5. **Phase 5**: E2E Test (test full flow)\n6. **Phase 6**: Validation (verify success criteria)\n\n## Phase 1: Characterization Testing\n\n### Task 1.1: Create Characterization Test\n\n- [ ] 1.1 Create characterization test to establish baseline behavior\n  - **File**: `tests/integration/test_characterization_baseline.py`\n  - **Purpose**: Document current system behavior before ANY changes (even if wrong)\n  - **Leverage**: pytest framework, existing test fixtures\n  - **Requirements**: All (establishes baseline for all requirements)\n  - **Acceptance Criteria**:\n    - Test runs successfully and documents current behavior\n    - Checks for f-string evaluation ({{}} presence in Docker code)\n    - Tests LLM API validation (or lack thereof)\n    - Verifies config module import behavior\n    - Checks exception state propagation\n  - **Expected Outcome**: Test may fail (system is broken), which is OK\n  - **Estimate**: 1 hour\n  - _Prompt: Role: Test Engineer specializing in characterization testing and pytest | Task: Create a comprehensive characterization test that documents current system behavior across all 4 integration bugs, even if that behavior is incorrect. The test should capture actual behavior for f-string evaluation, LLM API routing, config import, and exception handling | Restrictions: Do not fix bugs, only document current behavior. Test must run successfully even if it documents failure scenarios | Success: Test executes and produces clear documentation of current behavior for all 4 bug areas, serves as baseline for future comparison_\n\n## Phase 2: Unit Tests (Test-First)\n\n### Task 2.1: Unit Tests for LLM API Validation (Bug #2)\n\n- [ ] 2.1 Write unit tests for LLM API routing validation\n  - **File**: `tests/unit/test_llm_validation.py`\n  - **Purpose**: Test model/provider validation logic in isolation\n  - **Leverage**: pytest, pytest.raises for exception testing\n  - **Requirements**: R2 (Fix LLM API Routing Configuration)\n  - **Test Cases**:\n    - `test_google_provider_rejects_anthropic_model()` - Should raise ValueError\n    - `test_google_provider_accepts_gemini_model()` - Should not raise\n    - `test_google_provider_rejects_non_gemini_model()` - Should raise ValueError\n    - `test_openrouter_provider_accepts_anthropic_model()` - Should not raise\n  - **Expected Outcome**: All tests FAIL initially (validation function doesn't exist yet)\n  - **Estimate**: 30 minutes\n  - _Prompt: Role: Unit Test Specialist with expertise in pytest and test-driven development | Task: Write comprehensive unit tests for LLM API routing validation following requirement R2. Tests should cover valid and invalid model/provider combinations, using pytest.raises for exception testing | Restrictions: Tests must fail initially (function doesn't exist), focus on behavior specification not implementation | Success: 4+ unit tests written covering all validation scenarios, tests clearly specify expected behavior, all tests fail with \"function not found\" error_\n\n### Task 2.2: Unit Tests for ExperimentConfig (Bug #3)\n\n- [ ] 2.2 Write unit tests for ExperimentConfig module\n  - **File**: `tests/unit/test_experiment_config.py`\n  - **Purpose**: Test configuration serialization/deserialization\n  - **Leverage**: pytest, dataclasses\n  - **Requirements**: R6 (Create Missing Configuration Module)\n  - **Test Cases**:\n    - `test_experiment_config_creation()` - Create instance\n    - `test_experiment_config_to_dict()` - Serialize to dict\n    - `test_experiment_config_from_dict()` - Deserialize from dict\n    - `test_experiment_config_round_trip()` - Serialize + deserialize\n  - **Expected Outcome**: All tests FAIL initially (module doesn't exist)\n  - **Estimate**: 30 minutes\n  - _Prompt: Role: Python Developer with expertise in dataclasses and serialization testing | Task: Write unit tests for ExperimentConfig module following requirement R6. Tests should cover creation, serialization, deserialization, and round-trip scenarios | Restrictions: Module doesn't exist yet, tests must specify expected interface and behavior | Success: 4 unit tests written covering all config operations, tests fail with import error, interface is clearly specified_\n\n## Phase 3: Bug Fixes (Make Tests Pass)\n\n### Task 3.1: Implement LLM API Routing Validation (Bug #2)\n\n- [ ] 3.1 Implement LLM API routing validation function\n  - **File**: `src/innovation/llm_strategy_generator.py`\n  - **Purpose**: Add validation function to prevent mismatched model/provider\n  - **Leverage**: Existing LLM generator code\n  - **Requirements**: R2 (Fix LLM API Routing Configuration)\n  - **Implementation**:\n    - Add `_validate_model_provider_match(provider, model)` function\n    - Call validation in `__init__()` method before API initialization\n    - Raise ValueError with clear error message on mismatch\n  - **Acceptance Criteria**:\n    - Unit tests from Task 2.1 now PASS\n    - Function validates Google provider requires gemini-* models\n    - Function rejects anthropic/* models with Google provider\n    - Clear error messages guide user to correct configuration\n  - **Estimate**: 30 minutes\n  - _Prompt: Role: Backend Developer with expertise in Python validation and error handling | Task: Implement LLM API routing validation function in llm_strategy_generator.py following requirement R2 and design.md Component 2. Make unit tests from Task 2.1 pass | Restrictions: Minimal changes only, add validation function and call it in __init__, do not refactor existing code | Success: All unit tests from Task 2.1 pass, validation prevents mismatched configurations, error messages are clear and actionable_\n\n### Task 3.2: Create ExperimentConfig Module (Bug #3)\n\n- [ ] 3.2 Create ExperimentConfig module with minimal implementation\n  - **File**: `src/config/experiment_config.py` (NEW FILE)\n  - **Purpose**: Fix import error and enable config snapshots\n  - **Leverage**: Python dataclasses, typing module\n  - **Requirements**: R6 (Create Missing Configuration Module)\n  - **Implementation**:\n    - Create `@dataclass` class ExperimentConfig\n    - Add fields: iteration, config_snapshot, timestamp\n    - Implement `from_dict()` class method\n    - Implement `to_dict()` instance method\n  - **Acceptance Criteria**:\n    - Unit tests from Task 2.2 now PASS\n    - Module can be imported without error\n    - Round-trip serialization works correctly\n    - Minimal implementation (<30 lines)\n  - **Estimate**: 30 minutes\n  - _Prompt: Role: Python Developer specializing in dataclasses and configuration management | Task: Create minimal ExperimentConfig module in src/config/experiment_config.py following requirement R6 and design.md Component 3. Make unit tests from Task 2.2 pass | Restrictions: Minimal implementation only (<30 lines), use dataclasses, do not add features beyond requirements | Success: All unit tests from Task 2.2 pass, import succeeds, serialization works, implementation is minimal and clean_\n\n### Task 3.3: Fix Exception State Propagation (Bug #4)\n\n- [ ] 3.3 Fix exception handler to update last_result state\n  - **File**: `artifacts/working/modules/autonomous_loop.py`\n  - **Location**: Line ~106-113 (exception handler)\n  - **Purpose**: Enable diversity fallback after failures\n  - **Leverage**: Existing autonomous_loop.py exception handling\n  - **Requirements**: R5 (Fix Exception Handling State Propagation)\n  - **Implementation**:\n    - Add `self.last_result = False` in exception handler\n    - Add INFO log: \"Setting last_result=False to enable diversity fallback\"\n    - Verify next iteration uses diversity LLM model\n  - **Acceptance Criteria**:\n    - Exception handler sets `last_result = False`\n    - State change is logged clearly\n    - Next iteration after exception triggers diversity mode\n    - No other code changes (2-3 lines total)\n  - **Estimate**: 15 minutes\n  - _Prompt: Role: Python Developer with expertise in state management and exception handling | Task: Fix exception handler in autonomous_loop.py to update last_result state following requirement R5 and design.md Component 4. Add 2-3 lines only | Restrictions: Minimal change only, add state update and log message, do not refactor exception handling | Success: Exception updates state correctly, log message is clear, diversity fallback works in next iteration_\n\n### Task 3.4: Investigate and Fix F-String Evaluation (Bug #1)\n\n- [ ] 3.4 Investigate f-string template evaluation issue\n  - **File**: `artifacts/working/modules/autonomous_loop.py`\n  - **Location**: Line ~202-344 (data_setup template and code assembly)\n  - **Purpose**: Ensure {{}} is resolved to {} before Docker execution\n  - **Leverage**: Existing data_setup template, autonomous_loop.py\n  - **Requirements**: R1 (Fix Critical Docker Execution Failure)\n  - **Investigation Steps**:\n    1. Add diagnostic logging before file write (first 500 chars)\n    2. Check if `data_setup` contains {{}} or {}\n    3. Verify file written to Docker contains correct syntax\n    4. If {{}} present, investigate why f-string not evaluated\n  - **Acceptance Criteria**:\n    - Diagnostic logging shows complete_code content\n    - {{}} is resolved to {} before Docker execution\n    - Manual test of generated code succeeds in Docker\n    - Root cause documented\n  - **Estimate**: 30 minutes (investigation) + 30 minutes (fix if needed)\n  - _Prompt: Role: Python Developer with debugging expertise and f-string knowledge | Task: Investigate and fix f-string template evaluation issue following requirement R1 and design.md Component 1. Add diagnostic logging first, then fix root cause | Restrictions: Add logging first to diagnose, then minimal fix only, do not refactor data_setup template | Success: Diagnostic logs show code content, {{}} resolved correctly, Docker execution succeeds, fix is minimal (likely <5 lines)_\n\n## Phase 4: Integration Tests\n\n### Task 4.1: Integration Test for F-String Evaluation (Bug #1)\n\n- [ ] 4.1 Write integration test for f-string evaluation boundary\n  - **File**: `tests/integration/test_docker_integration_boundaries.py`\n  - **Purpose**: Verify complete_code has correct syntax before Docker\n  - **Leverage**: pytest, tmp_path fixture, mock Docker client\n  - **Requirements**: R1, R3 (Fix Bug #1 + Integration Boundary Validation)\n  - **Test Implementation**:\n    - Mock autonomous loop with real data_setup template\n    - Execute code assembly\n    - Read file that would be sent to Docker\n    - Assert: No {{}} in file content\n    - Assert: Code is syntactically valid (exec() succeeds)\n  - **Acceptance Criteria**:\n    - Test verifies f-string evaluation at integration boundary\n    - Test fails before Bug #1 fix, passes after\n    - Test uses real template (not simplified mock)\n    - Test catches regression if f-string breaks again\n  - **Estimate**: 45 minutes\n  - _Prompt: Role: Integration Test Engineer with pytest and mocking expertise | Task: Write integration test for f-string evaluation boundary following requirements R1 and R3. Test should use real data_setup template and verify code assembly produces valid Python | Restrictions: Use mocks for Docker execution, test real template not simplified version, verify actual file content | Success: Test verifies f-string evaluation works, uses real template, catches regressions, passes after Bug #1 fix_\n\n### Task 4.2: Integration Test for Exception State Propagation (Bug #4)\n\n- [ ] 4.2 Write integration test for exception handling boundary\n  - **File**: `tests/integration/test_docker_integration_boundaries.py`\n  - **Purpose**: Verify exceptions trigger diversity fallback\n  - **Leverage**: pytest, mock Docker client, mock LLM client\n  - **Requirements**: R5, R3 (Fix Bug #4 + Integration Boundary Validation)\n  - **Test Implementation**:\n    - Mock Docker to raise exception\n    - Run iteration 1 (should fail and set last_result=False)\n    - Run iteration 2 (should use diversity LLM)\n    - Verify: last_result is False after exception\n    - Verify: diversity model used in iteration 2\n  - **Acceptance Criteria**:\n    - Test simulates Docker failure and verifies state update\n    - Test verifies diversity fallback activates\n    - Test uses real autonomous loop (not simplified)\n    - Test passes after Bug #4 fix\n  - **Estimate**: 45 minutes\n  - _Prompt: Role: Integration Test Engineer with expertise in state testing and mocking | Task: Write integration test for exception state propagation following requirements R5 and R3. Test should simulate failure and verify diversity activation | Restrictions: Mock Docker and LLM clients, test real autonomous loop behavior, verify both state and model selection | Success: Test verifies exception sets state correctly, diversity fallback activates, test uses real loop, passes after Bug #4 fix_\n\n### Task 4.3: Add Diagnostic Instrumentation\n\n- [ ] 4.3 Add diagnostic logging at all integration boundaries\n  - **Files**:\n    - `src/innovation/llm_strategy_generator.py` (LLM initialization)\n    - `artifacts/working/modules/autonomous_loop.py` (code assembly, exception)\n    - `src/sandbox/docker_executor.py` (Docker result)\n  - **Purpose**: Enable quick debugging of integration failures\n  - **Leverage**: Python logging module (existing loggers)\n  - **Requirements**: R4 (Add Diagnostic Instrumentation)\n  - **Implementation**:\n    - LLM init: DEBUG log with provider and model\n    - Code assembly: DEBUG log with first 500 chars\n    - Docker result: DEBUG log with full result structure\n    - Exception: INFO log with state change\n  - **Acceptance Criteria**:\n    - 4 diagnostic log statements added (DEBUG or INFO level)\n    - Logs don't spam (DEBUG level for verbose, INFO for important)\n    - Integration tests verify logs are generated\n    - Total addition: ~10 lines across 3 files\n  - **Estimate**: 30 minutes\n  - _Prompt: Role: DevOps Engineer with logging and observability expertise | Task: Add diagnostic logging at integration boundaries following requirement R4 and design.md diagnostic section. Add 4 log statements across 3 files | Restrictions: Use existing loggers, DEBUG for verbose INFO for state changes, do not add more than specified, keep messages concise | Success: 4 diagnostic logs added, appropriate levels used, integration tests verify logs, debugging is easier_\n\n## Phase 5: End-to-End Testing\n\n### Task 5.1: Create E2E Test for Full Integration Flow\n\n- [ ] 5.1 Write end-to-end test for complete Docker integration\n  - **File**: `tests/integration/test_docker_integration_e2e.py`\n  - **Purpose**: Verify complete flow from code generation to metrics extraction\n  - **Leverage**: pytest, mock LLM and Docker clients\n  - **Requirements**: All (R1-R6)\n  - **Test Implementation**:\n    - Mock LLM to return known-good strategy code\n    - Run full autonomous loop iteration\n    - Verify: complete_code has no {{}}\n    - Verify: Docker receives valid Python\n    - Verify: Metrics extraction works (if Docker succeeds)\n    - Verify: Diagnostic logs present at each boundary\n  - **Acceptance Criteria**:\n    - Test exercises full flow from LLM → Docker → Metrics\n    - Test uses real code paths (mocks only external APIs)\n    - Test verifies all 4 bug fixes work together\n    - Test serves as integration smoke test\n  - **Estimate**: 1 hour\n  - _Prompt: Role: QA Engineer with end-to-end testing and system integration expertise | Task: Write comprehensive E2E test for full Docker integration flow following all requirements (R1-R6). Test should exercise real code paths with mocked external dependencies | Restrictions: Mock only LLM and Docker APIs, use real code assembly and validation logic, verify all integration boundaries | Success: E2E test covers full flow, verifies all bug fixes work together, serves as smoke test, passes after all fixes implemented_\n\n## Phase 6: Validation and Completion\n\n### Task 6.1: Run Full Test Suite and Verify Coverage\n\n- [ ] 6.1 Execute full test suite and verify all tests pass\n  - **Command**: `pytest tests/ -v --cov=src --cov=artifacts/working/modules`\n  - **Purpose**: Verify all bug fixes and tests are working\n  - **Leverage**: pytest, pytest-cov\n  - **Requirements**: R3 (Establish Integration Boundary Validation)\n  - **Acceptance Criteria**:\n    - Characterization test updated and passing\n    - All unit tests passing (Tasks 2.1, 2.2)\n    - All integration tests passing (Tasks 4.1, 4.2)\n    - E2E test passing (Task 5.1)\n    - Coverage >90% for modified code\n    - Total test execution time <30 seconds\n  - **Estimate**: 30 minutes\n  - _Prompt: Role: QA Lead with test automation and coverage analysis expertise | Task: Run full test suite and verify coverage following requirement R3. Ensure all tests pass and coverage meets targets | Restrictions: All tests must pass, coverage >90% for modified code, execution time <30s | Success: Full test suite passes, coverage targets met, execution is fast, no flaky tests_\n\n### Task 6.2: Execute System Validation Test (30 Iterations)\n\n- [ ] 6.2 Run 30-iteration pilot test to verify success criteria\n  - **Script**: Create `run_integration_validation.py`\n  - **Purpose**: Verify Requirement 7 success criteria are met\n  - **Leverage**: Existing pilot test infrastructure\n  - **Requirements**: R7 (Definition of Done & Handoff Conditions)\n  - **Validation Points**:\n    - Docker execution success rate >80%\n    - Diversity-aware prompting activates ≥30% of eligible iterations\n    - No regression in direct-execution mode\n    - Config snapshots saved successfully\n  - **Acceptance Criteria**:\n    - Success rate >80% for 30+ iterations\n    - Diversity activates after failures (measured via logs)\n    - All metrics collected successfully\n    - No import warnings or errors\n  - **Estimate**: 1 hour (test run) + 30 minutes (analysis)\n  - _Prompt: Role: System Validation Engineer with performance testing and metrics analysis expertise | Task: Create and execute 30-iteration validation test following requirement R7 success criteria. Measure success rate, diversity activation, and system stability | Restrictions: Must run 30+ iterations, measure all success criteria, document results with evidence | Success: >80% success rate achieved, diversity activates ≥30%, no regressions, all criteria met with documented evidence_\n\n### Task 6.3: Document Maintenance Difficulties for Future Refactoring\n\n- [ ] 6.3 Create maintenance difficulties report\n  - **File**: `docs/MAINTENANCE_DIFFICULTIES.md`\n  - **Purpose**: Provide evidence for future refactoring decisions\n  - **Leverage**: Experience from bug fixes and testing\n  - **Requirements**: R7 (Handoff to Refactoring Work)\n  - **Documentation**:\n    - Files that were difficult to modify (why?)\n    - Code comprehension challenges encountered\n    - Performance bottlenecks observed\n    - Change frequency and bug density in modules\n  - **Acceptance Criteria**:\n    - Report documents specific pain points with examples\n    - Recommendations are evidence-based (not opinions)\n    - Potential refactoring candidates prioritized by impact\n    - Report informs future `autonomous-loop-refactoring` spec\n  - **Estimate**: 1 hour\n  - _Prompt: Role: Senior Developer with code quality and refactoring expertise | Task: Document maintenance difficulties encountered during bug fixes following requirement R7 handoff section. Provide evidence-based analysis for future refactoring decisions | Restrictions: Focus on observed difficulties not hypothetical issues, provide specific examples, prioritize by actual impact | Success: Report documents real pain points with evidence, recommendations are actionable, prioritization is clear, informs future spec_\n\n### Task 6.4: Update Characterization Test to Verify Correct Behavior\n\n- [ ] 6.4 Update characterization test to document new baseline\n  - **File**: `tests/integration/test_characterization_baseline.py`\n  - **Purpose**: Document corrected behavior as new baseline\n  - **Leverage**: Existing characterization test from Task 1.1\n  - **Requirements**: All (establishes new baseline after all fixes)\n  - **Updates**:\n    - Change expectations from broken to correct behavior\n    - Document: f-string evaluation works (no {{}} in Docker code)\n    - Document: LLM API validation works (rejects mismatches)\n    - Document: Config module imports successfully\n    - Document: Exceptions trigger diversity fallback\n  - **Acceptance Criteria**:\n    - Test passes with new correct behavior\n    - Test serves as regression prevention\n    - Test documents all 4 bug fixes working\n    - Test is updated with comments explaining changes\n  - **Estimate**: 30 minutes\n  - _Prompt: Role: Test Engineer with regression testing expertise | Task: Update characterization test from Task 1.1 to document corrected behavior as new baseline. Test should now verify fixes work correctly | Restrictions: Update expectations not test structure, add comments explaining changes, keep test comprehensive | Success: Test passes with correct behavior, prevents regressions, documents all fixes, serves as new baseline_\n\n## Completion Checklist (Requirement 7)\n\nBefore marking this spec as COMPLETE, verify ALL conditions:\n\n- [ ] ✅ All 4 critical bugs fixed (R1, R2, R5, R6) - Tasks 3.1-3.4\n- [ ] ✅ Test framework established and integrated into CI (R3) - Tasks 1.1, 2.1-2.2, 4.1-4.2, 5.1\n- [ ] ✅ Diagnostic instrumentation in place (R4) - Task 4.3\n- [ ] ✅ Characterization test passes (validates baseline behavior) - Tasks 1.1, 6.4\n- [ ] ✅ System execution success rate >80% for 30+ consecutive iterations - Task 6.2\n- [ ] ✅ Diversity-aware prompting activates ≥30% of eligible iterations - Task 6.2\n- [ ] ✅ No regression in direct-execution mode - Task 6.1, 6.2\n- [ ] ✅ Maintenance difficulties observed and documented - Task 6.3\n\n**After ALL conditions met**: This spec is COMPLETE. Evaluate whether to create `autonomous-loop-refactoring` spec.\n\n## Estimated Timeline\n\n- **Phase 1** (Characterization): 1 hour\n- **Phase 2** (Unit Tests): 1 hour\n- **Phase 3** (Bug Fixes): 2 hours\n- **Phase 4** (Integration Tests): 2 hours\n- **Phase 5** (E2E Test): 1 hour\n- **Phase 6** (Validation): 3 hours\n\n**Total**: ~10 hours (vs original estimate 5.5 hours, difference is validation phase)\n\n## Dependencies\n\n- Python 3.8+\n- pytest, pytest-cov\n- Docker (for E2E testing)\n- Existing project dependencies (docker-py, etc.)\n\n## Success Metrics\n\n- All tests passing (unit, integration, E2E)\n- Test coverage >90% for modified code\n- Docker execution success rate >80%\n- Diversity activation rate ≥30%\n- Zero import warnings\n- Total bug fix code: ~60 lines\n- Total test code: ~400 lines\n",
  "fileStats": {
    "size": 22841,
    "lines": 363,
    "lastModified": "2025-11-01T23:42:41.521Z"
  },
  "comments": []
}