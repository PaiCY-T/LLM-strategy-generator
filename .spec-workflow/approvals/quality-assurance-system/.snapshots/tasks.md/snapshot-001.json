{
  "id": "snapshot_1762442290826_2revxo0qo",
  "approvalId": "approval_1762442290642_frhrpst52",
  "approvalTitle": "Quality Assurance System - Tasks",
  "version": 1,
  "timestamp": "2025-11-06T15:18:10.826Z",
  "trigger": "initial",
  "status": "pending",
  "content": "# Quality Assurance System - Implementation Tasks\n\n## Task Organization\n\nTasks are organized into 3 sequential phases with clear dependencies and checkpoints.\n\n```\nPhase 1: Foundation (Day 1)\n  ├─> Protocol Interfaces\n  ├─> mypy Configuration\n  └─> Checkpoint: mypy runs successfully\n\nPhase 2: Type Hints (Day 2)\n  ├─> Learning System Types\n  ├─> Backtest Executor Types\n  ├─> Repository Types\n  └─> Checkpoint: mypy passes on all target modules\n\nPhase 3: CI Integration (Day 3)\n  ├─> GitHub Actions Workflow\n  ├─> E2E Smoke Tests\n  └─> Checkpoint: CI green on all commits\n```\n\n---\n\n## Phase 1: Foundation (Day 1)\n\n### TASK-QA-001: Create Protocol Interfaces Module\n**Priority**: P0 (Blocker)\n**Estimated Time**: 2 hours\n**Dependencies**: None\n\n**Objective**: Define structural type contracts for all component boundaries\n\n**Steps**:\n1. Create `src/interfaces.py` file\n2. Define 8 Protocol interfaces:\n   - `HistoryProvider` (iteration history management)\n   - `BacktestExecutor` (strategy backtesting)\n   - `ChampionTracker` (best strategy tracking)\n   - `FeedbackGenerator` (LLM feedback generation)\n   - `SuccessClassifier` (iteration classification)\n   - `HallOfFameRepository` (champion persistence)\n   - `IterationExecutor` (single iteration execution)\n   - `LearningLoop` (main learning orchestrator)\n\n3. Document each Protocol with:\n   - Docstring explaining purpose\n   - Method signatures with type hints\n   - Parameter descriptions\n\n**Implementation Template**:\n```python\n# src/interfaces.py\n\nfrom typing import Protocol, Optional\nfrom dataclasses import dataclass\nimport pandas as pd\n\n@dataclass\nclass IterationRecord:\n    \"\"\"Represents a single learning iteration\"\"\"\n    iteration_num: int\n    strategy_code: str\n    metrics: dict[str, float]\n    classification_level: Optional[str] = None\n    champion_updated: bool = False\n\n@dataclass\nclass BacktestResult:\n    \"\"\"Results from backtesting a strategy\"\"\"\n    sharpe_ratio: float\n    total_return: float\n    max_drawdown: float\n    annual_return: float\n\nclass HistoryProvider(Protocol):\n    \"\"\"Contract for components that provide iteration history\"\"\"\n\n    def save(self, record: IterationRecord) -> None:\n        \"\"\"Save iteration record to history\"\"\"\n        ...\n\n    def load_all(self) -> list[IterationRecord]:\n        \"\"\"Load all iteration records\"\"\"\n        ...\n\n    def get_champion(self) -> Optional[IterationRecord]:\n        \"\"\"Get current champion strategy\"\"\"\n        ...\n\nclass BacktestExecutor(Protocol):\n    \"\"\"Contract for strategy backtesting components\"\"\"\n\n    def execute(\n        self,\n        strategy_code: str,\n        data: pd.DataFrame\n    ) -> BacktestResult:\n        \"\"\"Execute backtest for given strategy\"\"\"\n        ...\n\n# ... (continue for all 8 Protocols)\n```\n\n**Acceptance Criteria**:\n- [ ] All 8 Protocol interfaces defined\n- [ ] Each Protocol has complete docstrings\n- [ ] Method signatures match actual usage patterns\n- [ ] File imports without errors\n- [ ] No runtime dependencies (typing only)\n\n**Validation**:\n```bash\npython -c \"from src.interfaces import *\"  # Should import cleanly\nmypy src/interfaces.py                     # Should pass\n```\n\n---\n\n### TASK-QA-002: Configure mypy for Project\n**Priority**: P0 (Blocker)\n**Estimated Time**: 1 hour\n**Dependencies**: TASK-QA-001\n\n**Objective**: Set up mypy with appropriate strictness for gradual typing\n\n**Steps**:\n1. Create `mypy.ini` in project root\n2. Configure lenient settings for initial adoption\n3. Specify target modules for type checking\n4. Add third-party library ignores\n\n**Configuration File**:\n```ini\n# mypy.ini\n\n[mypy]\n# Python version\npython_version = 3.10\n\n# Target modules for type checking\nfiles = src/learning, src/backtest, src/repository\n\n# Lenient settings for gradual adoption\ndisallow_untyped_defs = False        # Allow untyped function definitions\ndisallow_incomplete_defs = False     # Allow incomplete type annotations\nwarn_return_any = False              # Don't warn on 'Any' returns\nwarn_unused_ignores = True           # Warn on unnecessary '# type: ignore'\n\n# Error reporting\nshow_error_codes = True              # Show error codes (e.g., [arg-type])\npretty = True                        # Colorful output\nshow_column_numbers = True           # Show column in errors\n\n# Third-party library ignores\n[mypy-finlab.*]\nignore_missing_imports = True\n\n[mypy-pandas.*]\nignore_missing_imports = True\n\n[mypy-numpy.*]\nignore_missing_imports = True\n\n[mypy-matplotlib.*]\nignore_missing_imports = True\n\n# Future: Can tighten these settings gradually\n# disallow_untyped_defs = True\n# disallow_incomplete_defs = True\n# warn_return_any = True\n```\n\n**Acceptance Criteria**:\n- [ ] `mypy.ini` created in project root\n- [ ] mypy runs without configuration errors\n- [ ] Third-party imports don't cause failures\n- [ ] Configuration documented in file\n\n**Validation**:\n```bash\nmypy --version  # Ensure mypy ≥ 1.18.0 installed\nmypy src/interfaces.py  # Should pass (baseline)\n```\n\n---\n\n### TASK-QA-003: Add Type Hints to IterationHistory\n**Priority**: P0 (Blocker)\n**Estimated Time**: 1.5 hours\n**Dependencies**: TASK-QA-001, TASK-QA-002\n\n**Objective**: Type the first module as proof-of-concept\n\n**Steps**:\n1. Open `src/learning/iteration_history.py`\n2. Import necessary types from `src.interfaces`\n3. Add type hints to all public methods\n4. Add type hints to `__init__` parameters\n5. Update dataclass fields with types (if not already)\n6. Run mypy and fix any errors\n\n**Example**:\n```python\n# src/learning/iteration_history.py\n\nfrom typing import Optional\nfrom src.interfaces import IterationRecord, HistoryProvider\n\nclass IterationHistory(HistoryProvider):  # Implements Protocol\n    \"\"\"Manages iteration history persistence\"\"\"\n\n    def __init__(self, file_path: str) -> None:\n        self.file_path = file_path\n        self.records: list[IterationRecord] = []\n\n    def save(self, record: IterationRecord) -> None:\n        \"\"\"Save iteration record to JSON file\"\"\"\n        self.records.append(record)\n        # ... implementation\n\n    def load_all(self) -> list[IterationRecord]:\n        \"\"\"Load all records from JSON file\"\"\"\n        # ... implementation\n        return self.records\n\n    def get_champion(self) -> Optional[IterationRecord]:\n        \"\"\"Get best performing strategy\"\"\"\n        if not self.records:\n            return None\n        return max(self.records, key=lambda r: r.metrics[\"sharpe_ratio\"])\n```\n\n**Common Issues to Fix**:\n\n**Issue 1: Parameter name mismatch**\n```python\n# Before (Phase 8 error)\ndef __init__(self, filepath: str):  # Wrong parameter name\n    ...\n\n# After\ndef __init__(self, file_path: str):  # Matches call sites\n    ...\n```\n\n**Issue 2: Missing return type**\n```python\n# Before\ndef get_champion(self):\n    ...\n\n# After\ndef get_champion(self) -> Optional[IterationRecord]:\n    ...\n```\n\n**Acceptance Criteria**:\n- [ ] All public methods have type hints\n- [ ] Parameter names match actual usage\n- [ ] Return types specified\n- [ ] mypy passes on this module\n- [ ] Implements `HistoryProvider` Protocol\n\n**Validation**:\n```bash\nmypy src/learning/iteration_history.py  # 0 errors\npython -m pytest tests/learning/test_iteration_history.py  # All pass\n```\n\n---\n\n### TASK-QA-004: Checkpoint - Foundation Complete\n**Priority**: P0 (Blocker)\n**Estimated Time**: 30 minutes\n**Dependencies**: TASK-QA-001, TASK-QA-002, TASK-QA-003\n\n**Objective**: Validate foundation is solid before proceeding\n\n**Validation Steps**:\n1. Run mypy on interfaces module\n2. Run mypy on iteration_history module\n3. Run all existing tests (926 tests)\n4. Document any `# type: ignore` uses\n\n**Success Criteria**:\n- [ ] mypy passes on `src/interfaces.py`\n- [ ] mypy passes on `src/learning/iteration_history.py`\n- [ ] All 926 existing tests pass\n- [ ] Zero regressions introduced\n- [ ] Ready to proceed to Phase 2\n\n**Checkpoint Command**:\n```bash\n# Run comprehensive validation\nmypy src/interfaces.py src/learning/iteration_history.py\npytest tests/ -v\n```\n\n**Expected Output**:\n```\nmypy: Success: no issues found in 2 source files\npytest: 926 passed in X seconds\n```\n\n---\n\n## Phase 2: Type Hints on Public APIs (Day 2)\n\n### TASK-QA-005: Type Hints for Champion Tracker\n**Priority**: P0 (Blocker)\n**Estimated Time**: 1 hour\n**Dependencies**: TASK-QA-004\n\n**Objective**: Add types to champion management component\n\n**Target File**: `src/learning/champion_tracker.py`\n\n**Steps**:\n1. Import `ChampionTracker` Protocol from interfaces\n2. Add type hints to `__init__`\n3. Add type hints to `update_if_better()`\n4. Add type hints to `get_current_champion()`\n5. Fix parameter name issues (champion vs champion_tracker)\n\n**Key Fix** (Phase 8 error):\n```python\n# Before (Phase 8 error)\ndef update_if_better(self, champion: IterationRecord) -> bool:\n    # Parameter named 'champion' but called with champion_tracker\n    ...\n\n# After\ndef update_if_better(self, record: IterationRecord) -> bool:\n    # Clear parameter name\n    ...\n```\n\n**Acceptance Criteria**:\n- [ ] Implements `ChampionTracker` Protocol\n- [ ] All public methods typed\n- [ ] Parameter names consistent with call sites\n- [ ] mypy passes\n- [ ] Tests pass\n\n**Validation**:\n```bash\nmypy src/learning/champion_tracker.py\npytest tests/learning/test_champion_tracker.py\n```\n\n---\n\n### TASK-QA-006: Type Hints for Iteration Executor\n**Priority**: P0 (Blocker)\n**Estimated Time**: 1.5 hours\n**Dependencies**: TASK-QA-004\n\n**Objective**: Add types to iteration execution component\n\n**Target File**: `src/learning/iteration_executor.py`\n\n**Steps**:\n1. Import necessary Protocols (BacktestExecutor, SuccessClassifier, FeedbackGenerator)\n2. Add type hints to `__init__` parameters\n3. Add type hints to `execute_iteration()`\n4. Fix method signature issues (execute_code vs execute)\n\n**Key Fix** (Phase 8 error):\n```python\n# Before (Phase 8 error)\ndef execute_iteration(self, strategy: str) -> IterationRecord:\n    result = self.backtest.execute_code(strategy, self.data)  # Wrong method name\n    ...\n\n# After\ndef execute_iteration(self, strategy: str) -> IterationRecord:\n    result = self.backtest.execute(strategy, self.data)  # Correct method\n    ...\n```\n\n**Acceptance Criteria**:\n- [ ] Implements `IterationExecutor` Protocol\n- [ ] All dependencies typed correctly\n- [ ] Method names match actual implementations\n- [ ] mypy passes\n- [ ] Tests pass\n\n**Validation**:\n```bash\nmypy src/learning/iteration_executor.py\npytest tests/learning/test_iteration_executor.py\n```\n\n---\n\n### TASK-QA-007: Type Hints for Feedback Generator\n**Priority**: P0 (Blocker)\n**Estimated Time**: 1 hour\n**Dependencies**: TASK-QA-004\n\n**Objective**: Add types to LLM feedback component\n\n**Target File**: `src/learning/feedback_generator.py`\n\n**Steps**:\n1. Import `FeedbackGenerator` Protocol\n2. Add type hints to `generate_feedback()`\n3. Ensure parameter types match IterationRecord\n\n**Implementation**:\n```python\n# src/learning/feedback_generator.py\n\nfrom src.interfaces import FeedbackGenerator, IterationRecord\n\nclass LLMFeedbackGenerator(FeedbackGenerator):\n\n    def generate_feedback(\n        self,\n        record: IterationRecord,\n        history: list[IterationRecord]\n    ) -> str:\n        \"\"\"Generate LLM feedback for iteration\"\"\"\n        # ... implementation\n        return feedback_text\n```\n\n**Acceptance Criteria**:\n- [ ] Implements `FeedbackGenerator` Protocol\n- [ ] Parameters typed correctly\n- [ ] Return type specified\n- [ ] mypy passes\n\n**Validation**:\n```bash\nmypy src/learning/feedback_generator.py\n```\n\n---\n\n### TASK-QA-008: Type Hints for Learning Loop\n**Priority**: P0 (Blocker)\n**Estimated Time**: 2 hours\n**Dependencies**: TASK-QA-005, TASK-QA-006, TASK-QA-007\n\n**Objective**: Add types to main orchestration component\n\n**Target File**: `src/learning/learning_loop.py`\n\n**Steps**:\n1. Import all relevant Protocols\n2. Add type hints to `__init__` parameters\n3. Add type hints to `run_iterations()`\n4. Ensure all component dependencies are typed\n\n**Key Focus** (integrates all components):\n```python\n# src/learning/learning_loop.py\n\nfrom src.interfaces import (\n    LearningLoop as LearningLoopProtocol,\n    HistoryProvider,\n    ChampionTracker,\n    IterationExecutor\n)\n\nclass LearningLoop(LearningLoopProtocol):\n\n    def __init__(\n        self,\n        history: HistoryProvider,\n        champion: ChampionTracker,\n        executor: IterationExecutor\n    ) -> None:\n        self.history = history\n        self.champion = champion\n        self.executor = executor\n\n    def run_iterations(self, num_iterations: int) -> None:\n        \"\"\"Run specified number of learning iterations\"\"\"\n        for i in range(num_iterations):\n            record = self.executor.execute_iteration(f\"iteration_{i}\")\n            self.history.save(record)\n            self.champion.update_if_better(record)\n```\n\n**Acceptance Criteria**:\n- [ ] Implements `LearningLoop` Protocol\n- [ ] All dependencies typed with Protocols\n- [ ] Constructor parameters match actual usage\n- [ ] mypy passes\n- [ ] Integration tests pass\n\n**Validation**:\n```bash\nmypy src/learning/learning_loop.py\npytest tests/learning/test_learning_loop.py\n```\n\n---\n\n### TASK-QA-009: Type Hints for Backtest Executor\n**Priority**: P0 (Blocker)\n**Estimated Time**: 1 hour\n**Dependencies**: TASK-QA-004\n\n**Objective**: Add types to backtesting component\n\n**Target File**: `src/backtest/executor.py`\n\n**Steps**:\n1. Import `BacktestExecutor` Protocol and `BacktestResult`\n2. Add type hints to `execute()` method\n3. Ensure pandas DataFrame typing\n\n**Key Fix** (Phase 8 error - missing parameters):\n```python\n# Before (Phase 8 error)\ndef execute(self, strategy_code: str) -> BacktestResult:\n    # Missing 'data' parameter!\n    ...\n\n# After\ndef execute(\n    self,\n    strategy_code: str,\n    data: pd.DataFrame\n) -> BacktestResult:\n    \"\"\"Execute backtest with provided data\"\"\"\n    ...\n```\n\n**Acceptance Criteria**:\n- [ ] Implements `BacktestExecutor` Protocol\n- [ ] All required parameters present\n- [ ] Return type matches BacktestResult\n- [ ] mypy passes\n\n**Validation**:\n```bash\nmypy src/backtest/executor.py\npytest tests/backtest/test_executor.py\n```\n\n---\n\n### TASK-QA-010: Type Hints for Hall of Fame Repository\n**Priority**: P0 (Blocker)\n**Estimated Time**: 1 hour\n**Dependencies**: TASK-QA-004\n\n**Objective**: Add types to champion persistence component\n\n**Target File**: `src/repository/hall_of_fame.py`\n\n**Steps**:\n1. Import `HallOfFameRepository` Protocol\n2. Add type hints to `save_champion()`\n3. Add type hints to `load_champion()`\n\n**Implementation**:\n```python\n# src/repository/hall_of_fame.py\n\nfrom typing import Optional\nfrom src.interfaces import HallOfFameRepository, IterationRecord\n\nclass HallOfFame(HallOfFameRepository):\n\n    def __init__(self, file_path: str) -> None:\n        self.file_path = file_path\n\n    def save_champion(self, record: IterationRecord) -> None:\n        \"\"\"Persist champion to file\"\"\"\n        # ... implementation\n\n    def load_champion(self) -> Optional[IterationRecord]:\n        \"\"\"Load champion from file\"\"\"\n        # ... implementation\n        return champion\n```\n\n**Acceptance Criteria**:\n- [ ] Implements `HallOfFameRepository` Protocol\n- [ ] Type hints on all methods\n- [ ] mypy passes\n\n**Validation**:\n```bash\nmypy src/repository/hall_of_fame.py\npytest tests/repository/test_hall_of_fame.py\n```\n\n---\n\n### TASK-QA-011: Checkpoint - Type Hints Complete\n**Priority**: P0 (Blocker)\n**Estimated Time**: 30 minutes\n**Dependencies**: All TASK-QA-005 through TASK-QA-010\n\n**Objective**: Validate all type hints before CI integration\n\n**Validation Steps**:\n1. Run mypy on all target modules\n2. Run full test suite\n3. Document any `# type: ignore` uses with rationale\n4. Verify IDE autocomplete working\n\n**Success Criteria**:\n- [ ] mypy passes on all learning/ modules\n- [ ] mypy passes on backtest/ module\n- [ ] mypy passes on repository/ module\n- [ ] All 926 tests pass\n- [ ] < 5 `# type: ignore` suppressions\n- [ ] IDE shows proper type hints\n\n**Checkpoint Command**:\n```bash\n# Comprehensive type check\nmypy src/learning/ src/backtest/ src/repository/\n\n# Full test suite\npytest tests/ -v\n\n# Count type ignores\ngrep -r \"# type: ignore\" src/ | wc -l  # Should be < 5\n```\n\n**Expected Output**:\n```\nmypy: Success: no issues found in 10 source files\npytest: 926 passed in X seconds\nType ignores: 2 (documented)\n```\n\n---\n\n## Phase 3: CI Integration (Day 3)\n\n### TASK-QA-012: Create GitHub Actions Workflow\n**Priority**: P0 (Blocker)\n**Estimated Time**: 1 hour\n**Dependencies**: TASK-QA-011\n\n**Objective**: Automate type checking on every commit\n\n**Steps**:\n1. Create `.github/workflows/` directory (if not exists)\n2. Create `type-safety.yml` workflow\n3. Configure mypy job\n4. Configure E2E smoke test job\n\n**Workflow File**:\n```yaml\n# .github/workflows/type-safety.yml\n\nname: Type Safety & E2E Smoke Tests\n\non:\n  push:\n    branches: [ main, develop ]\n  pull_request:\n    branches: [ main, develop ]\n\njobs:\n  mypy-type-check:\n    name: Static Type Checking (mypy)\n    runs-on: ubuntu-latest\n\n    steps:\n      - name: Checkout code\n        uses: actions/checkout@v4\n\n      - name: Set up Python 3.10\n        uses: actions/setup-python@v5\n        with:\n          python-version: '3.10'\n\n      - name: Install dependencies\n        run: |\n          python -m pip install --upgrade pip\n          pip install mypy\n          pip install -r requirements.txt\n\n      - name: Run mypy\n        run: |\n          mypy src/learning/ src/backtest/ src/repository/\n\n      - name: Report results\n        if: always()\n        run: |\n          echo \"mypy type checking complete\"\n\n  e2e-smoke-tests:\n    name: E2E Integration Smoke Tests\n    runs-on: ubuntu-latest\n\n    steps:\n      - name: Checkout code\n        uses: actions/checkout@v4\n\n      - name: Set up Python 3.10\n        uses: actions/setup-python@v5\n        with:\n          python-version: '3.10'\n\n      - name: Install dependencies\n        run: |\n          python -m pip install --upgrade pip\n          pip install -r requirements.txt\n          pip install pytest\n\n      - name: Run E2E smoke tests\n        run: |\n          pytest tests/integration/test_phase8_e2e_smoke.py -v\n\n      - name: Report results\n        if: always()\n        run: |\n          echo \"E2E smoke tests complete\"\n```\n\n**Acceptance Criteria**:\n- [ ] Workflow file created\n- [ ] Runs on push to main/develop\n- [ ] Runs on pull requests\n- [ ] mypy job configured\n- [ ] E2E test job configured\n- [ ] Jobs run independently\n\n**Validation**:\n```bash\n# Test workflow locally with act (optional)\nact -j mypy-type-check\n\n# Or push to trigger CI\ngit add .github/workflows/type-safety.yml\ngit commit -m \"Add type safety CI workflow\"\ngit push\n```\n\n---\n\n### TASK-QA-013: Create E2E Smoke Test Suite\n**Priority**: P0 (Blocker)\n**Estimated Time**: 2 hours\n**Dependencies**: TASK-QA-011\n\n**Objective**: Test Phase 8 scenarios to prevent regression\n\n**Steps**:\n1. Create `tests/integration/test_phase8_e2e_smoke.py`\n2. Write smoke test for each Phase 8 error\n3. Use actual components (not mocks)\n4. Keep tests fast (< 10 seconds total)\n\n**Test File**:\n```python\n# tests/integration/test_phase8_e2e_smoke.py\n\n\"\"\"\nE2E Smoke Tests - Prevent Phase 8 API Mismatches\n\nThese tests validate the 8 specific errors discovered in Phase 8:\n1. Parameter name mismatch (file_path vs filepath)\n2. Method signature error (execute_code vs execute)\n3. Missing required parameter (data not provided)\n4. Wrong classifier usage (ErrorClassifier vs SuccessClassifier)\n5. Deserialization field mismatch\n6. Champion parameter name (champion vs champion_tracker)\n7. Missing sim parameter\n8. Return type mismatch\n\"\"\"\n\nimport pytest\nimport pandas as pd\nfrom src.learning.iteration_history import IterationHistory\nfrom src.learning.champion_tracker import ChampionTracker\nfrom src.learning.iteration_executor import IterationExecutor\nfrom src.backtest.executor import BacktestExecutor\nfrom src.interfaces import IterationRecord\n\nclass TestPhase8Regressions:\n    \"\"\"Smoke tests for Phase 8 API contract errors\"\"\"\n\n    def test_iteration_history_parameter_name(self):\n        \"\"\"ERROR 1: file_path parameter name consistency\"\"\"\n        # This should work without TypeError\n        history = IterationHistory(file_path=\"test.json\")  # Not 'filepath'\n        assert history.file_path == \"test.json\"\n\n    def test_backtest_executor_method_signature(self):\n        \"\"\"ERROR 2: BacktestExecutor.execute() method name\"\"\"\n        executor = BacktestExecutor()\n        data = pd.DataFrame({\"close\": [100, 101, 102]})\n\n        # Should use execute(), not execute_code()\n        result = executor.execute(\"lambda df: df['close'] > 100\", data)\n        assert hasattr(result, \"sharpe_ratio\")\n\n    def test_backtest_executor_required_parameters(self):\n        \"\"\"ERROR 3: Missing 'data' parameter in execute()\"\"\"\n        executor = BacktestExecutor()\n        data = pd.DataFrame({\"close\": [100, 101, 102]})\n\n        # Must provide both strategy_code AND data\n        result = executor.execute(\n            strategy_code=\"lambda df: df['close'] > 100\",\n            data=data  # Required parameter\n        )\n        assert result is not None\n\n    def test_success_classifier_usage(self):\n        \"\"\"ERROR 4: Correct classifier (SuccessClassifier, not ErrorClassifier)\"\"\"\n        from src.learning.success_classifier import SuccessClassifier\n\n        classifier = SuccessClassifier()\n        record = IterationRecord(\n            iteration_num=1,\n            strategy_code=\"test\",\n            metrics={\"sharpe_ratio\": 1.5}\n        )\n\n        # Should use SuccessClassifier\n        level = classifier.classify(record)\n        assert level in [\"poor\", \"good\", \"excellent\"]\n\n    def test_iteration_record_deserialization(self):\n        \"\"\"ERROR 5: IterationRecord field names match\"\"\"\n        record_dict = {\n            \"iteration_num\": 1,\n            \"strategy_code\": \"lambda df: df['close'] > 100\",\n            \"metrics\": {\"sharpe_ratio\": 1.5},\n            \"classification_level\": \"excellent\",\n            \"champion_updated\": True\n        }\n\n        # Should deserialize without KeyError\n        record = IterationRecord(**record_dict)\n        assert record.iteration_num == 1\n\n    def test_champion_tracker_parameter_name(self):\n        \"\"\"ERROR 6: update_if_better() parameter name\"\"\"\n        tracker = ChampionTracker()\n        record = IterationRecord(\n            iteration_num=1,\n            strategy_code=\"test\",\n            metrics={\"sharpe_ratio\": 1.5}\n        )\n\n        # Parameter should be 'record', not 'champion' or 'champion_tracker'\n        updated = tracker.update_if_better(record=record)\n        assert isinstance(updated, bool)\n\n    def test_iteration_executor_all_dependencies(self):\n        \"\"\"ERROR 7 & 8: All required dependencies provided\"\"\"\n        # Mock dependencies\n        history = IterationHistory(file_path=\"test.json\")\n        backtest = BacktestExecutor()\n        data = pd.DataFrame({\"close\": [100, 101, 102]})\n\n        executor = IterationExecutor(\n            history=history,\n            backtest=backtest,\n            data=data  # Required: data\n            # sim parameter if needed\n        )\n\n        # Should initialize without missing parameter errors\n        assert executor.history is not None\n        assert executor.backtest is not None\n        assert executor.data is not None\n\n    def test_end_to_end_learning_iteration(self):\n        \"\"\"Full E2E test: One learning iteration without errors\"\"\"\n        # Setup\n        history = IterationHistory(file_path=\"test_e2e.json\")\n        champion = ChampionTracker()\n        backtest = BacktestExecutor()\n        data = pd.DataFrame({\"close\": [100, 101, 102, 103, 104]})\n\n        executor = IterationExecutor(\n            history=history,\n            backtest=backtest,\n            data=data\n        )\n\n        # Execute one iteration\n        record = executor.execute_iteration(\"lambda df: df['close'] > 100\")\n\n        # Validate all API contracts\n        assert isinstance(record, IterationRecord)\n        assert record.iteration_num >= 0\n        assert \"sharpe_ratio\" in record.metrics\n\n        # Save to history\n        history.save(record)\n\n        # Update champion\n        champion.update_if_better(record)\n\n        # Should complete without TypeError, AttributeError, or KeyError\n```\n\n**Acceptance Criteria**:\n- [ ] All 8 Phase 8 errors have test coverage\n- [ ] Tests run in < 10 seconds\n- [ ] Tests use real components (minimal mocking)\n- [ ] All tests pass\n- [ ] Clear test names and documentation\n\n**Validation**:\n```bash\npytest tests/integration/test_phase8_e2e_smoke.py -v\n```\n\n---\n\n### TASK-QA-014: Document Type System Usage\n**Priority**: P1 (Important)\n**Estimated Time**: 1 hour\n**Dependencies**: TASK-QA-011\n\n**Objective**: Create developer guide for type system\n\n**Steps**:\n1. Create `docs/TYPE_SYSTEM.md`\n2. Document how to add types to new code\n3. Document when to use `# type: ignore`\n4. Provide examples\n\n**Documentation File**:\n```markdown\n# Type System Usage Guide\n\n## Overview\n\nThis project uses Python type hints with mypy static type checking to catch API contract violations at development time.\n\n## Quick Start\n\n### Adding Type Hints to New Code\n\n```python\nfrom typing import Optional\nfrom src.interfaces import IterationRecord, HistoryProvider\n\nclass MyNewComponent(HistoryProvider):\n    def __init__(self, file_path: str) -> None:\n        self.file_path = file_path\n        self.records: list[IterationRecord] = []\n\n    def save(self, record: IterationRecord) -> None:\n        self.records.append(record)\n\n    def load_all(self) -> list[IterationRecord]:\n        return self.records\n\n    def get_champion(self) -> Optional[IterationRecord]:\n        if not self.records:\n            return None\n        return max(self.records, key=lambda r: r.metrics[\"sharpe_ratio\"])\n```\n\n### Running mypy\n\n```bash\n# Check specific file\nmypy src/learning/my_module.py\n\n# Check all target modules\nmypy src/learning/ src/backtest/ src/repository/\n\n# In CI (automatic)\n# Runs on every commit via GitHub Actions\n```\n\n### Protocol Interfaces\n\nUse Protocol types for component dependencies:\n\n```python\nfrom src.interfaces import BacktestExecutor, HistoryProvider\n\nclass MyComponent:\n    def __init__(\n        self,\n        backtest: BacktestExecutor,  # Protocol type, not concrete class\n        history: HistoryProvider\n    ) -> None:\n        self.backtest = backtest\n        self.history = history\n```\n\n### When to Use `# type: ignore`\n\nUse sparingly and with rationale:\n\n```python\n# OK: Third-party library without type stubs\nimport some_untyped_library  # type: ignore\n\n# OK: Dynamic attribute access\nresult = getattr(obj, dynamic_name)  # type: ignore[attr-defined]\n\n# NOT OK: Avoiding fixing actual type error\ndef broken(x):  # type: ignore  # BAD: Fix the signature instead!\n    ...\n```\n\n### Common Patterns\n\n**Optional Parameters**:\n```python\ndef process(data: Optional[pd.DataFrame] = None) -> None:\n    if data is None:\n        data = load_default_data()\n    ...\n```\n\n**List/Dict Types**:\n```python\nrecords: list[IterationRecord] = []\nmetrics: dict[str, float] = {}\n```\n\n**Union Types**:\n```python\nfrom typing import Union\n\ndef handle(value: Union[int, str]) -> None:\n    ...\n```\n\n## Troubleshooting\n\n### Error: \"Missing return statement\"\nAdd return type hint and ensure all code paths return a value.\n\n### Error: \"Incompatible type in argument\"\nCheck parameter names match between function definition and call sites.\n\n### Error: \"Module has no attribute\"\nAdd `# type: ignore[import]` if third-party library lacks stubs.\n\n## References\n\n- PEP 484: Type Hints\n- PEP 544: Protocol (Structural Subtyping)\n- mypy documentation: https://mypy.readthedocs.io/\n```\n\n**Acceptance Criteria**:\n- [ ] Developer guide created\n- [ ] Common patterns documented\n- [ ] Troubleshooting section included\n- [ ] Examples provided\n\n---\n\n### TASK-QA-015: Final Validation & Cleanup\n**Priority**: P0 (Blocker)\n**Estimated Time**: 1 hour\n**Dependencies**: All previous tasks\n\n**Objective**: Ensure entire system works end-to-end\n\n**Validation Checklist**:\n1. [ ] mypy passes on all target modules (0 errors)\n2. [ ] All 926 existing tests pass\n3. [ ] All 8 E2E smoke tests pass\n4. [ ] CI workflow runs successfully\n5. [ ] Documentation complete\n6. [ ] No regressions introduced\n\n**Validation Commands**:\n```bash\n# 1. Type checking\nmypy src/learning/ src/backtest/ src/repository/\n\n# 2. Unit tests\npytest tests/ -v\n\n# 3. E2E smoke tests\npytest tests/integration/test_phase8_e2e_smoke.py -v\n\n# 4. CI simulation (if using act)\nact\n\n# 5. Check documentation\nls docs/TYPE_SYSTEM.md\n```\n\n**Success Criteria**:\n```\n✓ mypy: Success: no issues found in 10 source files\n✓ pytest: 926 passed\n✓ E2E smoke tests: 8 passed\n✓ CI: All jobs passed\n✓ Documentation: Complete\n```\n\n**Cleanup Tasks**:\n- [ ] Remove any debug print statements\n- [ ] Remove unused imports\n- [ ] Format code consistently\n- [ ] Update README if needed\n\n---\n\n## Summary\n\n### Timeline\n\n```\nDay 1: Foundation (4-6 hours)\n├─ TASK-QA-001: Protocol Interfaces (2h)\n├─ TASK-QA-002: mypy Configuration (1h)\n├─ TASK-QA-003: Type Hints IterationHistory (1.5h)\n└─ TASK-QA-004: Checkpoint (0.5h)\n\nDay 2: Type Hints (6-8 hours)\n├─ TASK-QA-005: Champion Tracker (1h)\n├─ TASK-QA-006: Iteration Executor (1.5h)\n├─ TASK-QA-007: Feedback Generator (1h)\n├─ TASK-QA-008: Learning Loop (2h)\n├─ TASK-QA-009: Backtest Executor (1h)\n├─ TASK-QA-010: Hall of Fame (1h)\n└─ TASK-QA-011: Checkpoint (0.5h)\n\nDay 3: CI Integration (4-6 hours)\n├─ TASK-QA-012: GitHub Actions (1h)\n├─ TASK-QA-013: E2E Smoke Tests (2h)\n├─ TASK-QA-014: Documentation (1h)\n└─ TASK-QA-015: Final Validation (1h)\n\nTotal: 14-20 hours (2-3 days)\n```\n\n### Critical Path\n\n```\nQA-001 → QA-002 → QA-003 → QA-004 (Foundation)\n                               ↓\n            All Day 2 tasks (Type Hints)\n                               ↓\n            QA-011 (Checkpoint)\n                               ↓\n            All Day 3 tasks (CI)\n                               ↓\n            QA-015 (Complete)\n```\n\n### Resource Requirements\n\n**Developer Time**: 14-20 hours over 2-3 days\n**Computational Resources**: None (all development-time)\n**Dependencies**: mypy ≥ 1.18.0 (dev dependency only)\n**Risk**: Low (non-breaking changes, gradual adoption)\n\n### Success Metrics\n\n**Code Quality**:\n- mypy: 0 errors on target modules\n- Test coverage: Maintained >80%\n- Type coverage: 100% on public APIs\n\n**Reliability**:\n- Phase 8 errors: Cannot recur (prevented by CI)\n- Regression rate: 0 (CI blocks bad commits)\n\n**Developer Experience**:\n- IDE autocomplete: Working perfectly\n- Error detection: < 1 second (in IDE)\n- Debugging time: Reduced 30%\n",
  "fileStats": {
    "size": 30552,
    "lines": 1179,
    "lastModified": "2025-11-06T14:54:11.039Z"
  },
  "comments": []
}