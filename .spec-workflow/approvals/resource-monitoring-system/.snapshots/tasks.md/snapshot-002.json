{
  "id": "snapshot_1761401759598_0ptf44qgb",
  "approvalId": "approval_1761396011185_nwnuj0y8j",
  "approvalTitle": "Review tasks.md for resource-monitoring-system (15 tasks, 2 days)",
  "version": 2,
  "timestamp": "2025-10-25T14:15:59.598Z",
  "trigger": "approved",
  "status": "pending",
  "content": "# Tasks Document: Resource Monitoring System\n\n## Phase 1: Core Monitoring Components (Tasks 1-5)\n\n- [ ] 1. Create ResourceMonitor module\n  - File: `src/monitoring/resource_monitor.py`\n  - Implement system resource tracking (CPU, memory, disk)\n  - Start background thread for 5-second collection intervals\n  - Record metrics to Prometheus via MetricsCollector\n  - Purpose: Real-time system resource visibility\n  - _Leverage: `psutil` library, existing `MetricsCollector` from `src/monitoring/metrics_collector.py`_\n  - _Requirements: 1.1, 1.2_\n  - _Prompt: Implement the task for spec resource-monitoring-system, first run spec-workflow-guide to get the workflow guide then implement the task: Role: SRE with expertise in system monitoring and Python threading | Task: Create ResourceMonitor class following requirements 1.1-1.2, using psutil for system stats and integrating with existing MetricsCollector for Prometheus export | Restrictions: Must run in background thread without blocking main loop, handle psutil errors gracefully, collection overhead <1% CPU | _Leverage: psutil library, src/monitoring/metrics_collector.py | _Requirements: Requirements 1.1 (Resource tracking), 1.2 (Background collection) | Success: System metrics collected every 5s, exported to Prometheus correctly, <1% overhead | Instructions: Set task to [-] in tasks.md, mark [x] when ResourceMonitor tests pass with <1% overhead_\n\n- [ ] 2. Create DiversityMonitor module\n  - File: `src/monitoring/diversity_monitor.py`\n  - Track population diversity score (0.0-1.0)\n  - Calculate champion staleness (iterations since last update)\n  - Detect diversity collapse (<0.1 for 5 consecutive iterations)\n  - Purpose: Prevent learning system degradation\n  - _Leverage: Existing `MetricsCollector`, diversity calculation from `src/population/population_manager.py`_\n  - _Requirements: 1.3, 1.4_\n  - _Prompt: Implement the task for spec resource-monitoring-system, first run spec-workflow-guide to get the workflow guide then implement the task: Role: ML Engineer with expertise in population-based algorithms and monitoring | Task: Create DiversityMonitor class following requirements 1.3-1.4, tracking diversity metrics and detecting collapse conditions using existing population manager | Restrictions: Must not impact iteration performance, maintain 5-iteration sliding window for collapse detection, handle edge cases (empty population) | _Leverage: src/population/population_manager.py for diversity calculation, src/monitoring/metrics_collector.py | _Requirements: Requirements 1.3 (Diversity tracking), 1.4 (Collapse detection) | Success: Diversity tracked accurately, collapse detected within 1 iteration, metrics exported | Instructions: Set task to [-], mark [x] when DiversityMonitor tests pass_\n\n- [ ] 3. Create ContainerMonitor module\n  - File: `src/monitoring/container_monitor.py`\n  - Track Docker container stats (memory, CPU per container)\n  - Scan for orphaned containers (finlab-sandbox label, status=exited)\n  - Implement automated cleanup of orphaned containers\n  - Purpose: Prevent resource leaks from Docker containers\n  - _Leverage: `docker` library, existing `MetricsCollector`, integration with `src/sandbox/docker_executor.py`_\n  - _Requirements: 1.5_\n  - _Prompt: Implement the task for spec resource-monitoring-system, first run spec-workflow-guide to get the workflow guide then implement the task: Role: DevOps Engineer with expertise in Docker and container lifecycle management | Task: Create ContainerMonitor class following requirement 1.5, querying docker.stats() for resource usage and implementing orphaned container cleanup | Restrictions: Must handle Docker daemon unavailability, timeout stats queries after 2s, verify container labels before cleanup | _Leverage: docker library, src/sandbox/docker_executor.py, src/monitoring/metrics_collector.py | _Requirements: Requirement 1.5 (Container monitoring and cleanup) | Success: Container stats accurate, orphaned containers detected and cleaned, no false positives | Instructions: Set task to [-], mark [x] when ContainerMonitor cleanup verified_\n\n- [ ] 4. Create AlertManager module\n  - File: `src/monitoring/alert_manager.py`\n  - Implement alert condition evaluation (memory >80%, diversity <0.1, etc.)\n  - Trigger alerts via logging and metrics increment\n  - Implement alert suppression (don't re-alert same condition)\n  - Purpose: Proactive issue detection and notification\n  - _Leverage: Existing `MetricsCollector`, `json_logger.py` for structured logging_\n  - _Requirements: 3.1, 3.2, 3.3, 3.4, 3.5_\n  - _Prompt: Implement the task for spec resource-monitoring-system, first run spec-workflow-guide to get the workflow guide then implement the task: Role: SRE with expertise in alerting systems and threshold management | Task: Create AlertManager class following requirements 3.1-3.5, evaluating multiple alert conditions and triggering notifications with suppression to avoid alert fatigue | Restrictions: Must evaluate alerts every 10s without impacting loop, suppress duplicate alerts, provide clear alert messages | _Leverage: src/monitoring/metrics_collector.py, src/utils/json_logger.py | _Requirements: Requirements 3.1-3.5 (Alert conditions for memory, diversity, staleness, success rate, orphaned containers) | Success: All 5 alert types functional, suppression works, alerts logged clearly | Instructions: Set task to [-], mark [x] when AlertManager tests pass for all conditions_\n\n- [ ] 5. Extend MetricsCollector with new metrics\n  - File: `src/monitoring/metrics_collector.py` (modify existing)\n  - Add system metrics (memory_usage, cpu_usage)\n  - Add diversity metrics (population_diversity, unique_strategy_count, champion_staleness)\n  - Add container metrics (active_containers, container_memory_usage, orphaned_containers)\n  - Add alert metrics (alerts_triggered_total by type)\n  - Purpose: Centralized metric definitions for Prometheus export\n  - _Leverage: Existing `MetricsCollector` class and Prometheus client library_\n  - _Requirements: All metric export requirements_\n  - _Prompt: Implement the task for spec resource-monitoring-system, first run spec-workflow-guide to get the workflow guide then implement the task: Role: SRE with expertise in Prometheus instrumentation and metrics design | Task: Extend existing MetricsCollector with 15+ new metrics following Prometheus best practices, ensuring proper metric types (Gauge, Counter, Histogram) | Restrictions: Must maintain backward compatibility with existing metrics, follow Prometheus naming conventions, add help text for all metrics | _Leverage: src/monitoring/metrics_collector.py, prometheus_client library | _Requirements: All metric requirements from design | Success: All 15+ metrics defined with correct types, exposed via /metrics endpoint, documented | Instructions: Set task to [-], mark [x] when metrics visible in Prometheus_\n\n## Phase 2: Configuration & Dashboards (Tasks 6-7)\n\n- [ ] 6. Create monitoring configuration file\n  - File: `config/monitoring_config.yaml`\n  - Define all alert thresholds (memory 80%, diversity 0.1, etc.)\n  - Configure collection intervals and retention policy\n  - Document each configuration parameter\n  - Purpose: Centralized monitoring configuration\n  - _Leverage: Existing config structure from `config/learning_system.yaml`_\n  - _Requirements: All configuration requirements_\n  - _Prompt: Implement the task for spec resource-monitoring-system, first run spec-workflow-guide to get the workflow guide then implement the task: Role: DevOps Engineer with expertise in YAML configuration and monitoring best practices | Task: Create comprehensive monitoring_config.yaml with all alert thresholds, intervals, and settings, providing production-ready defaults with clear documentation | Restrictions: Must use sensible production defaults, document all parameters with comments, follow existing config structure | _Leverage: config/learning_system.yaml for structure | _Requirements: All configuration requirements | Success: Config complete with all parameters, well-documented, loads correctly | Instructions: Set task to [-], mark [x] when config validates and loads_\n\n- [ ] 7. Create Grafana dashboard template\n  - File: `config/grafana_dashboard.json`\n  - Design 4 panels: Resource Usage, Strategy Performance, Diversity Metrics, Container Stats\n  - Add alert threshold lines to graphs\n  - Add annotations for champion update events\n  - Purpose: Visual monitoring interface for operators\n  - _Leverage: Grafana dashboard JSON schema, Prometheus as datasource_\n  - _Requirements: 2.1, 2.2, 2.3, 2.4_\n  - _Prompt: Implement the task for spec resource-monitoring-system, first run spec-workflow-guide to get the workflow guide then implement the task: Role: SRE with expertise in Grafana dashboard design and visualization | Task: Create comprehensive Grafana dashboard JSON following requirements 2.1-2.4, designing 4 panels with appropriate visualizations and alert thresholds | Restrictions: Must use Prometheus datasource, include all metrics from design, add helpful annotations, set 5s refresh interval | _Leverage: Grafana dashboard JSON format, Prometheus queries | _Requirements: Requirements 2.1-2.4 (4 dashboard panels) | Success: Dashboard displays all 4 panels correctly, metrics update in real-time, alert lines visible | Instructions: Set task to [-], mark [x] when dashboard imports successfully in Grafana_\n\n## Phase 3: Integration (Tasks 8-9)\n\n- [ ] 8. Integrate monitoring into autonomous loop\n  - File: `artifacts/working/modules/autonomous_loop.py` (modify)\n  - Initialize all monitors at loop startup\n  - Record diversity after each generation\n  - Record champion updates when they occur\n  - Cleanup monitors on loop shutdown\n  - Purpose: Enable monitoring in production iteration loop\n  - _Leverage: Existing autonomous_loop.py structure, all monitor modules_\n  - _Requirements: 1.1, 1.2_\n  - _Prompt: Implement the task for spec resource-monitoring-system, first run spec-workflow-guide to get the workflow guide then implement the task: Role: Backend Developer with expertise in system integration and lifecycle management | Task: Integrate all monitoring components into autonomous_loop.py following requirements 1.1-1.2, initializing monitors at startup and recording events during iteration | Restrictions: Must not break existing loop functionality, ensure monitors start/stop cleanly, handle monitor failures gracefully | _Leverage: artifacts/working/modules/autonomous_loop.py, all monitor modules | _Requirements: Requirements 1.1 (Integration), 1.2 (Production readiness) | Success: Monitors run during loop execution, metrics exported correctly, loop performance unaffected | Instructions: Set task to [-], mark [x] when integration tests pass_\n\n- [ ] 9. Add monitoring configuration to learning system config\n  - File: `config/learning_system.yaml` (modify)\n  - Add `monitoring` section with enabled flags for each monitor\n  - Add `alerts` section with threshold configuration\n  - Document configuration options\n  - Purpose: Enable/disable monitoring features in production\n  - _Leverage: Existing learning_system.yaml structure_\n  - _Requirements: All configuration requirements_\n  - _Prompt: Implement the task for spec resource-monitoring-system, first run spec-workflow-guide to get the workflow guide then implement the task: Role: DevOps Engineer with expertise in YAML configuration and feature flags | Task: Extend learning_system.yaml with monitoring configuration section, providing clear documentation and sensible defaults following all configuration requirements | Restrictions: Must maintain backward compatibility (default enabled), document all options clearly, follow existing YAML structure | _Leverage: config/learning_system.yaml | _Requirements: All configuration requirements | Success: Monitoring can be configured via YAML, all settings documented, backward compatible | Instructions: Set task to [-], mark [x] when config schema validated_\n\n## Phase 4: Testing (Tasks 10-13)\n\n- [ ] 10. Write monitor unit tests\n  - File: `tests/monitoring/test_resource_monitor.py`, `test_diversity_monitor.py`, `test_container_monitor.py`, `test_alert_manager.py`\n  - Test ResourceMonitor: psutil metrics collection, background thread lifecycle\n  - Test DiversityMonitor: diversity calculation, staleness tracking, collapse detection\n  - Test ContainerMonitor: orphaned detection, cleanup logic\n  - Test AlertManager: all 5 alert conditions, suppression logic\n  - Coverage target: >85% per module\n  - _Leverage: `pytest`, `unittest.mock` for mocking external dependencies_\n  - _Requirements: All functional requirements_\n  - _Prompt: Implement the task for spec resource-monitoring-system, first run spec-workflow-guide to get the workflow guide then implement the task: Role: QA Engineer with expertise in unit testing and mocking | Task: Create comprehensive unit tests for all 4 monitor modules covering all functional requirements, using mocks for external dependencies (psutil, docker) | Restrictions: Must test both success and failure scenarios, mock all external calls, achieve >85% coverage per module | _Leverage: pytest, unittest.mock for mocking | _Requirements: All functional requirements for each monitor | Success: All monitors tested with >85% coverage, edge cases covered, tests run in <10s | Instructions: Set task to [-], mark [x] when all tests pass with >85% coverage_\n\n- [ ] 11. Write integration tests with real Prometheus\n  - File: `tests/integration/test_monitoring_system.py`\n  - Test 1: Start monitors, query /metrics endpoint, verify all metrics present\n  - Test 2: Trigger high memory alert, verify alert logged and metric incremented\n  - Test 3: Simulate diversity collapse, verify alert triggered correctly\n  - Test 4: Create/cleanup containers, verify container metrics accurate\n  - _Leverage: Real Prometheus HTTP endpoint, pytest fixtures_\n  - _Requirements: All integration requirements_\n  - _Prompt: Implement the task for spec resource-monitoring-system, first run spec-workflow-guide to get the workflow guide then implement the task: Role: Integration Test Engineer with expertise in Prometheus and end-to-end testing | Task: Create integration tests validating metrics export and alert triggering using real Prometheus endpoint, covering all integration requirements | Restrictions: Must use real /metrics endpoint, simulate realistic conditions, cleanup after tests | _Leverage: Prometheus HTTP endpoint, pytest fixtures | _Requirements: All integration requirements | Success: All 4 integration scenarios pass, metrics accurate, alerts functional | Instructions: Set task to [-], mark [x] when integration tests pass_\n\n- [ ] 12. Write autonomous loop integration tests\n  - File: `tests/integration/test_autonomous_loop_monitoring.py`\n  - Run 20 iterations with full monitoring enabled\n  - Verify metrics exported correctly throughout run\n  - Simulate diversity collapse during run, verify alert\n  - Verify final metrics export to JSON\n  - _Leverage: Existing iteration loop test patterns_\n  - _Requirements: 1.1, 1.2_\n  - _Prompt: Implement the task for spec resource-monitoring-system, first run spec-workflow-guide to get the workflow guide then implement the task: Role: Integration Test Engineer with expertise in end-to-end system testing | Task: Create autonomous loop integration tests with monitoring following requirements 1.1-1.2, running 20 iterations and verifying all monitoring features | Restrictions: Must test real loop execution, verify metrics accuracy, limit to 20 iterations for speed | _Leverage: Existing loop test patterns, all monitors | _Requirements: Requirements 1.1 (Full integration), 1.2 (Production readiness) | Success: 20 iterations complete with monitoring, metrics accurate, alerts functional | Instructions: Set task to [-], mark [x] when integration test passes_\n\n- [ ] 13. Validate Grafana dashboard with test data\n  - File: `tests/integration/test_grafana_dashboard.py` (manual test script)\n  - Import dashboard JSON into Grafana\n  - Run 20-iteration test to generate metrics\n  - Manually verify all 4 panels display correctly\n  - Verify alert thresholds visible, annotations appear\n  - Purpose: Ensure dashboard works in production\n  - _Leverage: Grafana import API, test data generation_\n  - _Requirements: 2.1, 2.2, 2.3, 2.4_\n  - _Prompt: Implement the task for spec resource-monitoring-system, first run spec-workflow-guide to get the workflow guide then implement the task: Role: QA Engineer with expertise in Grafana and visual testing | Task: Create validation script for Grafana dashboard following requirements 2.1-2.4, importing dashboard and generating test data for manual verification | Restrictions: Must document manual verification steps, generate realistic test data, provide clear pass/fail criteria | _Leverage: Grafana import API, pytest for data generation | _Requirements: Requirements 2.1-2.4 (Dashboard panels) | Success: Dashboard imports successfully, all panels display data, manual checklist completed | Instructions: Set task to [-], mark [x] when dashboard validation checklist passes_\n\n## Phase 5: Documentation & Deployment (Tasks 14-15)\n\n- [ ] 14. Create user documentation\n  - File: `docs/MONITORING.md`\n  - Document Prometheus setup and scrape configuration\n  - Document Grafana dashboard import and usage\n  - Document alert configuration and customization\n  - Provide troubleshooting guide\n  - _Leverage: Existing documentation structure_\n  - _Requirements: All_\n  - _Prompt: Implement the task for spec resource-monitoring-system, first run spec-workflow-guide to get the workflow guide then implement the task: Role: Technical Writer with expertise in monitoring and observability documentation | Task: Create comprehensive monitoring documentation covering Prometheus setup, Grafana usage, alert configuration, and troubleshooting | Restrictions: Must be clear for both novice and experienced users, include real examples, maintain consistent structure | _Leverage: Existing documentation structure and style | _Requirements: All requirements (complete monitoring setup) | Success: Documentation is complete, clear, users can set up monitoring successfully | Instructions: Set task to [-], mark [x] when documentation review passes_\n\n- [ ] 15. Create deployment and setup scripts\n  - File: `scripts/setup_monitoring.sh`\n  - Verify Prometheus installation (v2.40+)\n  - Configure Prometheus scrape job for http://localhost:8000/metrics\n  - Import Grafana dashboard automatically\n  - Validate monitoring configuration\n  - _Leverage: Shell scripting, Prometheus/Grafana APIs_\n  - _Requirements: Deployment requirements_\n  - _Prompt: Implement the task for spec resource-monitoring-system, first run spec-workflow-guide to get the workflow guide then implement the task: Role: DevOps Engineer with expertise in Prometheus/Grafana and deployment automation | Task: Create automated setup script that configures Prometheus, imports Grafana dashboard, and validates monitoring setup | Restrictions: Must check prerequisites, provide clear error messages, be idempotent (safe to re-run) | _Leverage: bash scripting, prometheus config, grafana API | _Requirements: Deployment requirements (Prometheus 2.40+, Grafana 9.0+) | Success: Script automates complete setup, validates configuration, provides clear success/failure messages | Instructions: Set task to [-], mark [x] when script successfully deploys monitoring on clean system_\n\n## Summary\n\n**Total Tasks**: 15\n**Estimated Time**: 2 days (full-time)\n\n**Phase Breakdown**:\n- Phase 1 (Core Monitors): Tasks 1-5 → 6-8 hours\n- Phase 2 (Config/Dashboard): Tasks 6-7 → 2-3 hours\n- Phase 3 (Integration): Tasks 8-9 → 2-3 hours\n- Phase 4 (Testing): Tasks 10-13 → 6-8 hours\n- Phase 5 (Docs): Tasks 14-15 → 2-3 hours\n\n**Dependencies**:\n- Tasks 1-4 can run in parallel\n- Task 5 can start after Tasks 1-4 begin (extend existing class)\n- Tasks 6-7 can run in parallel with Tasks 1-5\n- Tasks 8-9 depend on Tasks 1-5 complete\n- Tasks 10-13 depend on respective implementation tasks\n- Tasks 14-15 depend on all previous tasks\n\n**Critical Path**: 1→5→8→11→15\n",
  "fileStats": {
    "size": 20163,
    "lines": 188,
    "lastModified": "2025-10-25T00:18:58.462Z"
  },
  "comments": []
}