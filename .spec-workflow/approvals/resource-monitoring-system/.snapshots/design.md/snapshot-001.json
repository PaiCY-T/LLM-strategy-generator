{
  "id": "snapshot_1761344139099_t2oc99iqk",
  "approvalId": "approval_1761344139064_s1dfkszq2",
  "approvalTitle": "Resource Monitoring System - Design Document",
  "version": 1,
  "timestamp": "2025-10-24T22:15:39.099Z",
  "trigger": "initial",
  "status": "pending",
  "content": "# Design Document: Resource Monitoring System\n\n## Overview\n\nThis design implements a **comprehensive resource monitoring system** using Prometheus metrics, Grafana dashboards, and automated alerting to detect memory leaks, CPU exhaustion, diversity collapse, and other production stability issues in real-time.\n\n**Architecture Pattern**: Observer/Publisher-Subscriber pattern - components publish metrics, centralized collector aggregates and exports to Prometheus.\n\n## Steering Document Alignment\n\n### Technical Standards (tech.md)\n- **Metrics Collection**: Use Prometheus client library with standard metric types (Counter, Gauge, Histogram)\n- **Logging**: Integrate with existing JSON logger for alert events\n- **Configuration**: Monitoring settings in `config/monitoring_config.yaml`\n- **Error Handling**: Non-blocking metrics collection - failures don't halt iteration loop\n\n### Project Structure (structure.md)\n- New module: `src/monitoring/resource_monitor.py` - System resource tracking\n- New module: `src/monitoring/diversity_monitor.py` - Population diversity tracking\n- New module: `src/monitoring/alert_manager.py` - Threshold-based alerting\n- Extend: `src/monitoring/metrics_collector.py` - Add container and diversity metrics\n- Config: `config/grafana_dashboard.json` - Pre-configured dashboard template\n- Config: `config/monitoring_config.yaml` - Alert thresholds and retention policy\n\n## Code Reuse Analysis\n\n### Existing Components to Leverage\n- **`src/monitoring/metrics_collector.py`**: Already exists, will be extended with new metrics\n- **`prometheus_client` library**: Already in requirements.txt\n- **`psutil` library**: For system resource monitoring\n- **`docker` library**: For container stats (from docker-sandbox-security spec)\n\n### Integration Points\n- **Autonomous Loop**: Instrument iteration lifecycle events in `artifacts/working/modules/autonomous_loop.py`\n- **Population Manager**: Track diversity in `src/population/population_manager.py`\n- **Docker Executor**: Export container stats from `src/sandbox/docker_executor.py`\n- **Champion Tracking**: Monitor staleness in `autonomous_loop.py` champion update logic\n\n## Architecture\n\n```mermaid\ngraph TD\n    A[Autonomous Loop] -->|Iteration Events| B[Metrics Collector]\n    C[Population Manager] -->|Diversity Metrics| B\n    D[Docker Executor] -->|Container Stats| B\n    E[System Monitor] -->|Resource Usage| B\n\n    B -->|/metrics Endpoint| F[Prometheus]\n    F -->|Scrape 5s| G[Grafana Dashboard]\n    F -->|Evaluate Rules| H[Alert Manager]\n\n    H -->|High Memory| I[Alert: Memory]\n    H -->|Diversity Collapse| J[Alert: Diversity]\n    H -->|Champion Stale| K[Alert: Staleness]\n    H -->|Low Success Rate| L[Alert: Performance]\n\n    style B fill:#e6f3ff\n    style F fill:#e6ffe6\n    style G fill:#ffe6f3\n    style H fill:#ffe6e6\n```\n\n### Modular Design Principles\n- **Single File Responsibility**:\n  - `resource_monitor.py` - System CPU/memory only\n  - `diversity_monitor.py` - Population diversity only\n  - `alert_manager.py` - Alert evaluation only\n- **Component Isolation**: Monitors don't depend on each other\n- **Service Layer Separation**: Collection → Aggregation → Export → Alerting\n- **Utility Modularity**: Reuse existing MetricsCollector, extend with new metrics\n\n## Components and Interfaces\n\n### Component 1: ResourceMonitor\n- **Purpose:** Track system-level resource usage (CPU, memory, disk)\n- **Interfaces:**\n  ```python\n  class ResourceMonitor:\n      def __init__(self, metrics_collector: MetricsCollector):\n          \"\"\"Initialize with shared metrics collector\"\"\"\n\n      def start_monitoring(self, interval_seconds: int = 5):\n          \"\"\"Start background thread collecting resource stats\"\"\"\n\n      def stop_monitoring(self):\n          \"\"\"Stop background thread\"\"\"\n\n      def get_current_stats(self) -> dict:\n          \"\"\"Return current CPU%, memory%, disk% for logging\"\"\"\n\n      def _record_system_metrics(self):\n          \"\"\"Internal: collect and record to Prometheus\"\"\"\n  ```\n- **Dependencies:** `psutil`, `MetricsCollector`\n- **Reuses:** Existing `MetricsCollector` infrastructure\n\n### Component 2: DiversityMonitor\n- **Purpose:** Track population diversity and champion staleness\n- **Interfaces:**\n  ```python\n  class DiversityMonitor:\n      def __init__(self, metrics_collector: MetricsCollector):\n          \"\"\"Initialize with shared metrics collector\"\"\"\n\n      def record_diversity(self, iteration: int, diversity: float,\n                          unique_count: int, total_count: int):\n          \"\"\"Record diversity metrics after each generation\"\"\"\n\n      def record_champion_update(self, iteration: int, old_sharpe: float,\n                                 new_sharpe: float):\n          \"\"\"Record champion change event\"\"\"\n\n      def calculate_staleness(self, current_iteration: int,\n                             last_update_iteration: int) -> int:\n          \"\"\"Return iterations since last champion update\"\"\"\n\n      def check_diversity_collapse(self, window: int = 5) -> bool:\n          \"\"\"Return True if diversity <0.1 for last 5 iterations\"\"\"\n  ```\n- **Dependencies:** `MetricsCollector`, Population Manager\n- **Reuses:** Diversity calculation from `src/population/population_manager.py`\n\n### Component 3: ContainerMonitor\n- **Purpose:** Track Docker container resource usage and cleanup status\n- **Interfaces:**\n  ```python\n  class ContainerMonitor:\n      def __init__(self, metrics_collector: MetricsCollector, docker_client):\n          \"\"\"Initialize with Docker client\"\"\"\n\n      def record_container_created(self, container_id: str):\n          \"\"\"Increment active_containers gauge\"\"\"\n\n      def record_container_stats(self, container_id: str):\n          \"\"\"Query docker.stats() and record memory/CPU usage\"\"\"\n\n      def record_container_cleanup(self, container_id: str, success: bool):\n          \"\"\"Decrement active_containers, track cleanup failures\"\"\"\n\n      def scan_orphaned_containers(self) -> list[str]:\n          \"\"\"Find containers with label=finlab-sandbox but status=exited\"\"\"\n\n      def cleanup_orphaned_containers(self) -> int:\n          \"\"\"Remove orphaned containers, return count cleaned\"\"\"\n  ```\n- **Dependencies:** `docker` library, `MetricsCollector`\n- **Reuses:** Docker client from `src/sandbox/docker_executor.py`\n\n### Component 4: AlertManager\n- **Purpose:** Evaluate alert conditions and trigger notifications\n- **Interfaces:**\n  ```python\n  class AlertManager:\n      def __init__(self, config: AlertConfig, metrics_collector: MetricsCollector):\n          \"\"\"Load alert thresholds from config\"\"\"\n\n      def evaluate_alerts(self):\n          \"\"\"Check all conditions, trigger alerts if thresholds exceeded\"\"\"\n\n      def _check_memory_threshold(self) -> Optional[Alert]:\n          \"\"\"Return alert if memory >80% of system\"\"\"\n\n      def _check_diversity_collapse(self) -> Optional[Alert]:\n          \"\"\"Return alert if diversity <0.1 for 5 iterations\"\"\"\n\n      def _check_champion_staleness(self) -> Optional[Alert]:\n          \"\"\"Return alert if champion unchanged for 20 iterations\"\"\"\n\n      def _check_success_rate(self) -> Optional[Alert]:\n          \"\"\"Return alert if success rate <20% over 10 iterations\"\"\"\n\n      def _trigger_alert(self, alert: Alert):\n          \"\"\"Log alert, increment alert counter, send notification\"\"\"\n  ```\n- **Dependencies:** `MetricsCollector`, Alert configuration\n- **Reuses:** JSON logger for alert notifications\n\n### Component 5: MetricsExporter (Extend Existing)\n- **Purpose:** Expose /metrics endpoint for Prometheus scraping\n- **Interfaces:**\n  ```python\n  # Extend existing MetricsCollector in src/monitoring/metrics_collector.py\n  class MetricsCollector:\n      # Existing metrics...\n\n      # NEW: Resource metrics\n      system_memory_usage = Gauge('system_memory_usage_bytes', 'System memory usage')\n      system_cpu_usage = Gauge('system_cpu_usage_percent', 'System CPU usage')\n\n      # NEW: Diversity metrics\n      population_diversity = Gauge('population_diversity', 'Population diversity score')\n      unique_strategy_count = Gauge('unique_strategy_count', 'Number of unique strategies')\n      champion_staleness = Gauge('champion_staleness_iterations', 'Iterations since champion update')\n\n      # NEW: Container metrics\n      active_containers = Gauge('active_containers', 'Number of running containers')\n      container_memory_usage = Gauge('container_memory_usage_bytes', 'Container memory', ['container_id'])\n      container_cpu_usage = Gauge('container_cpu_usage_percent', 'Container CPU', ['container_id'])\n      orphaned_containers = Gauge('orphaned_containers', 'Number of orphaned containers')\n\n      # NEW: Alert metrics\n      alerts_triggered_total = Counter('alerts_triggered_total', 'Total alerts', ['alert_type'])\n  ```\n- **Dependencies:** Prometheus client library\n- **Reuses:** Existing HTTP server on port 8000\n\n## Data Models\n\n### Alert\n```python\n@dataclass\nclass Alert:\n    alert_type: str          # \"high_memory\", \"diversity_collapse\", etc.\n    severity: str            # \"warning\", \"critical\"\n    message: str\n    current_value: float\n    threshold_value: float\n    timestamp: str\n    iteration_context: int\n```\n\n### MonitoringConfig\n```python\n@dataclass\nclass AlertConfig:\n    memory_threshold_percent: float = 80.0\n    diversity_collapse_threshold: float = 0.1\n    diversity_collapse_window: int = 5\n    champion_staleness_threshold: int = 20\n    success_rate_threshold: float = 0.2\n    success_rate_window: int = 10\n    orphaned_container_threshold: int = 3\n\n    @classmethod\n    def from_yaml(cls, path: str) -> 'AlertConfig':\n        \"\"\"Load from config/monitoring_config.yaml\"\"\"\n```\n\n### SystemStats\n```python\n@dataclass\nclass SystemStats:\n    timestamp: str\n    memory_used_gb: float\n    memory_total_gb: float\n    memory_percent: float\n    cpu_percent: float\n    disk_used_gb: float\n    disk_percent: float\n```\n\n## Error Handling\n\n### Error Scenarios\n\n1. **Prometheus HTTP Server Fails to Start**\n   - **Handling:** Log error, continue without metrics export (degraded mode)\n   - **User Impact:** Warning: \"Metrics export disabled, Prometheus unavailable\"\n   - **Recovery:** Retry on next iteration, auto-recover if port becomes available\n\n2. **psutil Resource Collection Fails**\n   - **Handling:** Catch `psutil.Error`, log warning, skip this collection cycle\n   - **User Impact:** Missing data points in Grafana (gaps in graphs)\n   - **Recovery:** Automatic on next 5-second interval\n\n3. **Docker Stats Query Timeout**\n   - **Handling:** Timeout after 2s, log warning, use cached stats\n   - **User Impact:** Stale container metrics (acceptable for monitoring)\n   - **Recovery:** Next container stats query may succeed\n\n4. **Alert Notification Failure**\n   - **Handling:** Log alert locally even if notification fails\n   - **User Impact:** Alert logged to JSON but may not reach operator\n   - **Recovery:** Retry notification on next alert evaluation cycle\n\n5. **Grafana Dashboard Connection Lost**\n   - **Handling:** Prometheus continues collecting, Grafana auto-reconnects\n   - **User Impact:** Dashboard shows \"No data\" until reconnection\n   - **Recovery:** Grafana polls Prometheus every 5s, auto-recovers\n\n## Testing Strategy\n\n### Unit Testing\n\n**ResourceMonitor Tests** (`tests/monitoring/test_resource_monitor.py`):\n- Test psutil metrics collection\n- Test background thread start/stop\n- Test memory/CPU percentage calculations\n- Mock psutil failures, verify graceful degradation\n- **Coverage Target:** >85%\n\n**DiversityMonitor Tests** (`tests/monitoring/test_diversity_monitor.py`):\n- Test diversity recording with various values (0.0, 0.5, 1.0)\n- Test champion staleness calculation\n- Test diversity collapse detection (5 consecutive <0.1)\n- **Coverage Target:** >90%\n\n**ContainerMonitor Tests** (`tests/monitoring/test_container_monitor.py`):\n- Mock Docker client, test container stats collection\n- Test orphaned container detection\n- Test cleanup success/failure tracking\n- **Coverage Target:** >85%\n\n**AlertManager Tests** (`tests/monitoring/test_alert_manager.py`):\n- Test each alert condition independently\n- Test alert triggering and logging\n- Test threshold configuration loading\n- Test alert suppression (don't re-alert same condition)\n- **Coverage Target:** >90%\n\n### Integration Testing\n\n**End-to-End Monitoring** (`tests/integration/test_monitoring_system.py`):\n1. **Metrics Export**:\n   - Start monitors, wait 10s\n   - Query http://localhost:8000/metrics\n   - Verify all expected metrics present\n\n2. **Alert Triggering**:\n   - Simulate high memory (mock psutil to return 85%)\n   - Verify alert triggered and logged\n   - Verify `alerts_triggered_total{alert_type=\"high_memory\"}` incremented\n\n3. **Diversity Collapse Detection**:\n   - Record diversity <0.1 for 5 iterations\n   - Verify diversity collapse alert triggered\n   - Record diversity >0.3, verify alert clears\n\n4. **Container Monitoring**:\n   - Create 3 containers, verify `active_containers=3`\n   - Clean up 2, verify `active_containers=1`\n   - Leave 1 orphaned, verify `orphaned_containers=1`\n   - Call cleanup, verify `orphaned_containers=0`\n\n**Performance Tests**:\n- Measure metrics collection overhead (<1% of iteration time)\n- Test with 100 containers (verify no slowdown)\n- Test 1000 iteration history (verify diversity calculation <100ms)\n\n### End-to-End Testing\n\n**Production Simulation** (`tests/integration/test_production_monitoring.py`):\n1. Run 50-iteration loop with full monitoring enabled\n2. Inject failures: kill containers, exhaust memory, force diversity collapse\n3. Verify all alerts triggered correctly\n4. Verify Grafana dashboard reflects actual events\n5. Verify final metrics export to JSON\n\n**Grafana Dashboard Validation**:\n1. Import `config/grafana_dashboard.json`\n2. Run 20-iteration test\n3. Manually verify all 4 panels update correctly\n4. Verify annotations for champion updates appear\n5. Verify alert thresholds displayed correctly\n\n## Grafana Dashboard Design\n\n### config/grafana_dashboard.json\n\n**Panel 1: Resource Usage** (Top Left)\n- Graph: Memory usage (GB) over time\n- Graph: CPU usage (%) over time\n- Graph: Execution time per iteration (seconds)\n- Alert Threshold Lines: Memory at 80%, CPU at 90%\n\n**Panel 2: Strategy Performance** (Top Right)\n- Graph: Success rate (%) rolling 10-iteration average\n- Graph: Sharpe ratio over iterations\n- Graph: Max drawdown over iterations\n- Annotation: Champion update events\n\n**Panel 3: Diversity Metrics** (Bottom Left)\n- Graph: Population diversity (0-1) over time\n- Graph: Unique strategy count vs total population\n- Graph: Champion age (iterations since update)\n- Alert Threshold Line: Diversity at 0.1\n\n**Panel 4: Container Stats** (Bottom Right)\n- Graph: Active containers over time\n- Graph: Total memory used by containers (GB)\n- Graph: Orphaned containers count\n- Alert Threshold Line: Orphaned at 3\n\n## Configuration Example\n\n### config/monitoring_config.yaml\n```yaml\nmonitoring:\n  enabled: true\n  prometheus:\n    port: 8000\n    metrics_path: \"/metrics\"\n    scrape_interval: 5  # seconds\n\n  resource_monitor:\n    enabled: true\n    collection_interval: 5  # seconds\n\n  diversity_monitor:\n    enabled: true\n    diversity_collapse_threshold: 0.1\n    diversity_collapse_window: 5\n\n  container_monitor:\n    enabled: true\n    stats_collection_interval: 10  # seconds\n\n  alerts:\n    enabled: true\n    evaluation_interval: 10  # seconds\n\n    memory_threshold_percent: 80.0\n    cpu_threshold_percent: 90.0\n\n    diversity_collapse:\n      threshold: 0.1\n      window: 5\n\n    champion_staleness:\n      threshold_iterations: 20\n\n    success_rate:\n      threshold_percent: 20.0\n      window_iterations: 10\n\n    orphaned_containers:\n      threshold: 3\n      auto_cleanup: true\n\n  export:\n    final_metrics_path: \"metrics_final.json\"\n    prometheus_snapshot_path: \"prometheus_metrics_final.txt\"\n    retention_days: 30\n\ngrafana:\n  dashboard_path: \"config/grafana_dashboard.json\"\n  datasource_url: \"http://localhost:9090\"  # Prometheus URL\n  refresh_interval: \"5s\"\n```\n\n## Prometheus Metrics Catalog\n\n### Resource Metrics\n- `system_memory_usage_bytes` (Gauge): Current system memory usage\n- `system_memory_total_bytes` (Gauge): Total system memory\n- `system_cpu_usage_percent` (Gauge): CPU usage percentage\n- `execution_time_seconds` (Histogram): Iteration execution time\n\n### Strategy Performance Metrics\n- `strategy_success_total` (Counter): Successful strategy executions\n- `strategy_failure_total` (Counter): Failed strategy executions\n- `strategy_sharpe_ratio` (Gauge): Most recent Sharpe ratio\n- `strategy_max_drawdown` (Gauge): Most recent max drawdown\n\n### Diversity Metrics\n- `population_diversity` (Gauge): Diversity score 0.0-1.0\n- `unique_strategy_count` (Gauge): Number of unique strategies\n- `champion_staleness_iterations` (Gauge): Iterations since champion update\n- `champion_updates_total` (Counter): Total champion updates\n\n### Container Metrics\n- `active_containers` (Gauge): Currently running containers\n- `container_memory_usage_bytes` (Gauge, labeled by container_id): Memory per container\n- `container_cpu_usage_percent` (Gauge, labeled by container_id): CPU per container\n- `orphaned_containers` (Gauge): Containers not properly cleaned up\n- `container_cleanup_success_total` (Counter): Successful cleanups\n- `container_cleanup_failure_total` (Counter): Failed cleanups\n\n### Alert Metrics\n- `alerts_triggered_total` (Counter, labeled by alert_type): Total alerts triggered\n- `alert_active` (Gauge, labeled by alert_type): Current active alerts (1=active, 0=resolved)\n\n## Deployment Checklist\n\n1. ✅ Install Prometheus (v2.40+)\n2. ✅ Configure Prometheus scrape job for http://localhost:8000/metrics\n3. ✅ Install Grafana (v9.0+)\n4. ✅ Import dashboard from `config/grafana_dashboard.json`\n5. ✅ Configure alert notification channels (email, Slack, PagerDuty)\n6. ✅ Set `monitoring.enabled: true` in config\n7. ✅ Run integration tests to verify metrics export\n8. ✅ Verify Grafana dashboard displays correctly\n9. ✅ Test alert triggering with simulated conditions\n\n## Performance Considerations\n\n- **Metrics Collection Overhead**: <1% of iteration execution time\n- **Memory Overhead**: ~50MB for Prometheus client + metrics storage\n- **Network Overhead**: ~10KB/s for Prometheus scraping\n- **Storage**: ~1GB for 30 days of metrics at 5s resolution\n- **Alert Evaluation**: <50ms per evaluation cycle (10s interval)\n\n## Future Enhancements (Out of Scope)\n\n- **Custom Metrics**: User-defined strategy-specific metrics\n- **Anomaly Detection**: ML-based alert thresholds\n- **Distributed Tracing**: OpenTelemetry integration for multi-node setups\n- **Long-term Storage**: Prometheus remote write to InfluxDB/Victoria Metrics\n- **Advanced Dashboards**: Template variables, drill-down panels\n",
  "fileStats": {
    "size": 18663,
    "lines": 500,
    "lastModified": "2025-10-24T22:15:28.690Z"
  },
  "comments": []
}