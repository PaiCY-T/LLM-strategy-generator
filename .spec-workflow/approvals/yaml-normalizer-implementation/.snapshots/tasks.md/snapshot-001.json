{
  "id": "snapshot_1761519734770_z0r6bkrfr",
  "approvalId": "approval_1761519734637_gqisyfnjg",
  "approvalTitle": "Tasks Document Approval - YAML Normalizer Implementation",
  "version": 1,
  "timestamp": "2025-10-26T23:02:14.770Z",
  "trigger": "initial",
  "status": "pending",
  "content": "# Tasks Document\n\n## Overview\n\nThis document breaks down the yaml-normalizer-implementation spec into executable tasks with detailed prompts for autonomous execution. The implementation follows TDD principles using 15 real failure cases to achieve 90%+ YAML validation success rate.\n\n**Estimated Total Effort**: 4.5 hours\n- Task 1: 30 minutes (Test fixture extraction)\n- Task 2: 2 hours (Core normalizer with TDD)\n- Task 3: 1 hour (Pydantic model generation)\n- Task 4: 30 minutes (Integration)\n- Task 5: 15 minutes (Configuration)\n- Task 6: 15 minutes (Integration testing)\n\n---\n\n## Task List\n\n- [ ] 1. Extract Test Cases from Validation Reports\n  - **File**: `tests/generators/fixtures/yaml_normalizer_cases.py` (new)\n  - **Purpose**: Create test fixtures from 15 real failure cases for TDD workflow\n  - **Requirements**: 4.1, 4.2, 4.3\n  - **Estimated Effort**: 30 minutes\n  - **_Leverage**:\n    - `complete_validation_output.txt` (20 iterations, 15 failures)\n    - `QUICKWINS_VALIDATION_REPORT.md` (additional 10 iterations, 7 failures)\n  - **_Requirements**: 4.1, 4.2, 4.3\n  - **_Prompt**: |\n      Implement the task for spec yaml-normalizer-implementation, first run spec-workflow-guide to get the workflow guide then implement the task:\n\n      **Role**: Test Engineer with expertise in test data extraction and fixture creation\n\n      **Task**: Extract 15 real failure cases from validation reports and create structured test fixtures for TDD workflow.\n\n      Create the following file:\n      - `tests/generators/fixtures/yaml_normalizer_cases.py` - Test fixtures module with 15 categorized failure cases\n\n      **Restrictions**:\n      - Do not modify validation report files (read-only)\n      - Maintain exact LLM output (no manual fixes)\n      - Follow project principle \"避免過度工程化\" (simple data structures, no complex parsing)\n      - Organize by error category (40% indicators, 30% aliases, 15% types, 10% params, 5% unfixable)\n\n      **_Leverage**:\n      - `/mnt/c/Users/jnpi/documents/finlab/complete_validation_output.txt` - Extract iterations 1, 3, 5, 7, 9, 11, 13, 15, 17, 19\n      - `/mnt/c/Users/jnpi/documents/finlab/QUICKWINS_VALIDATION_REPORT.md` - Extract additional failure cases\n      - Error distribution: 40% indicators array→object, 30% field aliases, 15% type case, 10% nested params, 5% Jinja/unfixable\n\n      **_Requirements**: 4.1, 4.2, 4.3\n\n      **Success Criteria**:\n      - Exactly 15 test cases extracted (6 indicators, 5 aliases, 3 types, 2 params, 1 unfixable)\n      - Each fixture includes: raw_yaml (input), expected_yaml (output), error_description\n      - Fixtures are Python dicts (not strings) for easy testing\n      - All fixtures validate against schema after normalization (except unfixable)\n      - Code coverage N/A (fixture data only)\n\n      **Instructions**:\n      1. Read tasks.md and change status from [ ] to [-] for this task\n      2. Read complete_validation_output.txt and extract failure cases\n      3. Read QUICKWINS_VALIDATION_REPORT.md for additional cases\n      4. Create tests/generators/fixtures/yaml_normalizer_cases.py with structured data\n      5. Verify fixture count and categorization (15 total, correct distribution)\n      6. Change status from [-] to [x] when complete\n\n- [ ] 2. Implement YAMLNormalizer with TDD\n  - **Files**:\n    - `src/generators/yaml_normalizer.py` (new) - Core normalizer implementation\n    - `tests/generators/test_yaml_normalizer.py` (new) - Unit tests with 15 test cases\n  - **Purpose**: Implement stateless YAML transformation layer with 5 transformation patterns\n  - **Requirements**: 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7\n  - **Estimated Effort**: 2 hours\n  - **_Leverage**:\n    - `tests/generators/fixtures/yaml_normalizer_cases.py` (from Task 1)\n    - `src/utils/exceptions.py` - Existing exception patterns\n    - `src/utils/logger.py` - Existing logging infrastructure\n  - **_Requirements**: 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7\n  - **_Prompt**: |\n      Implement the task for spec yaml-normalizer-implementation, first run spec-workflow-guide to get the workflow guide then implement the task:\n\n      **Role**: Backend Developer with expertise in data transformation and test-driven development\n\n      **Task**: Implement YAMLNormalizer class with 5 transformation patterns using TDD approach with 15 real failure cases.\n\n      Create/modify the following files:\n      - `src/generators/yaml_normalizer.py` (new) - Core normalizer with pure function API\n      - `tests/generators/test_yaml_normalizer.py` (new) - Comprehensive unit tests\n      - `src/utils/exceptions.py` (modify) - Add NormalizationError exception class\n\n      **Restrictions**:\n      - Use only Python stdlib (copy, logging, typing, re) - no external dependencies\n      - Stateless pure function design: normalize_yaml(dict) -> dict\n      - No AST manipulation (follow \"避免過度工程化\" principle)\n      - Deep copy input to prevent mutation\n      - No modification of schemas/strategy_schema_v1.json\n\n      **_Leverage**:\n      - `tests/generators/fixtures/yaml_normalizer_cases.py` - 15 real test cases\n      - `src/utils/exceptions.py` - Exception patterns (add NormalizationError)\n      - `src/utils/logger.py` - Structured logging\n      - Design document transformation patterns (5 transformations)\n\n      **_Requirements**: 1.1 (indicators array→object), 1.2 (field aliases), 1.3 (params flattening), 1.4 (type uppercase), 1.5 (Jinja detection), 1.6 (required fields check), 1.7 (immutability)\n\n      **Success Criteria**:\n      - All 15 test cases pass (pytest)\n      - Code coverage >80% for yaml_normalizer.py\n      - All transformations logged at INFO level\n      - NormalizationError raised for Jinja and missing fields\n      - Input dict not mutated (immutability test passes)\n      - Configuration constants externalized (FIELD_ALIASES, INDICATOR_TYPE_MAP)\n      - Type hints on all functions (mypy strict compliance)\n\n      **Instructions**:\n      1. Read tasks.md and change status from [ ] to [-] for this task\n      2. Follow TDD workflow:\n         a. RED: Write failing test for transformation 1 (indicators array→object)\n         b. GREEN: Implement _normalize_indicators() to pass test\n         c. REFACTOR: Extract configuration constants\n         d. Repeat for remaining 4 transformations\n      3. Implement public API: normalize_yaml(raw_data: dict) -> dict\n      4. Implement private functions: _check_for_jinja, _validate_required_fields, _normalize_indicators, _flatten_params, _map_field_aliases, _normalize_indicator_type, _normalize_conditions\n      5. Add NormalizationError to src/utils/exceptions.py\n      6. Run pytest to verify all tests pass\n      7. Run pytest-cov to verify >80% coverage\n      8. Change status from [-] to [x] when complete\n\n- [ ] 3. Generate Pydantic Models from Schema\n  - **File**: `src/models/strategy_models.py` (new)\n  - **Purpose**: Auto-generate type-safe Pydantic models from JSON Schema with field validators\n  - **Requirements**: 2.1, 2.2, 2.3, 2.4\n  - **Estimated Effort**: 1 hour\n  - **_Leverage**:\n    - `schemas/strategy_schema_v1.json` - Source schema\n    - `datamodel-code-generator` CLI tool (existing dependency)\n  - **_Requirements**: 2.1, 2.2, 2.3, 2.4\n  - **_Prompt**: |\n      Implement the task for spec yaml-normalizer-implementation, first run spec-workflow-guide to get the workflow guide then implement the task:\n\n      **Role**: Backend Developer with expertise in Pydantic and schema-driven development\n\n      **Task**: Generate Pydantic models from JSON Schema and add field validators for automatic type conversions.\n\n      Create the following file:\n      - `src/models/strategy_models.py` (new) - Auto-generated Pydantic models with custom validators\n\n      **Restrictions**:\n      - Do not modify schemas/strategy_schema_v1.json (read-only)\n      - Use datamodel-code-generator for initial generation (do not hand-write models)\n      - Target Python 3.10+ (existing project standard)\n      - Pydantic v2.11.0+ (existing dependency)\n\n      **_Leverage**:\n      - `schemas/strategy_schema_v1.json` - Source schema for generation\n      - Design document Component 2 for generation command and validator examples\n      - Existing pydantic dependency (no new packages)\n\n      **_Requirements**: 2.1 (enum enforcement), 2.2 (field validators), 2.3 (detailed errors), 2.4 (discriminated unions)\n\n      **Success Criteria**:\n      - Models successfully generated from schema\n      - All enum types enforced as Literal types\n      - Field validator added for type uppercase conversion (double-insurance)\n      - Models support oneOf patterns via discriminated unions (entry_conditions, exit_conditions)\n      - ValidationError includes field paths (e.g., \"indicators.technical_indicators.0.type\")\n      - Models can validate normalized YAML from Task 2 tests\n      - Import and instantiation work correctly\n\n      **Instructions**:\n      1. Read tasks.md and change status from [ ] to [-] for this task\n      2. Run datamodel-code-generator command:\n         ```bash\n         datamodel-code-generator \\\n           --input schemas/strategy_schema_v1.json \\\n           --output src/models/strategy_models.py \\\n           --input-file-type jsonschema \\\n           --target-python-version 3.10\n         ```\n      3. Add field validator to TechnicalIndicator model:\n         ```python\n         @field_validator('type', mode='before')\n         @classmethod\n         def uppercase_type(cls, v: Any) -> str:\n             return v.upper() if isinstance(v, str) else v\n         ```\n      4. Test model validation with sample normalized YAML from Task 2\n      5. Verify detailed error messages include field paths\n      6. Change status from [-] to [x] when complete\n\n- [ ] 4. Integrate Normalizer into YAMLSchemaValidator\n  - **File**: `src/generators/yaml_schema_validator.py` (modify)\n  - **Purpose**: Add normalization step before validation with feature flag support\n  - **Requirements**: 3.1, 3.2, 3.3, 3.4, 3.5\n  - **Estimated Effort**: 30 minutes\n  - **_Leverage**:\n    - `src/generators/yaml_normalizer.py` (from Task 2)\n    - Existing `validate()` method in YAMLSchemaValidator\n  - **_Requirements**: 3.1, 3.2, 3.3, 3.4, 3.5\n  - **_Prompt**: |\n      Implement the task for spec yaml-normalizer-implementation, first run spec-workflow-guide to get the workflow guide then implement the task:\n\n      **Role**: Integration Engineer with expertise in backward-compatible API evolution\n\n      **Task**: Modify YAMLSchemaValidator.validate() to call normalizer before validation with optional parameter for backward compatibility.\n\n      Modify the following file:\n      - `src/generators/yaml_schema_validator.py` - Add normalizer integration to validate() method\n\n      **Restrictions**:\n      - Maintain 100% backward compatibility (926 tests must pass)\n      - Graceful degradation: fall back to direct validation on normalizer failure\n      - No modification to existing validation logic (only add pre-processing)\n      - Feature flag controlled (normalize parameter defaults to False initially)\n\n      **_Leverage**:\n      - `src/generators/yaml_normalizer.py` - normalize_yaml() function\n      - Existing validate() method signature and logic\n      - Design document Component 3 for integration pattern\n\n      **_Requirements**: 3.1 (integration point), 3.2 (pytest baseline), 3.3 (flow documentation), 3.4 (backward compatibility), 3.5 (unit tests)\n\n      **Success Criteria**:\n      - validate() method has new optional parameter: normalize=False\n      - When normalize=True, normalizer called before validation\n      - NormalizationError caught with warning log, validation continues with original YAML\n      - Unexpected errors caught with error log, validation continues (graceful degradation)\n      - All 926 existing tests pass with normalize=False (backward compatibility)\n      - Integration logged: \"YAML normalization successful\" or \"Normalization failed: {error}\"\n      - No breaking changes to method signature (optional parameter)\n\n      **Instructions**:\n      1. Read tasks.md and change status from [ ] to [-] for this task\n      2. Modify validate() method signature: add normalize: bool = False parameter\n      3. Add normalizer call at method start (before existing validation):\n         ```python\n         if normalize:\n             try:\n                 from src.generators.yaml_normalizer import normalize_yaml\n                 yaml_spec = normalize_yaml(yaml_spec)\n                 logger.info(\"YAML normalization successful\")\n             except NormalizationError as e:\n                 logger.warning(f\"Normalization failed: {e}. Falling back to direct validation.\")\n             except Exception as e:\n                 logger.error(f\"Unexpected normalization error: {e}. Falling back.\")\n         ```\n      4. Run pytest to verify 926 tests still pass\n      5. Add unit test for normalizer integration (normalize=True case)\n      6. Change status from [-] to [x] when complete\n\n- [ ] 5. Add Feature Flag Configuration\n  - **File**: `config/learning_system.yaml` (modify)\n  - **Purpose**: Add yaml_normalization configuration section with feature flag\n  - **Requirements**: 5.1, 5.2, 5.3\n  - **Estimated Effort**: 15 minutes\n  - **_Leverage**:\n    - Existing `config/learning_system.yaml` structure\n    - Design document Data Models section for configuration structure\n  - **_Requirements**: 5.1, 5.2, 5.3\n  - **_Prompt**: |\n      Implement the task for spec yaml-normalizer-implementation, first run spec-workflow-guide to get the workflow guide then implement the task:\n\n      **Role**: DevOps Engineer with expertise in configuration management\n\n      **Task**: Add yaml_normalization configuration section to learning_system.yaml for feature flag control.\n\n      Modify the following file:\n      - `config/learning_system.yaml` - Add yaml_normalization section\n\n      **Restrictions**:\n      - Follow existing YAML structure and conventions\n      - Default enabled: true (for new runs after Phase 1 validation)\n      - Do not modify other configuration sections\n      - Keep configuration simple (avoid nested complexity)\n\n      **_Leverage**:\n      - Existing `config/learning_system.yaml` structure\n      - Design document configuration model for exact structure\n\n      **_Requirements**: 5.1 (feature flag), 5.2 (graceful degradation), 5.3 (success rate tracking)\n\n      **Success Criteria**:\n      - yaml_validation section exists or is created\n      - normalization subsection added with enabled: true\n      - Optional debug_mode: false for verbose logging\n      - Optional strict_mode: false for development testing\n      - Configuration loads without errors\n      - Feature flag can be toggled without code changes\n\n      **Instructions**:\n      1. Read tasks.md and change status from [ ] to [-] for this task\n      2. Open config/learning_system.yaml\n      3. Add or modify yaml_validation section:\n         ```yaml\n         yaml_validation:\n           enabled: true\n           normalization:\n             enabled: true  # Feature flag for normalizer\n             debug_mode: false  # Log all transformations\n             strict_mode: false  # Fail on any transformation (testing only)\n         ```\n      4. Verify YAML syntax is valid\n      5. Test configuration loading (no errors)\n      6. Change status from [-] to [x] when complete\n\n- [ ] 6. Integration Testing and Validation\n  - **Files**:\n    - `tests/integration/test_yaml_normalizer_integration.py` (new)\n  - **Purpose**: Validate end-to-end integration and measure success rate improvement\n  - **Requirements**: 3.5, 5.4\n  - **Estimated Effort**: 15 minutes\n  - **_Leverage**:\n    - All previous tasks (1-5)\n    - Existing integration test patterns\n    - Real LLM API for validation\n  - **_Requirements**: 3.5 (unit test coverage), 5.4 (success rate improvement)\n  - **_Prompt**: |\n      Implement the task for spec yaml-normalizer-implementation, first run spec-workflow-guide to get the workflow guide then implement the task:\n\n      **Role**: QA Engineer with expertise in integration testing and metrics validation\n\n      **Task**: Create integration tests and run validation to measure success rate improvement from 25% to 70%+.\n\n      Create the following file:\n      - `tests/integration/test_yaml_normalizer_integration.py` (new) - End-to-end integration tests\n\n      **Restrictions**:\n      - All 926 existing tests must pass (no regressions)\n      - Use real LLM API for 10-iteration validation test\n      - Measure success rate improvement (baseline vs. with normalizer)\n      - No modification to InnovationEngine core logic (only configuration)\n\n      **_Leverage**:\n      - All components from Tasks 1-5\n      - Existing integration test patterns in tests/integration/\n      - Design document Testing Strategy section\n      - 15 test fixtures from Task 1 for unit-level integration\n\n      **_Requirements**: 3.5 (integration tests), 5.4 (success rate target 70%+)\n\n      **Success Criteria**:\n      - All 926 existing tests pass (pytest)\n      - Integration test file created with 4 test scenarios:\n        1. End-to-end with 15 real failure cases (≥13/15 pass = 87%)\n        2. InnovationEngine integration with mock LLM\n        3. Backward compatibility verification\n        4. Graceful degradation on normalizer failure\n      - 10-iteration validation test with real LLM API shows ≥70% success rate\n      - Success rate improvement documented (baseline 25% → 70%+ measured)\n      - Performance overhead measured (<10ms per iteration)\n\n      **Instructions**:\n      1. Read tasks.md and change status from [ ] to [-] for this task\n      2. Run pytest to verify 926 tests pass (baseline)\n      3. Create tests/integration/test_yaml_normalizer_integration.py:\n         - Test 1: Load 15 fixtures, normalize, validate (expect ≥13 pass)\n         - Test 2: Mock LLM in InnovationEngine, verify normalizer called\n         - Test 3: Run with normalize=False, verify no regressions\n         - Test 4: Force normalizer exception, verify fallback\n      4. Run integration tests (pytest tests/integration/test_yaml_normalizer_integration.py)\n      5. Run 10-iteration validation test with real LLM API:\n         ```bash\n         python scripts/test_yaml_normalization_e2e.py \\\n           --iterations 10 \\\n           --template momentum \\\n           --enable-normalizer \\\n           --compare-baseline\n         ```\n      6. Document results: success rate (before/after), performance overhead\n      7. Change status from [-] to [x] when complete\n\n---\n\n## Task Dependencies\n\n```mermaid\ngraph TD\n    T1[Task 1: Extract Test Cases] --> T2[Task 2: Implement Normalizer]\n    T2 --> T3[Task 3: Generate Pydantic Models]\n    T2 --> T4[Task 4: Integrate Validator]\n    T3 --> T6[Task 6: Integration Testing]\n    T4 --> T5[Task 5: Add Configuration]\n    T5 --> T6\n```\n\n**Critical Path**: T1 → T2 → T4 → T5 → T6 (3 hours 15 minutes)\n\n**Parallel Work**: T3 can be done in parallel with T4 (saves 30 minutes)\n\n---\n\n## Success Metrics\n\n### Phase 1 Target (After Task 6)\n- **Validation Success Rate**: 70-75% (from 25% baseline)\n- **Test Coverage**: >80% for yaml_normalizer.py\n- **Backward Compatibility**: 100% (926 tests pass)\n- **Performance Overhead**: <10ms per iteration (<1% of total)\n\n### Future Phases (Not in This Spec)\n- **Phase 2** (Pydantic Integration): 80-85% success rate\n- **Phase 3** (Pipeline Integration): 85-90% success rate\n- **Phase 4** (Prompt Optimization): 90-95% success rate\n\n---\n\n## Risk Mitigation\n\n### Risk 1: Test Fixtures Don't Cover All Failure Patterns\n- **Mitigation**: Extract from two sources (20 + 10 iterations)\n- **Fallback**: Add more fixtures in Task 2 as transformations are implemented\n\n### Risk 2: Normalizer Breaks Existing Tests\n- **Mitigation**: Feature flag defaults to False, graceful degradation\n- **Validation**: Run 926 tests at every integration point\n\n### Risk 3: Success Rate Doesn't Reach 70% Target\n- **Mitigation**: TDD approach ensures each transformation works\n- **Measurement**: 10-iteration validation with real LLM in Task 6\n- **Fallback**: Iterate on transformation patterns based on remaining failures\n\n---\n\n**Document Version**: 1.0\n**Created**: 2025-10-27\n**Status**: Draft - Pending Approval\n**Total Tasks**: 6\n**Estimated Total Effort**: 4.5 hours\n**Critical Path**: 3 hours 15 minutes\n",
  "fileStats": {
    "size": 20331,
    "lines": 429,
    "lastModified": "2025-10-26T23:02:08.722Z"
  },
  "comments": []
}