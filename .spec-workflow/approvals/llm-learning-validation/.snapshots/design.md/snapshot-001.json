{
  "id": "snapshot_1762442289737_l2yyfo5d5",
  "approvalId": "approval_1762442289615_1rmn4ee1d",
  "approvalTitle": "LLM Learning Validation - Design",
  "version": 1,
  "timestamp": "2025-11-06T15:18:09.737Z",
  "trigger": "initial",
  "status": "pending",
  "content": "# LLM Learning Validation - Technical Design\n\n## 1. System Architecture\n\n### 1.1 High-Level Architecture\n```\n┌─────────────────────────────────────────────────────────────────┐\n│                   EXPERIMENT ORCHESTRATOR                        │\n│  ┌────────────────────────────────────────────────────────┐    │\n│  │              Configuration Manager                      │    │\n│  │  - Load YAML config                                    │    │\n│  │  - Validate experiment parameters                      │    │\n│  └────────────────────────────────────────────────────────┘    │\n│                              │                                   │\n│              ┌───────────────┼───────────────┐                 │\n│              ▼               ▼               ▼                 │\n│    ┌──────────────┐ ┌──────────────┐ ┌──────────────┐        │\n│    │  Group A     │ │  Group B     │ │  Group C     │        │\n│    │  Hybrid 30%  │ │  FG-Only 0%  │ │  LLM-Only    │        │\n│    │              │ │              │ │  100%        │        │\n│    └──────┬───────┘ └──────┬───────┘ └──────┬───────┘        │\n│           │                │                │                 │\n│           └────────────────┼────────────────┘                 │\n│                            ▼                                   │\n│              ┌─────────────────────────────┐                  │\n│              │  Novelty Quantification     │                  │\n│              │  System (3 Layers)          │                  │\n│              └─────────────┬───────────────┘                  │\n│                            ▼                                   │\n│              ┌─────────────────────────────┐                  │\n│              │  Statistical Analysis       │                  │\n│              │  Pipeline                   │                  │\n│              └─────────────┬───────────────┘                  │\n│                            ▼                                   │\n│              ┌─────────────────────────────┐                  │\n│              │  Results & Reports          │                  │\n│              └─────────────────────────────┘                  │\n└─────────────────────────────────────────────────────────────────┘\n```\n\n### 1.2 Component Overview\n\n#### 1.2.1 Experiment Orchestrator\n- **Responsibility**: Manage end-to-end experiment execution\n- **Location**: `experiments/llm_learning_validation/orchestrator.py`\n- **Key Functions**:\n  - Load and validate configuration\n  - Initialize LearningSystem instances per group\n  - Execute iterations sequentially\n  - Collect and aggregate results\n\n#### 1.2.2 Novelty Quantification System\n- **Responsibility**: Measure strategy innovation across 3 layers\n- **Location**: `src/analysis/novelty/`\n- **Key Components**:\n  - Factor Diversity Analyzer\n  - Combination Pattern Detector\n  - Logic Complexity Analyzer\n  - Novelty Scorer (aggregator)\n\n#### 1.2.3 Statistical Analysis Pipeline\n- **Responsibility**: Statistical testing and visualization\n- **Location**: `src/analysis/`\n- **Key Components**:\n  - Statistical Tests Module\n  - Visualization Engine\n  - Report Generator\n\n#### 1.2.4 Data Layer\n- **Responsibility**: Persistence and data integrity\n- **Location**: `src/learning/iteration_history.py` (extended)\n- **Key Functions**:\n  - Enhanced IterationRecord dataclass\n  - JSON serialization/deserialization\n  - Data validation\n\n## 2. Detailed Component Design\n\n### 2.1 Configuration System\n\n#### 2.1.1 Config Schema\n```yaml\n# experiments/llm_learning_validation/config.yaml\n\nexperiment:\n  name: \"llm-learning-validation\"\n  version: \"1.0.0\"\n  description: \"A/B/C testing to validate LLM learning effectiveness\"\n\nphases:\n  pilot:\n    iterations_per_run: 50\n    runs_per_group: 2\n    total_iterations: 300  # 50 × 2 × 3\n\n  full:\n    iterations_per_run: 200\n    runs_per_group: 5\n    total_iterations: 3000  # 200 × 5 × 3\n\ngroups:\n  - id: \"hybrid\"\n    name: \"Hybrid (30% LLM)\"\n    innovation_rate: 0.30\n    output_dir: \"artifacts/experiments/llm_validation/hybrid\"\n\n  - id: \"fg_only\"\n    name: \"Factor Graph Only (0% LLM)\"\n    innovation_rate: 0.00\n    output_dir: \"artifacts/experiments/llm_validation/fg_only\"\n\n  - id: \"llm_only\"\n    name: \"LLM Only (100% LLM)\"\n    innovation_rate: 1.00\n    output_dir: \"artifacts/experiments/llm_validation/llm_only\"\n\nnovelty_weights:\n  layer1_factor_diversity: 0.30\n  layer2_combination_patterns: 0.40\n  layer3_logic_complexity: 0.30\n\nvalidation:\n  max_failure_rate: 0.05  # 5%\n  min_sharpe_threshold: 0.5\n  statistical_significance: 0.05\n\ngo_no_go_criteria:\n  min_criteria_met: 2\n  statistical_signal_pvalue: 0.10\n  novelty_advantage_threshold: 0.15  # 15%\n  execution_time_buffer: 1.50  # 150%\n```\n\n#### 2.1.2 Config Loader\n```python\nfrom dataclasses import dataclass\nfrom typing import List, Dict\nimport yaml\n\n@dataclass\nclass GroupConfig:\n    id: str\n    name: str\n    innovation_rate: float\n    output_dir: str\n\n@dataclass\nclass PhaseConfig:\n    iterations_per_run: int\n    runs_per_group: int\n    total_iterations: int\n\n@dataclass\nclass ExperimentConfig:\n    name: str\n    version: str\n    description: str\n    phases: Dict[str, PhaseConfig]\n    groups: List[GroupConfig]\n    novelty_weights: Dict[str, float]\n    validation: Dict[str, float]\n    go_no_go_criteria: Dict[str, float]\n\n    @classmethod\n    def load(cls, config_path: str) -> 'ExperimentConfig':\n        \"\"\"Load and validate experiment configuration\"\"\"\n        with open(config_path, 'r') as f:\n            data = yaml.safe_load(f)\n\n        # Validation logic\n        cls._validate_config(data)\n\n        return cls(...)\n\n    @staticmethod\n    def _validate_config(data: dict) -> None:\n        \"\"\"Validate configuration schema and constraints\"\"\"\n        # Check required fields\n        # Validate innovation_rates in [0, 1]\n        # Ensure output directories don't conflict\n        # Validate novelty weights sum to 1.0\n        pass\n```\n\n### 2.2 Orchestrator Design\n\n#### 2.2.1 Class Structure\n```python\nfrom typing import List, Dict\nfrom pathlib import Path\nimport logging\n\nclass ExperimentOrchestrator:\n    \"\"\"Manages A/B/C experiment execution and result aggregation\"\"\"\n\n    def __init__(self, config_path: str):\n        self.config = ExperimentConfig.load(config_path)\n        self.groups: Dict[str, LearningSystem] = {}\n        self.results: Dict[str, List[IterationRecord]] = {}\n        self.logger = self._setup_logging()\n\n    def initialize_groups(self) -> None:\n        \"\"\"Initialize LearningSystem instances for each experimental group\"\"\"\n        for group_config in self.config.groups:\n            learning_system = LearningSystem(\n                config_path=\"config/learning_system.yaml\",\n                innovation_rate_override=group_config.innovation_rate\n            )\n            self.groups[group_config.id] = learning_system\n\n            # Create output directory\n            Path(group_config.output_dir).mkdir(parents=True, exist_ok=True)\n\n    def run_phase(self, phase: str = \"pilot\", dry_run: bool = False) -> None:\n        \"\"\"Execute specified experiment phase\"\"\"\n        phase_config = self.config.phases[phase]\n\n        for group_id, learning_system in self.groups.items():\n            self.logger.info(f\"Starting {phase} for group: {group_id}\")\n\n            for run in range(phase_config.runs_per_group):\n                self.logger.info(f\"  Run {run + 1}/{phase_config.runs_per_group}\")\n\n                results = self._run_iterations(\n                    learning_system=learning_system,\n                    group_id=group_id,\n                    num_iterations=phase_config.iterations_per_run,\n                    run_id=run,\n                    dry_run=dry_run\n                )\n\n                self.results.setdefault(group_id, []).extend(results)\n                self._save_intermediate_results(group_id, results)\n\n    def _run_iterations(\n        self,\n        learning_system: LearningSystem,\n        group_id: str,\n        num_iterations: int,\n        run_id: int,\n        dry_run: bool\n    ) -> List[IterationRecord]:\n        \"\"\"Execute iterations for a single run\"\"\"\n        results = []\n        failure_count = 0\n\n        for i in range(num_iterations):\n            try:\n                # Execute iteration\n                strategy = learning_system.generate_strategy()\n                metrics = learning_system.evaluate_strategy(strategy)\n\n                # Calculate novelty scores\n                novelty_scores = NoveltyScorer.score(strategy.code)\n\n                # Create record\n                record = IterationRecord(\n                    iteration_num=i,\n                    strategy_code=strategy.code,\n                    metrics=metrics,\n                    novelty_scores=novelty_scores.to_dict(),\n                    experiment_group=group_id,\n                    run_id=run_id\n                )\n\n                results.append(record)\n\n                # Logging\n                self.logger.info(\n                    f\"    Iteration {i}: Sharpe={metrics.sharpe_ratio:.4f}, \"\n                    f\"Novelty={novelty_scores.total_score:.4f}\"\n                )\n\n            except Exception as e:\n                failure_count += 1\n                self.logger.error(f\"    Iteration {i} failed: {str(e)}\")\n\n                if failure_count / (i + 1) > self.config.validation.max_failure_rate:\n                    raise RuntimeError(\n                        f\"Failure rate exceeded threshold: \"\n                        f\"{failure_count}/{i+1} = {failure_count/(i+1):.2%}\"\n                    )\n\n        return results\n\n    def generate_report(self, phase: str) -> Path:\n        \"\"\"Generate comprehensive analysis report\"\"\"\n        analyzer = ExperimentAnalyzer(\n            results=self.results,\n            config=self.config\n        )\n\n        report_path = analyzer.generate_html_report(phase=phase)\n        self.logger.info(f\"Report generated: {report_path}\")\n\n        return report_path\n\n    def evaluate_go_no_go(self) -> Dict[str, any]:\n        \"\"\"Evaluate go/no-go criteria for Full Study\"\"\"\n        analyzer = ExperimentAnalyzer(self.results, self.config)\n\n        criteria = {\n            \"statistical_signal\": analyzer.check_statistical_signal(),\n            \"novelty_signal\": analyzer.check_novelty_signal(),\n            \"execution_stability\": analyzer.check_execution_stability(),\n            \"champion_emergence\": analyzer.check_champion_emergence()\n        }\n\n        criteria_met = sum(criteria.values())\n        decision = \"GO\" if criteria_met >= self.config.go_no_go_criteria.min_criteria_met else \"NO-GO\"\n\n        return {\n            \"decision\": decision,\n            \"criteria_met\": criteria_met,\n            \"criteria\": criteria,\n            \"recommendation\": self._generate_recommendation(criteria)\n        }\n```\n\n### 2.3 Novelty Quantification System\n\n#### 2.3.1 Layer 1: Factor Diversity Analyzer\n```python\n# src/analysis/novelty/factor_diversity.py\n\nimport re\nfrom typing import Set, List\nimport numpy as np\n\nclass FactorDiversityAnalyzer:\n    \"\"\"Analyzes factor usage diversity and template deviation\"\"\"\n\n    def __init__(self, template_library_path: str):\n        self.templates = self._load_templates(template_library_path)\n        self.factor_pattern = re.compile(r\"df\\['(\\w+)'\\]|df\\.(\\w+)\")\n\n    def analyze_factor_usage(self, strategy_code: str) -> Set[str]:\n        \"\"\"Extract unique factors from strategy code\"\"\"\n        matches = self.factor_pattern.findall(strategy_code)\n        factors = {m[0] or m[1] for m in matches}\n        return factors\n\n    def calculate_jaccard_distance(self, factors1: Set[str], factors2: Set[str]) -> float:\n        \"\"\"Calculate Jaccard distance between two factor sets\"\"\"\n        if not factors1 and not factors2:\n            return 0.0\n\n        intersection = len(factors1 & factors2)\n        union = len(factors1 | factors2)\n\n        jaccard_similarity = intersection / union if union > 0 else 0\n        return 1.0 - jaccard_similarity\n\n    def score_template_deviation(self, factors: Set[str]) -> float:\n        \"\"\"Score how much factor set deviates from template library\"\"\"\n        if not self.templates:\n            return 0.5  # Neutral score if no templates\n\n        # Calculate minimum Jaccard distance to any template\n        min_distance = min(\n            self.calculate_jaccard_distance(factors, template_factors)\n            for template_factors in self.templates\n        )\n\n        # Normalize to [0, 1] where 1 = maximum deviation\n        return np.clip(min_distance, 0.0, 1.0)\n\n    def score(self, strategy_code: str) -> float:\n        \"\"\"Calculate Layer 1 novelty score\"\"\"\n        factors = self.analyze_factor_usage(strategy_code)\n        deviation_score = self.score_template_deviation(factors)\n\n        # Bonus for using many unique factors\n        factor_diversity_bonus = min(len(factors) / 10.0, 0.3)  # Cap at 30%\n\n        return np.clip(deviation_score + factor_diversity_bonus, 0.0, 1.0)\n```\n\n#### 2.3.2 Layer 2: Combination Pattern Detector\n```python\n# src/analysis/novelty/combination_patterns.py\n\nfrom dataclasses import dataclass\nfrom typing import List, Set, Tuple\nimport re\n\n@dataclass\nclass FactorCombo:\n    factors: Set[str]\n    weights: List[float]\n    operation: str  # 'weighted_sum', 'ratio', 'product', 'custom'\n\nclass CombinationPatternDetector:\n    \"\"\"Detects and scores novel factor combination patterns\"\"\"\n\n    def __init__(self, template_library_path: str):\n        self.template_combos = self._load_template_combinations(template_library_path)\n\n    def extract_combinations(self, strategy_code: str) -> List[FactorCombo]:\n        \"\"\"Identify factor combinations in strategy code\"\"\"\n        combos = []\n\n        # Pattern 1: Weighted sum (a*f1 + b*f2 + c*f3)\n        weighted_sum_pattern = r\"(\\d+\\.?\\d*)\\s*\\*\\s*df\\['(\\w+)'\\]\"\n        matches = re.findall(weighted_sum_pattern, strategy_code)\n        if matches:\n            weights = [float(m[0]) for m in matches]\n            factors = {m[1] for m in matches}\n            combos.append(FactorCombo(factors, weights, 'weighted_sum'))\n\n        # Pattern 2: Ratios (f1 / f2)\n        ratio_pattern = r\"df\\['(\\w+)'\\]\\s*/\\s*df\\['(\\w+)'\\]\"\n        matches = re.findall(ratio_pattern, strategy_code)\n        for m in matches:\n            combos.append(FactorCombo({m[0], m[1]}, [1.0, -1.0], 'ratio'))\n\n        # Pattern 3: Products (f1 * f2)\n        product_pattern = r\"df\\['(\\w+)'\\]\\s*\\*\\s*df\\['(\\w+)'\\]\"\n        matches = re.findall(product_pattern, strategy_code)\n        for m in matches:\n            combos.append(FactorCombo({m[0], m[1]}, [1.0, 1.0], 'product'))\n\n        return combos\n\n    def identify_novel_combinations(self, combo: FactorCombo) -> bool:\n        \"\"\"Check if combination is novel compared to templates\"\"\"\n        for template_combo in self.template_combos:\n            # Exact match on factors and operation\n            if (combo.factors == template_combo.factors and\n                combo.operation == template_combo.operation):\n                return False\n\n        return True\n\n    def score_combination_complexity(self, combo: FactorCombo) -> float:\n        \"\"\"Score combination complexity\"\"\"\n        # More factors = higher complexity\n        factor_score = min(len(combo.factors) / 5.0, 0.5)\n\n        # Non-linear operations score higher\n        operation_scores = {\n            'weighted_sum': 0.2,\n            'ratio': 0.4,\n            'product': 0.4,\n            'custom': 0.6\n        }\n        operation_score = operation_scores.get(combo.operation, 0.3)\n\n        return factor_score + operation_score\n\n    def score(self, strategy_code: str) -> float:\n        \"\"\"Calculate Layer 2 novelty score\"\"\"\n        combos = self.extract_combinations(strategy_code)\n\n        if not combos:\n            return 0.0\n\n        # Score each combination\n        combo_scores = []\n        for combo in combos:\n            novelty_bonus = 0.5 if self.identify_novel_combinations(combo) else 0.0\n            complexity_score = self.score_combination_complexity(combo)\n            combo_scores.append(novelty_bonus + complexity_score)\n\n        # Average across all combinations\n        return np.clip(np.mean(combo_scores), 0.0, 1.0)\n```\n\n#### 2.3.3 Layer 3: Logic Complexity Analyzer\n```python\n# src/analysis/novelty/logic_complexity.py\n\nimport ast\nfrom typing import List, Set\nimport numpy as np\n\nclass LogicComplexityAnalyzer:\n    \"\"\"Analyzes code logic complexity via AST parsing\"\"\"\n\n    def parse_strategy_ast(self, code: str) -> ast.Module:\n        \"\"\"Parse strategy code into AST\"\"\"\n        try:\n            # Wrap lambda in function for parsing\n            wrapped_code = f\"def strategy(df):\\n    return {code}\"\n            tree = ast.parse(wrapped_code)\n            return tree\n        except SyntaxError as e:\n            raise ValueError(f\"Failed to parse strategy code: {str(e)}\")\n\n    def measure_cyclomatic_complexity(self, ast_node: ast.Module) -> int:\n        \"\"\"Calculate cyclomatic complexity (McCabe metric)\"\"\"\n        complexity = 1  # Base complexity\n\n        for node in ast.walk(ast_node):\n            # Each decision point adds 1\n            if isinstance(node, (ast.If, ast.While, ast.For, ast.ExceptHandler)):\n                complexity += 1\n            elif isinstance(node, ast.BoolOp):\n                complexity += len(node.values) - 1\n\n        return complexity\n\n    def detect_nonlinear_patterns(self, ast_node: ast.Module) -> List[str]:\n        \"\"\"Detect non-linear logic patterns\"\"\"\n        patterns = []\n\n        for node in ast.walk(ast_node):\n            # np.where / pd.where (conditional logic)\n            if isinstance(node, ast.Call):\n                if hasattr(node.func, 'attr') and node.func.attr in ['where', 'select']:\n                    patterns.append('conditional_where')\n\n            # Custom function calls\n            if isinstance(node, ast.Call):\n                if isinstance(node.func, ast.Name):\n                    # Not a built-in or numpy/pandas function\n                    if node.func.id not in ['min', 'max', 'abs', 'sum']:\n                        patterns.append('custom_function')\n\n            # Nested conditions\n            if isinstance(node, ast.If):\n                for child in ast.walk(node):\n                    if isinstance(child, ast.If) and child != node:\n                        patterns.append('nested_condition')\n                        break\n\n            # Lambda functions (higher-order)\n            if isinstance(node, ast.Lambda):\n                patterns.append('lambda_function')\n\n        return list(set(patterns))\n\n    def score_logic_deviation(self, ast_node: ast.Module, template_baseline: int = 1) -> float:\n        \"\"\"Score deviation from linear template baseline\"\"\"\n        complexity = self.measure_cyclomatic_complexity(ast_node)\n        patterns = self.detect_nonlinear_patterns(ast_node)\n\n        # Complexity score (normalized)\n        complexity_score = min((complexity - template_baseline) / 10.0, 0.5)\n\n        # Pattern score\n        pattern_score = min(len(patterns) * 0.2, 0.5)\n\n        return np.clip(complexity_score + pattern_score, 0.0, 1.0)\n\n    def score(self, strategy_code: str) -> float:\n        \"\"\"Calculate Layer 3 novelty score\"\"\"\n        try:\n            ast_tree = self.parse_strategy_ast(strategy_code)\n            return self.score_logic_deviation(ast_tree)\n        except Exception as e:\n            # Fallback for unparseable code\n            return 0.0\n```\n\n#### 2.3.4 Novelty Scorer (Aggregator)\n```python\n# src/analysis/novelty/novelty_scorer.py\n\nfrom dataclasses import dataclass\nimport numpy as np\n\n@dataclass\nclass NoveltyScores:\n    layer1_factor_diversity: float\n    layer2_combination_patterns: float\n    layer3_logic_complexity: float\n    total_score: float\n\n    def to_dict(self) -> dict:\n        return {\n            'layer1': self.layer1_factor_diversity,\n            'layer2': self.layer2_combination_patterns,\n            'layer3': self.layer3_logic_complexity,\n            'total': self.total_score\n        }\n\nclass NoveltyScorer:\n    \"\"\"Aggregates 3-layer novelty scores\"\"\"\n\n    def __init__(self, config: ExperimentConfig):\n        self.weights = config.novelty_weights\n        self.layer1 = FactorDiversityAnalyzer(\"artifacts/factor_graph/templates.json\")\n        self.layer2 = CombinationPatternDetector(\"artifacts/factor_graph/templates.json\")\n        self.layer3 = LogicComplexityAnalyzer()\n\n    def score(self, strategy_code: str) -> NoveltyScores:\n        \"\"\"Calculate comprehensive novelty score\"\"\"\n        # Individual layer scores\n        layer1_score = self.layer1.score(strategy_code)\n        layer2_score = self.layer2.score(strategy_code)\n        layer3_score = self.layer3.score(strategy_code)\n\n        # Weighted aggregation\n        total_score = (\n            layer1_score * self.weights['layer1_factor_diversity'] +\n            layer2_score * self.weights['layer2_combination_patterns'] +\n            layer3_score * self.weights['layer3_logic_complexity']\n        )\n\n        return NoveltyScores(\n            layer1_factor_diversity=layer1_score,\n            layer2_combination_patterns=layer2_score,\n            layer3_logic_complexity=layer3_score,\n            total_score=total_score\n        )\n\n    def validate_independence(self, scores_list: List[NoveltyScores]) -> dict:\n        \"\"\"Validate that layers measure different aspects (correlation < 0.7)\"\"\"\n        layer1_scores = [s.layer1_factor_diversity for s in scores_list]\n        layer2_scores = [s.layer2_combination_patterns for s in scores_list]\n        layer3_scores = [s.layer3_logic_complexity for s in scores_list]\n\n        correlations = {\n            'layer1_layer2': np.corrcoef(layer1_scores, layer2_scores)[0, 1],\n            'layer1_layer3': np.corrcoef(layer1_scores, layer3_scores)[0, 1],\n            'layer2_layer3': np.corrcoef(layer2_scores, layer3_scores)[0, 1]\n        }\n\n        max_corr = max(abs(c) for c in correlations.values())\n\n        return {\n            'correlations': correlations,\n            'max_correlation': max_corr,\n            'independent': max_corr < 0.7\n        }\n```\n\n### 2.4 Statistical Analysis Pipeline\n\n#### 2.4.1 Statistical Tests Module\n```python\n# src/analysis/statistical_tests.py\n\nfrom dataclasses import dataclass\nfrom typing import List\nimport numpy as np\nfrom scipy import stats\nimport pymannkendall as mk\n\n@dataclass\nclass TestResult:\n    statistic: float\n    p_value: float\n    effect_size: float\n    significant: bool\n    interpretation: str\n\ndef mann_whitney_u_test(\n    group_a: List[float],\n    group_b: List[float],\n    alternative: str = 'two-sided',\n    alpha: float = 0.05\n) -> TestResult:\n    \"\"\"Mann-Whitney U test for comparing distributions\"\"\"\n    statistic, p_value = stats.mannwhitneyu(\n        group_a, group_b,\n        alternative=alternative\n    )\n\n    # Calculate effect size (rank-biserial correlation)\n    n1, n2 = len(group_a), len(group_b)\n    effect_size = 1 - (2*statistic) / (n1 * n2)\n\n    significant = p_value < alpha\n\n    interpretation = (\n        f\"Group A (n={n1}) vs Group B (n={n2}): \"\n        f\"U={statistic:.2f}, p={p_value:.4f}, r={effect_size:.3f}\"\n    )\n\n    return TestResult(statistic, p_value, effect_size, significant, interpretation)\n\ndef mann_kendall_trend_test(\n    time_series: List[float],\n    alpha: float = 0.05\n) -> TestResult:\n    \"\"\"Mann-Kendall trend test for time series\"\"\"\n    result = mk.original_test(time_series, alpha=alpha)\n\n    interpretation = (\n        f\"Trend: {result.trend}, \"\n        f\"tau={result.tau:.3f}, p={result.p:.4f}\"\n    )\n\n    return TestResult(\n        statistic=result.tau,\n        p_value=result.p,\n        effect_size=abs(result.tau),\n        significant=result.p < alpha,\n        interpretation=interpretation\n    )\n\ndef sliding_window_analysis(\n    data: List[float],\n    window: int = 20\n) -> List[dict]:\n    \"\"\"Sliding window statistical analysis\"\"\"\n    results = []\n\n    for i in range(len(data) - window + 1):\n        window_data = data[i:i+window]\n        results.append({\n            'window_start': i,\n            'window_end': i + window,\n            'mean': np.mean(window_data),\n            'std': np.std(window_data),\n            'min': np.min(window_data),\n            'max': np.max(window_data),\n            'median': np.median(window_data)\n        })\n\n    return results\n```\n\n## 3. Data Model\n\n### 3.1 Extended IterationRecord\n```python\n# src/learning/iteration_history.py (extended)\n\nfrom dataclasses import dataclass, asdict\nfrom typing import Dict, Optional\nimport json\n\n@dataclass\nclass IterationRecord:\n    iteration_num: int\n    strategy_code: str\n    metrics: Dict[str, float]\n\n    # NEW FIELDS\n    novelty_scores: Dict[str, float]  # {'layer1': 0.5, 'layer2': 0.7, ...}\n    experiment_group: str  # 'hybrid', 'fg_only', 'llm_only'\n    run_id: Optional[int] = None\n\n    # EXISTING FIELDS (preserved)\n    classification_level: Optional[str] = None\n    champion_updated: bool = False\n    timestamp: Optional[str] = None\n\n    def to_json(self) -> str:\n        \"\"\"Serialize to JSON\"\"\"\n        return json.dumps(asdict(self), indent=2)\n\n    @classmethod\n    def from_json(cls, json_str: str) -> 'IterationRecord':\n        \"\"\"Deserialize from JSON\"\"\"\n        data = json.loads(json_str)\n        return cls(**data)\n```\n\n## 4. Visualization Design\n\n### 4.1 Learning Curves\n```python\n# src/analysis/visualization.py\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef plot_learning_curves(results: Dict[str, List[IterationRecord]]) -> plt.Figure:\n    \"\"\"Plot Sharpe ratio learning curves for all groups\"\"\"\n    fig, ax = plt.subplots(figsize=(12, 6))\n\n    for group_id, records in results.items():\n        sharpe_ratios = [r.metrics['sharpe_ratio'] for r in records]\n        iterations = range(len(sharpe_ratios))\n\n        ax.plot(iterations, sharpe_ratios, label=group_id, linewidth=2)\n\n    ax.set_xlabel('Iteration', fontsize=12)\n    ax.set_ylabel('Sharpe Ratio', fontsize=12)\n    ax.set_title('Learning Curves: Sharpe Ratio Over Iterations', fontsize=14)\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n\n    return fig\n\ndef plot_novelty_comparison(results: Dict[str, List[IterationRecord]]) -> plt.Figure:\n    \"\"\"Box plot comparison of novelty scores across groups\"\"\"\n    fig, ax = plt.subplots(figsize=(10, 6))\n\n    data = []\n    labels = []\n\n    for group_id, records in results.items():\n        novelty_scores = [r.novelty_scores['total'] for r in records]\n        data.append(novelty_scores)\n        labels.append(group_id)\n\n    ax.boxplot(data, labels=labels)\n    ax.set_ylabel('Novelty Score', fontsize=12)\n    ax.set_title('Novelty Score Distribution by Group', fontsize=14)\n    ax.grid(True, alpha=0.3, axis='y')\n\n    return fig\n\ndef plot_sharpe_distributions(results: Dict[str, List[IterationRecord]]) -> plt.Figure:\n    \"\"\"KDE overlay of Sharpe distributions\"\"\"\n    fig, ax = plt.subplots(figsize=(12, 6))\n\n    for group_id, records in results.items():\n        sharpe_ratios = [r.metrics['sharpe_ratio'] for r in records]\n        sns.kdeplot(sharpe_ratios, label=group_id, ax=ax, linewidth=2)\n\n    ax.set_xlabel('Sharpe Ratio', fontsize=12)\n    ax.set_ylabel('Density', fontsize=12)\n    ax.set_title('Sharpe Ratio Distribution Comparison', fontsize=14)\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n\n    return fig\n```\n\n## 5. Error Handling & Validation\n\n### 5.1 Error Handling Strategy\n- **Iteration Failures**: Log error, continue execution, track failure rate\n- **Validation Errors**: Fail fast on configuration/schema errors\n- **Resource Errors**: Graceful degradation, clear error messages\n- **Statistical Errors**: Return NaN, log warning, continue analysis\n\n### 5.2 Validation Checkpoints\n1. **Pre-execution**: Config validation, dependency checks\n2. **Post-infrastructure**: Unit tests for all components\n3. **Post-novelty-system**: Validation against champion and templates\n4. **Post-dry-run**: End-to-end validation with 15 iterations\n5. **Post-pilot**: Go/no-go criteria evaluation\n\n## 6. Performance Considerations\n\n### 6.1 Optimization Strategies\n- **Sequential Execution**: Avoid resource contention on single machine\n- **Lazy Loading**: Load templates once, reuse across iterations\n- **Caching**: Cache AST parsing results for identical code\n- **Batch Processing**: Aggregate statistical tests for efficiency\n\n### 6.2 Resource Management\n- **Memory**: Stream large result sets, don't load all in memory\n- **Storage**: Compress JSON outputs, cleanup intermediate files\n- **Logging**: Rotate logs, limit verbosity in production\n\n## 7. Security & Data Integrity\n\n### 7.1 Data Protection\n- Atomic file writes to prevent corruption\n- JSON schema validation before saving\n- Backup before Pilot and Full Study execution\n\n### 7.2 Code Execution Safety\n- AST parsing (no eval/exec)\n- Sandboxed strategy execution in backtesting engine\n- Input validation on all external data\n\n## 8. Testing Strategy\n\n### 8.1 Unit Tests\n- All novelty layer analyzers (>80% coverage)\n- Statistical test implementations\n- Configuration loading and validation\n- Data serialization/deserialization\n\n### 8.2 Integration Tests\n- End-to-end novelty scoring\n- Orchestrator with mock learning system\n- Statistical pipeline with synthetic data\n\n### 8.3 Validation Tests\n- Champion strategy baseline (novelty > 0.3)\n- Template strategy baseline (novelty < 0.15)\n- Layer independence (correlation < 0.7)\n- Statistical test accuracy vs reference implementations\n\n## 9. Deployment Plan\n\n### 9.1 Pre-Deployment Checklist\n- [ ] All unit tests passing\n- [ ] Integration tests passing\n- [ ] Validation tests passing\n- [ ] Dry run successful (15 iterations)\n- [ ] Configuration backed up\n- [ ] Output directories created\n- [ ] Dependencies installed\n\n### 9.2 Execution Sequence\n1. **Day 1-3**: Development and testing\n2. **Day 4**: Dry run and final validation\n3. **Day 5**: Pilot execution\n4. **Day 5 evening**: Go/no-go decision\n5. **Day 6-7**: Optional Full Study\n\n### 9.3 Rollback Plan\n- Restore learning_system.yaml from backup\n- Delete experimental output directories\n- Revert any code changes if needed\n- Document lessons learned\n",
  "fileStats": {
    "size": 31397,
    "lines": 898,
    "lastModified": "2025-11-06T13:23:02.204Z"
  },
  "comments": []
}