{
  "id": "snapshot_1761952951483_1foiylfub",
  "approvalId": "approval_1761952274221_uz2ngww95",
  "approvalTitle": "Design: Validation Framework Critical Fixes",
  "version": 2,
  "timestamp": "2025-10-31T23:22:31.483Z",
  "trigger": "approved",
  "status": "pending",
  "content": "# Validation Framework Critical Fixes - Design Document\n\n**Created**: 2025-11-01\n**Priority**: P0 CRITICAL - BLOCKING Phase 3\n**Version**: 1.0\n\n---\n\n## Overview\n\nThis design addresses critical validation framework bugs discovered through `mcp__zen__challenge` reassessment. The validation framework currently uses incorrect Bonferroni threshold logic (0.8 instead of 0.5), has duplicate strategies contaminating training data, and lacks diversity analysis necessary for Phase 3 GO/NO-GO decision.\n\n**Design Philosophy**:\n- **Minimal Disruption**: Fix bugs without redesigning working systems\n- **Modular Components**: Each fix is independent, testable, and reversible\n- **Data Integrity**: Preserve original results for comparison\n- **Clear Separation**: Statistical validation vs dynamic threshold vs diversity analysis\n\n---\n\n## Steering Document Alignment\n\n### Technical Standards (tech.md)\n\nThis design follows established technical patterns:\n- **Statistical Rigor**: Uses proven methods (Bonferroni correction, AST analysis, correlation matrices)\n- **Error Handling**: Graceful degradation if diversity analysis fails\n- **Logging**: Detailed diagnostic logs for debugging\n- **Testing**: Unit tests for each component before integration\n\n### Project Structure (structure.md)\n\nImplementation follows project organization:\n- `src/validation/` - Core validation logic fixes\n- `src/analysis/` - NEW: Duplicate detection and diversity analysis\n- `scripts/` - Standalone analysis and re-validation scripts\n- `docs/` - Updated documentation and decision reports\n\n---\n\n## Code Reuse Analysis\n\n### Existing Components to Leverage\n\n**1. Validation Framework** (`src/validation/integration.py`)\n- **BonferroniIntegrator** (lines 776-1043): Core threshold calculation\n  - Reuse: `calculate_significance_threshold()` method\n  - Fix: Separate bonferroni_threshold from dynamic_threshold\n  - Status: Minor modification needed\n\n- **BootstrapIntegrator** (lines 457-774): Bootstrap confidence intervals\n  - Reuse: Already working correctly\n  - No changes needed\n\n**2. Backtest Executor** (`src/backtest/executor.py`)\n- **ExecutionResult** dataclass: Strategy execution results\n  - Reuse: Sharpe ratio, return data, report objects\n  - Status: No changes needed\n\n**3. Multiple Comparison Module** (`src/validation/multiple_comparison.py`)\n- **BonferroniValidator**: Z-score threshold calculation\n  - Reuse: `calculate_significance_threshold()` method\n  - Formula: `z_score / sqrt(n_periods)` with conservative floor\n  - Status: Working correctly, no changes needed\n\n**4. Dynamic Threshold Module** (`src/validation/dynamic_threshold.py`)\n- **DynamicThresholdCalculator**: Taiwan market benchmark\n  - Reuse: `get_threshold()` method (returns 0.8)\n  - Status: Working correctly, no changes needed\n\n### Integration Points\n\n**1. run_phase2_with_validation.py** (main validation script)\n- **Lines 370-520**: Validation execution loop\n  - Bug Location: Line 398 - `bonferroni_threshold = validation.get('significance_threshold', 0.5)`\n  - Current: Gets final_threshold (0.8) instead of statistical_threshold (0.5)\n  - Fix: Separate Bonferroni and dynamic threshold checks\n\n**2. Phase 2 Validation Results** (JSON output)\n- File: `phase2_validated_results_20251101_060315.json`\n- Structure: Summary stats + per-strategy validation details\n- Reuse: Existing format, add new fields without removing old ones\n- Backward compatibility: Critical for comparison\n\n**3. Generated Strategy Files**\n- Location: `generated_strategy_loop_iter*.py` (20 files)\n- Access: AST parsing for duplicate detection\n- Diversity: Factor extraction from data.get() calls\n- Status: Read-only access, no modifications\n\n---\n\n## Architecture\n\n### High-Level Design\n\n```mermaid\ngraph TD\n    A[run_phase2_with_validation.py] --> B[1. Fix Threshold Logic]\n    A --> C[2. Duplicate Detector]\n    A --> D[3. Diversity Analyzer]\n    A --> E[4. Re-validation Engine]\n\n    B --> F[Separate Bonferroni + Dynamic]\n    C --> G[AST + Sharpe Matching]\n    D --> H[Factor + Correlation + Risk]\n    E --> I[Execute with Fixed Logic]\n\n    F --> J[Decision Framework]\n    G --> J\n    H --> J\n    I --> J\n\n    J --> K{GO/NO-GO?}\n    K --> L[GO: Phase 3]\n    K --> M[CONDITIONAL GO: +Mitigation]\n    K --> N[NO-GO: Remediation Plan]\n```\n\n### Modular Design Principles\n\n1. **Single File Responsibility**\n   - `threshold_fix.py`: Threshold calculation logic only\n   - `duplicate_detector.py`: AST-based similarity detection only\n   - `diversity_analyzer.py`: Factor/correlation/risk analysis only\n   - `decision_framework.py`: GO criteria evaluation only\n\n2. **Component Isolation**\n   - Each component can run independently\n   - Duplicate detector doesn't need validation results\n   - Diversity analyzer doesn't need duplicate detection\n   - Graceful degradation if any component fails\n\n3. **Service Layer Separation**\n   - Data access: Read strategy files and results JSON\n   - Business logic: Duplicate detection, diversity calculation\n   - Presentation: Markdown reports, comparison tables\n\n4. **Utility Modularity**\n   - AST parsing utilities (variable name normalization)\n   - Statistical utilities (Jaccard similarity, correlation matrices)\n   - Reporting utilities (Markdown generation, visualization)\n\n---\n\n## Components and Interfaces\n\n### Component 1: Threshold Logic Fix\n\n**File**: `run_phase2_with_validation.py` (lines 370-450)\n\n**Purpose**: Separate Bonferroni statistical threshold from Taiwan market dynamic threshold\n\n**Current Bug**:\n```python\n# Line 398: WRONG - gets final_threshold (0.8)\nbonferroni_threshold = validation.get('significance_threshold', 0.5)\n\n# Line 403: Uses wrong threshold\nstatistically_significant = result.sharpe_ratio > bonferroni_threshold  # Compares to 0.8!\n```\n\n**Fixed Logic**:\n```python\n# Get both thresholds separately\nvalidation_response = self.bonferroni.validate_single_strategy(\n    sharpe_ratio=result.sharpe_ratio,\n    n_periods=252\n)\n\n# Extract separate thresholds (new keys from BonferroniIntegrator)\nbonferroni_threshold = validation_response.get('statistical_threshold', 0.5)  # 0.5\ndynamic_threshold = validation_response.get('dynamic_threshold', 0.8)         # 0.8\n\n# Test each threshold independently\nstatistically_significant = result.sharpe_ratio > bonferroni_threshold  # Sharpe > 0.5\nbeats_dynamic = result.sharpe_ratio >= dynamic_threshold                # Sharpe >= 0.8\n\n# Validation passes if BOTH criteria met\nvalidation_passed = statistically_significant and beats_dynamic\n```\n\n**Interfaces (Modified)**:\n- **Input**: `ExecutionResult` (from BacktestExecutor)\n- **Output**: `Dict[str, Any]` with keys:\n  - `validation_passed: bool` - Overall pass/fail\n  - `statistically_significant: bool` - Bonferroni test result\n  - `beats_dynamic_threshold: bool` - Dynamic test result\n  - `bonferroni_threshold: float` - Statistical threshold (0.5)\n  - `dynamic_threshold: float` - Market benchmark (0.8)\n  - `bonferroni_alpha: float` - Adjusted alpha (0.0025 for N=20)\n\n**Dependencies**:\n- `BonferroniIntegrator` (src/validation/integration.py)\n- `DynamicThresholdCalculator` (src/validation/dynamic_threshold.py)\n\n**Reuses**:\n- Existing BonferroniValidator threshold calculation\n- Existing DynamicThresholdCalculator.get_threshold()\n\n**Changes to BonferroniIntegrator** (`src/validation/integration.py`):\n```python\n# Line 863-873: Modify to return BOTH thresholds\ndef validate_single_strategy(self, sharpe_ratio, n_periods=252, use_conservative=True):\n    # Calculate statistical threshold (Bonferroni-corrected)\n    statistical_threshold = self.validator.calculate_significance_threshold(\n        n_periods=n_periods,\n        use_conservative=use_conservative\n    )\n\n    # Get dynamic threshold (Taiwan market benchmark)\n    if use_conservative and self.threshold_calc:\n        dynamic_threshold = self.threshold_calc.get_threshold()\n    else:\n        dynamic_threshold = 0.5\n\n    # Return BOTH thresholds separately (DON'T use max())\n    return {\n        'statistical_threshold': statistical_threshold,  # NEW: 0.5\n        'dynamic_threshold': dynamic_threshold,          # NEW: 0.8\n        'significance_threshold': statistical_threshold, # DEPRECATED: backward compat\n        # ... other fields\n    }\n```\n\n---\n\n### Component 2: Duplicate Strategy Detector\n\n**File**: `src/analysis/duplicate_detector.py` (NEW)\n\n**Purpose**: Identify strategies with identical or near-identical code using AST-based comparison\n\n**Interfaces**:\n\n```python\nclass DuplicateDetector:\n    \"\"\"Detect duplicate strategies using AST analysis.\"\"\"\n\n    def __init__(self, sharpe_tolerance: float = 1e-8):\n        \"\"\"\n        Args:\n            sharpe_tolerance: Maximum difference for Sharpe ratios to be considered identical\n        \"\"\"\n        self.sharpe_tolerance = sharpe_tolerance\n\n    def find_duplicates(\n        self,\n        strategy_files: List[str],\n        validation_results: Dict[str, Any]\n    ) -> List[DuplicateGroup]:\n        \"\"\"\n        Find all duplicate strategy groups.\n\n        Args:\n            strategy_files: List of strategy file paths\n            validation_results: Validation results JSON with Sharpe ratios\n\n        Returns:\n            List of DuplicateGroup objects containing:\n            - strategies: List of strategy indices in group\n            - sharpe_ratio: Common Sharpe ratio\n            - similarity_score: Code similarity (0-1)\n            - differences: List of code differences\n            - recommendation: 'keep' or 'remove' for each\n        \"\"\"\n        pass\n\n    def compare_strategies(\n        self,\n        strategy_a_path: str,\n        strategy_b_path: str\n    ) -> Tuple[float, List[str]]:\n        \"\"\"\n        Compare two strategies using AST analysis.\n\n        Args:\n            strategy_a_path: Path to first strategy\n            strategy_b_path: Path to second strategy\n\n        Returns:\n            Tuple of (similarity_score, differences_list)\n            - similarity_score: 0-1 (1 = identical after normalization)\n            - differences_list: Human-readable diff of changes\n        \"\"\"\n        pass\n\n    def normalize_ast(self, tree: ast.AST) -> ast.AST:\n        \"\"\"\n        Normalize AST by removing comments, standardizing variable names.\n\n        Args:\n            tree: Python AST\n\n        Returns:\n            Normalized AST tree\n        \"\"\"\n        pass\n```\n\n**Algorithm**:\n\n1. **Phase 1: Sharpe Ratio Matching**\n   ```python\n   # Group strategies by Sharpe ratio (tolerance 1e-8)\n   sharpe_groups = {}\n   for idx, validation in enumerate(validation_results['strategies_validation']):\n       sharpe = validation['sharpe_ratio']\n       if sharpe is None:\n           continue\n\n       # Find existing group with matching Sharpe\n       for group_sharpe, group_indices in sharpe_groups.items():\n           if abs(sharpe - group_sharpe) < sharpe_tolerance:\n               group_indices.append(idx)\n               break\n       else:\n           sharpe_groups[sharpe] = [idx]\n\n   # Filter to groups with 2+ strategies\n   duplicate_candidates = {s: indices for s, indices in sharpe_groups.items() if len(indices) >= 2}\n   ```\n\n2. **Phase 2: AST Comparison**\n   ```python\n   for sharpe, indices in duplicate_candidates.items():\n       for i, idx_a in enumerate(indices):\n           for idx_b in indices[i+1:]:\n               # Load and parse strategy files\n               tree_a = ast.parse(open(f'generated_strategy_loop_iter{idx_a}.py').read())\n               tree_b = ast.parse(open(f'generated_strategy_loop_iter{idx_b}.py').read())\n\n               # Normalize ASTs (variable name standardization)\n               norm_a = normalize_ast(tree_a)\n               norm_b = normalize_ast(tree_b)\n\n               # Compare normalized ASTs\n               similarity = ast_similarity(norm_a, norm_b)\n\n               if similarity > 0.95:\n                   # Flag as duplicate\n                   differences = generate_diff(tree_a, tree_b)\n                   duplicates.append(DuplicateGroup(\n                       strategies=[idx_a, idx_b],\n                       sharpe_ratio=sharpe,\n                       similarity_score=similarity,\n                       differences=differences\n                   ))\n   ```\n\n3. **Phase 3: Recommendation Generation**\n   ```python\n   for group in duplicates:\n       # Keep lower index (assumed to be generated first)\n       group.recommendations = {\n           group.strategies[0]: 'KEEP',\n           group.strategies[1:]: 'REMOVE'\n       }\n   ```\n\n**Dependencies**:\n- Python `ast` module (standard library)\n- `difflib` for diff generation\n- `json` for reading validation results\n\n**Reuses**:\n- Validation results JSON (Sharpe ratios)\n- Existing strategy files (read-only)\n\n**Output Example**:\n```json\n{\n  \"duplicate_groups\": [\n    {\n      \"group_id\": 1,\n      \"strategies\": [9, 13],\n      \"sharpe_ratio\": 0.9443348034803672,\n      \"similarity_score\": 0.99,\n      \"differences\": [\n        \"Line 24: 'value_factor' renamed to 'value'\",\n        \"Line 35: 'value_factor' renamed to 'value'\"\n      ],\n      \"recommendations\": {\n        \"9\": \"KEEP\",\n        \"13\": \"REMOVE\"\n      }\n    }\n  ],\n  \"total_duplicates\": 1,\n  \"unique_strategies\": 19\n}\n```\n\n---\n\n### Component 3: Diversity Analyzer\n\n**File**: `src/analysis/diversity_analyzer.py` (NEW)\n\n**Purpose**: Analyze validated strategies for factor diversity, return correlation, and risk profile spread\n\n**Interfaces**:\n\n```python\nclass DiversityAnalyzer:\n    \"\"\"Analyze strategy diversity for learning system readiness.\"\"\"\n\n    def __init__(self, min_strategies: int = 3):\n        \"\"\"\n        Args:\n            min_strategies: Minimum strategies required for analysis\n        \"\"\"\n        self.min_strategies = min_strategies\n\n    def analyze_diversity(\n        self,\n        strategy_files: List[str],\n        validation_results: Dict[str, Any],\n        exclude_indices: Optional[List[int]] = None\n    ) -> DiversityReport:\n        \"\"\"\n        Comprehensive diversity analysis.\n\n        Args:\n            strategy_files: List of validated strategy file paths\n            validation_results: Validation results JSON\n            exclude_indices: Strategy indices to exclude (duplicates)\n\n        Returns:\n            DiversityReport with:\n            - factor_diversity_score: 0-100\n            - correlation_matrix: NxN numpy array\n            - average_correlation: float\n            - risk_profile_diversity: 0-100\n            - diversity_score: 0-100 (overall)\n            - recommendation: SUFFICIENT/MARGINAL/INSUFFICIENT\n        \"\"\"\n        pass\n\n    def extract_factors(self, strategy_path: str) -> Set[str]:\n        \"\"\"\n        Extract FinLab factors used by strategy.\n\n        Args:\n            strategy_path: Path to strategy file\n\n        Returns:\n            Set of factor names (e.g., {'price:æ”¶ç›¤åƒ¹', 'RSI', 'ROEç¨…å¾Œ'})\n        \"\"\"\n        pass\n\n    def calculate_factor_diversity(\n        self,\n        factor_sets: List[Set[str]]\n    ) -> Tuple[float, np.ndarray]:\n        \"\"\"\n        Calculate Jaccard similarity matrix and diversity score.\n\n        Args:\n            factor_sets: List of factor sets (one per strategy)\n\n        Returns:\n            Tuple of (diversity_score, similarity_matrix)\n            - diversity_score: 0-100 (100 = completely different factors)\n            - similarity_matrix: NxN Jaccard similarity matrix\n        \"\"\"\n        pass\n\n    def calculate_return_correlation(\n        self,\n        strategy_returns: List[np.ndarray]\n    ) -> Tuple[float, np.ndarray]:\n        \"\"\"\n        Calculate pairwise return correlation matrix.\n\n        Args:\n            strategy_returns: List of return arrays\n\n        Returns:\n            Tuple of (average_correlation, correlation_matrix)\n        \"\"\"\n        pass\n\n    def calculate_risk_diversity(\n        self,\n        validation_results: Dict[str, Any],\n        strategy_indices: List[int]\n    ) -> float:\n        \"\"\"\n        Calculate risk profile diversity using max drawdown CV.\n\n        Args:\n            validation_results: Validation results JSON\n            strategy_indices: Indices of strategies to analyze\n\n        Returns:\n            Risk diversity score: 0-100 (higher = more diverse risk profiles)\n        \"\"\"\n        pass\n```\n\n**Algorithm**:\n\n1. **Factor Diversity Analysis**\n   ```python\n   # Extract factors from each strategy\n   all_factors = []\n   for strategy_path in validated_strategies:\n       factors = extract_factors_from_ast(strategy_path)\n       all_factors.append(factors)\n\n   # Calculate Jaccard similarity matrix\n   n = len(all_factors)\n   similarity_matrix = np.zeros((n, n))\n   for i in range(n):\n       for j in range(n):\n           if i == j:\n               similarity_matrix[i][j] = 1.0\n           else:\n               intersection = len(all_factors[i] & all_factors[j])\n               union = len(all_factors[i] | all_factors[j])\n               similarity_matrix[i][j] = intersection / union if union > 0 else 0\n\n   # Diversity score = 1 - average off-diagonal similarity\n   avg_similarity = np.mean(similarity_matrix[np.triu_indices(n, k=1)])\n   factor_diversity = (1 - avg_similarity) * 100\n   ```\n\n2. **Return Correlation Analysis**\n   ```python\n   # Extract returns from backtest reports (requires re-execution)\n   # Alternatively, use Sharpe ratios as proxy for performance similarity\n   correlation_matrix = np.corrcoef(strategy_sharpes)\n   avg_correlation = np.mean(correlation_matrix[np.triu_indices(n, k=1)])\n   ```\n\n3. **Risk Profile Diversity**\n   ```python\n   # Extract max drawdowns from validation results\n   drawdowns = [result['max_drawdown'] for result in validated_strategies]\n\n   # Coefficient of variation (CV) as diversity metric\n   mean_dd = np.mean(drawdowns)\n   std_dd = np.std(drawdowns)\n   risk_diversity = (std_dd / abs(mean_dd)) * 100 if mean_dd != 0 else 0\n   ```\n\n4. **Overall Diversity Score**\n   ```python\n   diversity_score = (\n       factor_diversity * 0.5 +\n       (1 - avg_correlation) * 100 * 0.3 +\n       risk_diversity * 0.2\n   )\n   ```\n\n**Dependencies**:\n- `numpy` for correlation matrices\n- `pandas` for data manipulation\n- `ast` for factor extraction\n- `matplotlib`/`seaborn` for visualization (optional)\n\n**Reuses**:\n- Validation results JSON (Sharpe, drawdowns)\n- Strategy files (AST parsing)\n\n**Output Example**:\n```json\n{\n  \"factor_diversity_score\": 65.4,\n  \"average_correlation\": 0.42,\n  \"risk_profile_diversity\": 58.3,\n  \"diversity_score\": 68.2,\n  \"recommendation\": \"MARGINAL\",\n  \"details\": {\n    \"unique_factors\": 18,\n    \"total_factor_usages\": 24,\n    \"factor_jaccard_matrix\": [[1.0, 0.4, 0.3], [0.4, 1.0, 0.5], [0.3, 0.5, 1.0]],\n    \"correlation_matrix\": [[1.0, 0.35, 0.48], [0.35, 1.0, 0.52], [0.48, 0.52, 1.0]],\n    \"max_drawdowns\": [-0.28, -0.35, -0.31],\n    \"warnings\": [\"Average correlation > 0.4 indicates moderate similarity\"]\n  }\n}\n```\n\n---\n\n### Component 4: Re-validation Engine\n\n**File**: `scripts/run_revalidation_with_fixes.py` (NEW)\n\n**Purpose**: Re-execute all 20 strategies with corrected threshold logic\n\n**Interfaces**:\n\n```python\nclass RevalidationEngine:\n    \"\"\"Re-run validation with fixed threshold logic.\"\"\"\n\n    def __init__(\n        self,\n        original_results_path: str,\n        output_path: str,\n        log_path: str\n    ):\n        \"\"\"\n        Args:\n            original_results_path: Path to original results JSON (for comparison)\n            output_path: Path to save new results JSON\n            log_path: Path to save execution log\n        \"\"\"\n        self.original_results = json.load(open(original_results_path))\n        self.output_path = output_path\n        self.log_path = log_path\n\n    def execute_revalidation(self) -> Dict[str, Any]:\n        \"\"\"\n        Re-execute all 20 strategies with fixed logic.\n\n        Returns:\n            Validation results dictionary (same format as original)\n        \"\"\"\n        pass\n\n    def generate_comparison_report(\n        self,\n        new_results: Dict[str, Any]\n    ) -> str:\n        \"\"\"\n        Generate before/after comparison report.\n\n        Args:\n            new_results: New validation results\n\n        Returns:\n            Markdown-formatted comparison report\n        \"\"\"\n        pass\n```\n\n**Execution Flow**:\n\n1. Load original results for comparison\n2. Initialize fixed Phase2WithValidation executor\n3. Execute all 20 strategies (same parameters as original)\n4. Compare results:\n   - Threshold values (before: 0.8, after: 0.5 + 0.8)\n   - Statistical significance counts (before: 4, after: ~18)\n   - Validation pass counts (before: 4, after: 3 unique)\n5. Generate comparison report\n\n**Dependencies**:\n- `run_phase2_with_validation.py` (modified version)\n- Original results JSON\n- Strategy files (iter0-iter19)\n\n**Reuses**:\n- Existing Phase2WithValidation class\n- Fixed threshold logic from Component 1\n\n---\n\n### Component 5: Decision Framework\n\n**File**: `src/analysis/decision_framework.py` (NEW)\n\n**Purpose**: Evaluate GO/CONDITIONAL GO/NO-GO criteria for Phase 3\n\n**Interfaces**:\n\n```python\nclass DecisionFramework:\n    \"\"\"Evaluate Phase 3 GO/NO-GO criteria.\"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize decision framework with criteria thresholds.\"\"\"\n        self.go_criteria = {\n            'min_unique_strategies': 3,\n            'min_diversity_score': 60,\n            'max_avg_correlation': 0.8,\n            'validation_framework_fixed': True,\n            'execution_success_rate': 1.0\n        }\n        self.conditional_criteria = {\n            'min_unique_strategies': 3,\n            'min_diversity_score': 40,\n            'max_avg_correlation': 0.8\n        }\n\n    def evaluate(\n        self,\n        validation_results: Dict[str, Any],\n        duplicate_report: Dict[str, Any],\n        diversity_report: Dict[str, Any]\n    ) -> DecisionReport:\n        \"\"\"\n        Evaluate all GO criteria.\n\n        Args:\n            validation_results: Fixed validation results\n            duplicate_report: Duplicate detection results\n            diversity_report: Diversity analysis results\n\n        Returns:\n            DecisionReport with:\n            - decision: 'GO' | 'CONDITIONAL_GO' | 'NO_GO'\n            - rationale: Explanation\n            - criteria_met: Dict of criterion -> bool\n            - risk_assessment: 'LOW' | 'MEDIUM' | 'HIGH'\n            - next_steps: List of action items\n        \"\"\"\n        pass\n\n    def check_go_criteria(self, ...) -> Tuple[bool, List[str]]:\n        \"\"\"Check if all GO criteria are met.\"\"\"\n        pass\n\n    def check_conditional_criteria(self, ...) -> Tuple[bool, List[str]]:\n        \"\"\"Check if CONDITIONAL GO criteria are met.\"\"\"\n        pass\n\n    def assess_risk(self, decision: str, criteria_met: Dict) -> str:\n        \"\"\"Assess risk level based on decision and criteria.\"\"\"\n        pass\n\n    def generate_decision_document(\n        self,\n        decision: str,\n        rationale: str,\n        criteria_met: Dict[str, bool],\n        risk: str\n    ) -> str:\n        \"\"\"Generate decision document in Markdown format.\"\"\"\n        pass\n```\n\n**Decision Logic**:\n\n```python\ndef evaluate(self, validation_results, duplicate_report, diversity_report):\n    # Extract metrics\n    unique_strategies = duplicate_report['unique_strategies']\n    diversity_score = diversity_report['diversity_score']\n    avg_correlation = diversity_report['average_correlation']\n    validation_fixed = self.check_threshold_fix(validation_results)\n    execution_rate = validation_results['summary']['execution_success_rate']\n\n    # Check GO criteria\n    go_criteria_met = {\n        'unique_strategies': unique_strategies >= self.go_criteria['min_unique_strategies'],\n        'diversity_score': diversity_score >= self.go_criteria['min_diversity_score'],\n        'correlation': avg_correlation < self.go_criteria['max_avg_correlation'],\n        'validation_fixed': validation_fixed,\n        'execution_rate': execution_rate >= self.go_criteria['execution_success_rate']\n    }\n\n    if all(go_criteria_met.values()):\n        return DecisionReport(\n            decision='GO',\n            rationale='All GO criteria met',\n            criteria_met=go_criteria_met,\n            risk_assessment='LOW',\n            next_steps=['Proceed to Phase 3 Task 2.1']\n        )\n\n    # Check CONDITIONAL GO criteria\n    conditional_met = {\n        'unique_strategies': unique_strategies >= self.conditional_criteria['min_unique_strategies'],\n        'diversity_score': diversity_score >= self.conditional_criteria['min_diversity_score'],\n        'correlation': avg_correlation < self.conditional_criteria['max_avg_correlation']\n    }\n\n    if all(conditional_met.values()) and validation_fixed and execution_rate >= 1.0:\n        return DecisionReport(\n            decision='CONDITIONAL_GO',\n            rationale='Minimum criteria met with mitigation required',\n            criteria_met=conditional_met,\n            risk_assessment='MEDIUM',\n            next_steps=[\n                'Proceed to Phase 3 with aggressive monitoring',\n                'Parallel strategy generation to increase diversity',\n                'Weekly diversity checks during Phase 3'\n            ]\n        )\n\n    # NO-GO\n    blockers = [k for k, v in go_criteria_met.items() if not v]\n    return DecisionReport(\n        decision='NO_GO',\n        rationale=f'Blocking issues: {\", \".join(blockers)}',\n        criteria_met=go_criteria_met,\n        risk_assessment='HIGH',\n        next_steps=[\n            'Fix validation framework bugs',\n            'Generate additional strategies',\n            'Re-run diversity analysis'\n        ]\n    )\n```\n\n**Dependencies**:\n- Validation results (fixed)\n- Duplicate detection results\n- Diversity analysis results\n\n**Reuses**:\n- All previous component outputs\n\n**Output Example**:\n```markdown\n# Phase 3 GO/NO-GO Decision Report\n\n**Date**: 2025-11-01\n**Decision**: CONDITIONAL GO\n**Risk Assessment**: MEDIUM\n\n## Criteria Evaluation\n\n| Criterion | Status | Value | Threshold |\n|-----------|--------|-------|-----------|\n| Unique Strategies | âœ… PASS | 3 | >= 3 |\n| Diversity Score | âš ï¸ MARGINAL | 52 | >= 60 (GO), >= 40 (CONDITIONAL) |\n| Avg Correlation | âœ… PASS | 0.45 | < 0.8 |\n| Validation Fixed | âœ… PASS | Yes | Yes |\n| Execution Rate | âœ… PASS | 100% | >= 100% |\n\n## Rationale\n\nSystem meets CONDITIONAL GO criteria:\n- Minimum 3 unique validated strategies achieved\n- Diversity score (52) is marginal but acceptable with mitigation\n- Validation framework bugs fixed and verified\n- Execution framework stable\n\n## Mitigation Strategies\n\n1. **Parallel Strategy Generation**: Generate 5-10 additional strategies during early Phase 3\n2. **Aggressive Monitoring**: Weekly diversity checks and correlation analysis\n3. **Overfitting Detection**: Track learning system convergence closely\n\n## Next Steps\n\n1. âœ… User approval of CONDITIONAL GO decision\n2. ðŸ”œ Proceed to Phase 3 Task 2.1 with monitoring plan\n3. ðŸ”œ Set up diversity tracking dashboard\n4. ðŸ”œ Generate additional strategies in parallel\n\n## Risk Profile\n\n**Risk Level**: MEDIUM\n- Small training set (3 strategies) increases overfitting risk\n- Marginal diversity may limit generalization\n- Mitigation strategies reduce risk to acceptable level\n```\n\n---\n\n## Data Models\n\n### ValidationResult (Enhanced)\n\n```python\n@dataclass\nclass ValidationResult:\n    \"\"\"Enhanced validation result with separate thresholds.\"\"\"\n    strategy_index: int\n    validation_passed: bool\n    sharpe_ratio: float\n\n    # Statistical validation (Bonferroni)\n    statistically_significant: bool\n    bonferroni_threshold: float  # NEW: 0.5 (separate from dynamic)\n    bonferroni_alpha: float      # NEW: 0.0025 for N=20\n\n    # Dynamic validation (Taiwan market)\n    beats_dynamic_threshold: bool\n    dynamic_threshold: float  # 0.8\n\n    # Metadata\n    validation_reason: str\n    execution_time: float\n```\n\n### DuplicateGroup\n\n```python\n@dataclass\nclass DuplicateGroup:\n    \"\"\"Group of duplicate strategies.\"\"\"\n    group_id: int\n    strategies: List[int]  # Strategy indices\n    sharpe_ratio: float    # Common Sharpe ratio\n    similarity_score: float  # 0-1 (AST similarity)\n    differences: List[str]   # Human-readable diffs\n    recommendations: Dict[int, str]  # strategy_idx -> 'KEEP'|'REMOVE'\n```\n\n### DiversityReport\n\n```python\n@dataclass\nclass DiversityReport:\n    \"\"\"Comprehensive diversity analysis report.\"\"\"\n    # Overall scores\n    diversity_score: float  # 0-100 (overall)\n    recommendation: str     # 'SUFFICIENT' | 'MARGINAL' | 'INSUFFICIENT'\n\n    # Factor diversity\n    factor_diversity_score: float  # 0-100\n    unique_factors: int\n    total_factor_usages: int\n    factor_jaccard_matrix: np.ndarray\n\n    # Return correlation\n    average_correlation: float\n    correlation_matrix: np.ndarray\n    high_correlation_pairs: List[Tuple[int, int, float]]\n\n    # Risk profile\n    risk_profile_diversity: float  # 0-100\n    max_drawdowns: List[float]\n    drawdown_cv: float\n\n    # Warnings\n    warnings: List[str]\n```\n\n### DecisionReport\n\n```python\n@dataclass\nclass DecisionReport:\n    \"\"\"Phase 3 GO/NO-GO decision report.\"\"\"\n    decision: str  # 'GO' | 'CONDITIONAL_GO' | 'NO_GO'\n    rationale: str\n    criteria_met: Dict[str, bool]\n    risk_assessment: str  # 'LOW' | 'MEDIUM' | 'HIGH'\n    next_steps: List[str]\n\n    # Supporting data\n    unique_strategies: int\n    diversity_score: float\n    validation_stats: Dict[str, Any]\n    mitigation_strategies: Optional[List[str]] = None\n```\n\n---\n\n## Error Handling\n\n### Error Scenarios\n\n1. **Scenario: Duplicate Detection Fails**\n   - **Cause**: AST parsing error, file not found, malformed strategy code\n   - **Handling**:\n     - Log warning with file path and error\n     - Continue with remaining strategies\n     - Set duplicate_report.status = 'PARTIAL'\n   - **User Impact**: Warning in decision report, may overestimate unique strategies\n\n2. **Scenario: Diversity Analysis Fails**\n   - **Cause**: Insufficient strategies (<2), missing return data, calculation error\n   - **Handling**:\n     - Log error with details\n     - Set diversity_report.status = 'FAILED'\n     - Use conservative fallback: diversity_score = 0, recommendation = 'INSUFFICIENT'\n   - **User Impact**: NO-GO decision triggered (fail-safe)\n\n3. **Scenario: Re-validation Execution Errors**\n   - **Cause**: Strategy timeout, data unavailable, execution failure\n   - **Handling**:\n     - Retry once with increased timeout (600s)\n     - If retry fails, mark strategy as 'EXECUTION_FAILED'\n     - Continue with remaining strategies\n   - **User Impact**: Reduced sample size, logged in comparison report\n\n4. **Scenario: Threshold Calculation Returns NaN**\n   - **Cause**: Invalid n_periods, edge case in Z-score calculation\n   - **Handling**:\n     - Use fallback threshold: bonferroni_threshold = 0.5\n     - Log warning with input parameters\n     - Set validation_result.threshold_fallback = True\n   - **User Impact**: Conservative validation (fail-safe)\n\n5. **Scenario: Decision Framework Missing Required Data**\n   - **Cause**: Validation/duplicate/diversity reports incomplete\n   - **Handling**:\n     - Check all required fields before evaluation\n     - Raise ValueError with missing fields list\n     - Require user intervention\n   - **User Impact**: NO-GO decision cannot be made, manual review needed\n\n---\n\n## Testing Strategy\n\n### Unit Testing\n\n**Component 1: Threshold Logic Fix**\n- Test: `test_separate_thresholds()`\n  - Input: Sharpe 0.6, N=20\n  - Expected: bonferroni_threshold=0.5, dynamic_threshold=0.8\n  - Assert: statistically_significant=True, beats_dynamic=False\n\n- Test: `test_bonferroni_calculation()`\n  - Input: N=1, 5, 10, 20, 50\n  - Expected: Correct adjusted_alpha and z_score for each N\n  - Assert: threshold = z_score / sqrt(252)\n\n- Test: `test_validation_passed_logic()`\n  - Input: Various Sharpe ratios (0.4, 0.6, 0.8, 1.0)\n  - Expected: Correct validation_passed for each\n  - Assert: validation_passed = (Sharpe > 0.5) AND (Sharpe >= 0.8)\n\n**Component 2: Duplicate Detector**\n- Test: `test_identical_sharpe_detection()`\n  - Input: Strategies 9 and 13 (Sharpe 0.9443348034803672)\n  - Expected: Grouped together\n  - Assert: len(duplicate_groups) == 1\n\n- Test: `test_ast_similarity()`\n  - Input: Two strategies with only variable name differences\n  - Expected: similarity_score > 0.95\n  - Assert: differences list contains variable name changes\n\n- Test: `test_no_false_positives()`\n  - Input: Two completely different strategies\n  - Expected: similarity_score < 0.5\n  - Assert: Not flagged as duplicates\n\n**Component 3: Diversity Analyzer**\n- Test: `test_factor_extraction()`\n  - Input: Strategy file with known factors\n  - Expected: Set of factor names extracted correctly\n  - Assert: {'price:æ”¶ç›¤åƒ¹', 'RSI', 'ROEç¨…å¾Œ'} in extracted_factors\n\n- Test: `test_jaccard_similarity()`\n  - Input: Two factor sets with known overlap\n  - Expected: Correct Jaccard similarity\n  - Assert: similarity = intersection / union\n\n- Test: `test_diversity_score_calculation()`\n  - Input: 3 strategies with known factor overlap\n  - Expected: diversity_score in 0-100 range\n  - Assert: diversity_score = (1 - avg_similarity) * 100\n\n**Component 4: Re-validation Engine**\n- Test: `test_revalidation_execution()`\n  - Input: 3 strategies (pilot test)\n  - Expected: All execute successfully\n  - Assert: success_rate == 1.0\n\n- Test: `test_comparison_report_generation()`\n  - Input: Original + new results\n  - Expected: Markdown report with before/after tables\n  - Assert: Report contains threshold changes\n\n**Component 5: Decision Framework**\n- Test: `test_go_decision()`\n  - Input: All criteria met\n  - Expected: decision='GO', risk='LOW'\n  - Assert: next_steps includes 'Proceed to Phase 3'\n\n- Test: `test_conditional_go_decision()`\n  - Input: Minimal criteria met (diversity=50)\n  - Expected: decision='CONDITIONAL_GO', risk='MEDIUM'\n  - Assert: mitigation_strategies is not None\n\n- Test: `test_no_go_decision()`\n  - Input: unique_strategies=2 (below minimum)\n  - Expected: decision='NO_GO', risk='HIGH'\n  - Assert: blockers list includes 'unique_strategies'\n\n### Integration Testing\n\n1. **Full Pipeline Test**\n   - Execute all components in sequence\n   - Input: 20 strategies (original dataset)\n   - Expected: GO/CONDITIONAL GO/NO-GO decision\n   - Verify: All intermediate outputs generated\n\n2. **Edge Case Test**\n   - Scenario: All strategies duplicate\n   - Expected: NO-GO decision\n   - Verify: duplicate_report.unique_strategies == 1\n\n3. **Regression Test**\n   - Execute fixed validation on pilot dataset (3 strategies)\n   - Expected: 2/3 validated (67%)\n   - Compare: Original pilot results (should match)\n\n### End-to-End Testing\n\n1. **Scenario: Full Fix + Re-validation + Decision**\n   - Step 1: Run duplicate detection\n   - Step 2: Run diversity analysis\n   - Step 3: Run re-validation with fixes\n   - Step 4: Generate decision report\n   - Expected: Complete workflow in <10 minutes\n   - Verify: Decision document generated\n\n2. **Scenario: User Approval Flow**\n   - Generate decision report\n   - User reviews criteria\n   - User approves/rejects\n   - System proceeds or halts accordingly\n\n---\n\n## Performance Targets\n\n| Operation | Target Time | Justification |\n|-----------|-------------|---------------|\n| Threshold fix implementation | 30 min | Simple code modification |\n| Duplicate detection (20 strategies) | < 30 sec | O(nÂ²) AST comparison acceptable for n=20 |\n| Diversity analysis | < 60 sec | Factor extraction + correlation calculation |\n| Re-validation (20 strategies) | < 350 sec | 17.5s/strategy average |\n| Decision evaluation | < 5 sec | Criteria checking and report generation |\n| **Total workflow** | **< 10 min** | End-to-end from fix to decision |\n\n---\n\n## Security Considerations\n\n1. **Code Injection Prevention**\n   - Duplicate detector uses AST parsing only (no `exec()` or `eval()`)\n   - Strategy files are read-only\n   - No dynamic code generation\n\n2. **File Access Control**\n   - Limit file reads to designated directories:\n     - `generated_strategy_loop_iter*.py`\n     - `phase2_validated_results_*.json`\n   - Validate file paths before access\n   - Reject paths with `..` or absolute paths outside project\n\n3. **Input Validation**\n   - Sharpe ratios: Must be float in range [-10, 10]\n   - Correlation values: Must be in range [-1, 1]\n   - Diversity scores: Must be in range [0, 100]\n   - Reject invalid inputs with descriptive errors\n\n4. **Data Integrity**\n   - Preserve original validation results (create backups)\n   - Use checksums to verify file integrity\n   - Log all modifications to results JSON\n\n---\n\n## Backward Compatibility\n\n**Critical**: Validation results format must remain compatible for comparison.\n\n**Strategy**:\n1. **Add new fields, don't remove old ones**\n   ```json\n   {\n     \"significance_threshold\": 0.5,  // DEPRECATED but kept\n     \"bonferroni_threshold\": 0.5,    // NEW\n     \"dynamic_threshold\": 0.8,       // NEW\n     \"statistically_significant\": true  // FIXED but same name\n   }\n   ```\n\n2. **Deprecation warnings**\n   - Log warnings when old fields are accessed\n   - Document migration path in CHANGELOG\n\n3. **Comparison scripts**\n   - Handle both old and new formats\n   - Map old field names to new equivalents\n\n---\n\n## Rollback Plan\n\nIf bugs are discovered in fixed implementation:\n\n1. **Immediate Rollback**\n   - Revert `run_phase2_with_validation.py` to git commit `86d27f3`\n   - Restore original results JSON from backup\n   - Document issues in rollback report\n\n2. **Backup Strategy**\n   - Before fixes: `cp phase2_validated_results_20251101_060315.json phase2_validated_results_20251101_060315.json.backup`\n   - Before re-validation: `cp run_phase2_with_validation.py run_phase2_with_validation.py.backup`\n\n3. **Verification**\n   - Re-run pilot test (3 strategies) with rolled-back code\n   - Compare results to original pilot test\n   - Verify 0% validation rate returns (confirms rollback)\n\n---\n\n## Documentation Updates\n\n**Files to Update**:\n1. `docs/VALIDATION_FRAMEWORK.md` - Add threshold logic explanation\n2. `docs/DIVERSITY_ANALYSIS.md` - NEW: Diversity metrics guide\n3. `docs/PHASE3_GO_CRITERIA.md` - NEW: Decision framework docs\n4. `CHANGELOG.md` - Document all changes and bug fixes\n\n**README Updates**:\n- Add section on validation framework fixes\n- Update Phase 3 readiness status\n- Document new analysis scripts\n\n---\n\n## Timeline Estimate\n\n| Phase | Tasks | Time |\n|-------|-------|------|\n| **Implementation** | Fix threshold logic, duplicate detector, diversity analyzer | 4-6 hours |\n| **Testing** | Unit tests, integration tests, edge cases | 2-3 hours |\n| **Re-validation** | Execute all 20 strategies | 1-2 hours |\n| **Documentation** | Update docs, generate reports | 1-2 hours |\n| **Total** | | **8-13 hours** |\n\n---\n\n**Generated**: 2025-11-01\n**Design Version**: 1.0\n**Status**: Ready for approval and implementation\n",
  "fileStats": {
    "size": 38541,
    "lines": 1225,
    "lastModified": "2025-10-31T23:11:01.164Z"
  },
  "comments": []
}