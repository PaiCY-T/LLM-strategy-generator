{
  "id": "snapshot_1761954468197_gzi1p81j2",
  "approvalId": "approval_1761954170072_uaea65ssn",
  "approvalTitle": "Review corrected tasks.md (17 tasks, critical fixes applied)",
  "version": 2,
  "timestamp": "2025-10-31T23:47:48.197Z",
  "trigger": "approved",
  "status": "pending",
  "content": "# Validation Framework Critical Fixes - Tasks\n\n**Created**: 2025-11-01\n**Updated**: 2025-11-01 (Corrected after critical review by Gemini 2.5 Pro + OpenAI o3)\n**Priority**: P0 CRITICAL - BLOCKING Phase 3\n**Total Tasks**: 17 (was 15, added Tasks 1.5 and 2.4)\n**Estimated Time**: 10-16 hours (revised from 8-13 hours after detailed analysis)\n\n---\n\n## Phase 1: Threshold Logic Fix (REQ-1)\n\n### Task 1.1: Verify BonferroniIntegrator Already Returns Separate Thresholds\n\n- [ ] 1.1 Verify BonferroniIntegrator threshold output (NO CODE CHANGES NEEDED)\n  - File: src/validation/integration.py (lines 860-916)\n  - **CRITICAL**: BonferroniIntegrator.validate_single_strategy() ALREADY returns both thresholds separately (lines 887-889)\n  - Existing return values: `significance_threshold` (max of both = 0.8), `statistical_threshold` (0.5), `dynamic_threshold` (0.8)\n  - The max() logic (line 866) is BY DESIGN and CORRECT for overall validation\n  - **ACTION**: Verify through code review that lines 887-889 expose both thresholds:\n    ```python\n    if dynamic_threshold is not None:\n        results['dynamic_threshold'] = dynamic_threshold      # 0.8\n        results['statistical_threshold'] = statistical_threshold  # 0.5\n    ```\n  - **NO MODIFICATIONS** to BonferroniIntegrator required - class is architecturally sound\n  - Purpose: Confirm existing architecture supports separate threshold testing (it does)\n  - _Leverage: Existing BonferroniIntegrator design pattern (max for composite + expose components)_\n  - _Requirements: REQ-1 (Acceptance Criteria 1)_\n  - _Prompt: Role: Senior Code Reviewer specializing in validation architecture | Task: Review BonferroniIntegrator.validate_single_strategy() in src/validation/integration.py (lines 860-916) and VERIFY that it already returns three threshold values: 'significance_threshold' (max of statistical and dynamic = 0.8), 'statistical_threshold' (Bonferroni = 0.5), and 'dynamic_threshold' (Taiwan market = 0.8). Confirm lines 887-889 correctly expose both thresholds separately. DO NOT modify code - this is verification only. Document findings. | Restrictions: Do not change any code in BonferroniIntegrator, do not modify max() logic, only review and document | Success: Documentation confirms BonferroniIntegrator returns all three threshold values correctly, max() pattern is intentional and sound, no code changes needed_\n\n### Task 1.2: Fix Threshold Logic in run_phase2_with_validation.py\n\n- [ ] 1.2 Fix threshold comparison logic in validation loop\n  - File: run_phase2_with_validation.py (lines 377-450)\n  - Replace line 398: `bonferroni_threshold = validation.get('significance_threshold', 0.5)` with `bonferroni_threshold = validation.get('statistical_threshold', 0.5)`\n  - Add line 399: `dynamic_threshold_actual = validation.get('dynamic_threshold', 0.8)`\n  - Modify line 403: `statistically_significant = result.sharpe_ratio > bonferroni_threshold` (must use bonferroni_threshold, NOT dynamic_threshold)\n  - Modify line 406: `beats_dynamic = result.sharpe_ratio >= dynamic_threshold_actual`\n  - Modify line 418-419: `validation['validation_passed'] = statistically_significant and beats_dynamic`\n  - Purpose: Fix bug where both tests incorrectly use 0.8 threshold instead of 0.5 for Bonferroni\n  - _Leverage: Modified BonferroniIntegrator from Task 1.1_\n  - _Requirements: REQ-1 (Acceptance Criteria 2, 3)_\n  - _Prompt: Role: Python Backend Developer specializing in validation frameworks | Task: Fix threshold comparison logic in run_phase2_with_validation.py (lines 377-450) to correctly use SEPARATE thresholds for Bonferroni (0.5) and dynamic (0.8) tests, following REQ-1 acceptance criteria 2 and 3. Replace line 398 to get 'statistical_threshold' instead of 'significance_threshold'. Ensure line 403 compares Sharpe to bonferroni_threshold (0.5) and line 406 compares to dynamic_threshold (0.8). Validation passes only if BOTH tests pass. | Restrictions: Must not modify other validation logic, preserve existing logging structure, do not change JSON output format (only values) | Success: Strategies with Sharpe 0.5-0.8 correctly show statistically_significant=true and beats_dynamic_threshold=false, validation_passed requires BOTH tests to pass, no regressions in existing functionality_\n\n### Task 1.3: Update JSON Output Fields\n\n- [ ] 1.3 Update JSON output to include separate threshold fields\n  - File: run_phase2_with_validation.py (lines 452-504)\n  - Modify `_generate_enhanced_json_report()` method (lines 489-502)\n  - Add to per-strategy output: `'bonferroni_threshold': val.get('statistical_threshold')` (NEW)\n  - Add to per-strategy output: `'dynamic_threshold': val.get('dynamic_threshold')` (NEW)\n  - Keep existing `'statistically_significant'` field (updated value from Task 1.2)\n  - Add `'beats_dynamic_threshold'` field (NEW)\n  - Update validation_statistics: Add `'bonferroni_threshold'` and `'bonferroni_alpha'` fields\n  - Purpose: Output correct threshold values in JSON results for comparison and debugging\n  - _Leverage: Fixed threshold logic from Task 1.2_\n  - _Requirements: REQ-1 (Acceptance Criteria 4, 5)_\n  - _Prompt: Role: Python Backend Developer specializing in JSON API design | Task: Update _generate_enhanced_json_report() in run_phase2_with_validation.py (lines 489-502) to output separate bonferroni_threshold (0.5) and dynamic_threshold (0.8) fields in per-strategy validation results, following REQ-1 acceptance criteria 4 and 5. Add new fields while maintaining backward compatibility. Update validation_statistics section to include bonferroni_threshold and bonferroni_alpha. | Restrictions: Must not remove existing fields (backward compatibility), maintain JSON schema structure, do not change field types | Success: JSON output contains bonferroni_threshold=0.5 and dynamic_threshold=0.8 for all strategies, validation_statistics includes both thresholds, approximately 18 strategies show statistically_significant=true (Sharpe > 0.5), 3-4 show validation_passed=true (Sharpe > 0.8)_\n\n### Task 1.4: Add Unit Tests for Threshold Fix\n\n- [ ] 1.4 Create unit tests for threshold separation\n  - File: tests/validation/test_bonferroni_threshold_fix.py (NEW)\n  - Test 1: `test_separate_thresholds()` - Verify BonferroniIntegrator returns both thresholds\n  - Test 2: `test_sharpe_0_6_validation()` - Strategy with Sharpe 0.6 should be statistically_significant=true, beats_dynamic=false, validation_passed=false\n  - Test 3: `test_sharpe_0_85_validation()` - Strategy with Sharpe 0.85 should be statistically_significant=true, beats_dynamic=true, validation_passed=true\n  - Test 4: `test_bonferroni_calculation_N20()` - Verify threshold calculation for N=20 strategies\n  - Test 5: `test_json_output_fields()` - Verify JSON contains all required fields\n  - Purpose: Ensure threshold fix works correctly and catches regressions\n  - _Leverage: Existing test utilities from tests/helpers, pytest framework_\n  - _Requirements: REQ-1 (All Acceptance Criteria)_\n  - _Prompt: Role: QA Engineer with expertise in Python pytest and statistical testing | Task: Create comprehensive unit tests in tests/validation/test_bonferroni_threshold_fix.py to verify threshold separation fix, covering all REQ-1 acceptance criteria. Test that BonferroniIntegrator returns separate thresholds, validation logic correctly identifies strategies in 0.5-0.8 range as statistically significant but not passing dynamic threshold, and JSON output contains correct fields. Use pytest framework and existing test utilities. | Restrictions: Must not modify production code, tests must be independent and reproducible, use mocking for external dependencies | Success: All 5 tests pass, code coverage >90% for modified validation logic, tests catch regression if thresholds are incorrectly combined_\n\n### Task 1.5: Run Pilot Verification Test (3 Strategies)\n\n- [ ] 1.5 Execute pilot test with fixed threshold logic\n  - Command: `python3 run_phase2_with_validation.py --limit 3 --timeout 420`\n  - Purpose: Verify threshold fix works correctly before full 20-strategy re-validation\n  - Expected results:\n    - If strategies 0, 1, 2 tested (Sharpe: 0.681, 0.818, 0.929):\n      - Strategy 0: statistically_significant=true (0.681 > 0.5), beats_dynamic=false (0.681 < 0.8), validation_passed=false\n      - Strategy 1: statistically_significant=true (0.818 > 0.5), beats_dynamic=true (0.818 >= 0.8), validation_passed=true\n      - Strategy 2: statistically_significant=true (0.929 > 0.5), beats_dynamic=true (0.929 >= 0.8), validation_passed=true\n    - JSON output contains bonferroni_threshold=0.5 and dynamic_threshold=0.8\n    - No execution errors or timeouts\n  - If pilot fails: Debug and fix before proceeding to Task 4.1\n  - If pilot succeeds: Document results and proceed to Phase 2\n  - _Leverage: Fixed run_phase2_with_validation.py from Tasks 1.2-1.3_\n  - _Requirements: REQ-1, REQ-4 (Partial - pilot validation)_\n  - _Estimated Time: 30 minutes (5 min execution + 25 min analysis if issues found)_\n\n---\n\n## Phase 2: Duplicate Detection (REQ-2)\n\n### Task 2.1: Create Duplicate Detector Module\n\n- [ ] 2.1 Implement DuplicateDetector class\n  - File: src/analysis/duplicate_detector.py (NEW)\n  - Create `DuplicateDetector` class with methods:\n    - `find_duplicates(strategy_files, validation_results)` - Main entry point\n    - `compare_strategies(strategy_a_path, strategy_b_path)` - AST comparison\n    - `normalize_ast(tree)` - Variable name normalization\n    - `_group_by_sharpe(validation_results)` - Group strategies by Sharpe ratio\n  - Implement AST-based similarity analysis with 95% threshold\n  - Use `ast` module for parsing, `difflib` for diff generation\n  - Return `List[DuplicateGroup]` dataclass instances\n  - Purpose: Detect duplicate strategies using Sharpe matching + AST similarity\n  - _Leverage: Python ast module, dataclasses for DuplicateGroup model_\n  - _Requirements: REQ-2 (Acceptance Criteria 1, 2, 3, 4)_\n  - _Prompt: Role: Python Developer specializing in AST analysis and code similarity detection | Task: Create DuplicateDetector class in new file src/analysis/duplicate_detector.py with methods to find duplicate strategies using Sharpe ratio matching (tolerance 1e-8) followed by AST similarity analysis (threshold 95%), following REQ-2 acceptance criteria 1-4. Implement variable name normalization to detect duplicates that only differ in naming. Use Python ast module for parsing and difflib for generating human-readable diffs. Return DuplicateGroup dataclass instances. | Restrictions: Must not execute strategy code (parse only), handle malformed Python gracefully, do not modify strategy files | Success: Class correctly identifies strategies 9 and 13 as duplicates (identical Sharpe 0.9443348034803672, similarity >95%), generates readable diff showing only variable name differences, returns structured DuplicateGroup objects_\n\n### Task 2.2: Create Duplicate Detection Script\n\n- [ ] 2.2 Create standalone duplicate detection script\n  - File: scripts/detect_duplicates.py (NEW)\n  - Command-line script that:\n    - Loads validation results JSON from file path argument\n    - Scans strategy files (generated_strategy_loop_iter*.py)\n    - Runs DuplicateDetector.find_duplicates()\n    - Generates JSON report and Markdown summary\n    - Outputs recommendations (KEEP/REMOVE) for each strategy\n  - Add argument parsing: `--validation-results`, `--strategy-dir`, `--output`\n  - Purpose: Standalone tool to detect duplicates in validation results\n  - _Leverage: DuplicateDetector from Task 2.1, argparse for CLI_\n  - _Requirements: REQ-2 (Acceptance Criteria 4, 5)_\n  - _Prompt: Role: Python DevOps Engineer specializing in CLI tools and automation | Task: Create standalone script scripts/detect_duplicates.py that uses DuplicateDetector to scan strategy files and validation results, generating JSON + Markdown reports with KEEP/REMOVE recommendations, following REQ-2 acceptance criteria 4 and 5. Add argparse for --validation-results (JSON path), --strategy-dir (strategy files location), --output (report path) arguments. Output both machine-readable JSON and human-readable Markdown. | Restrictions: Must handle missing files gracefully, provide clear error messages, do not modify input files | Success: Script runs successfully on phase2_validated_results_20251101_060315.json, identifies duplicate groups, generates comprehensive report with code diffs and recommendations, exits with appropriate status codes_\n\n### Task 2.3: Add Unit Tests for Duplicate Detector\n\n- [ ] 2.3 Create unit tests for duplicate detection\n  - File: tests/analysis/test_duplicate_detector.py (NEW)\n  - Test 1: `test_identical_sharpe_grouping()` - Verify Sharpe ratio matching\n  - Test 2: `test_ast_similarity_high()` - Test 99% similar strategies (only variable names differ)\n  - Test 3: `test_ast_similarity_low()` - Test completely different strategies (<50% similar)\n  - Test 4: `test_normalize_ast()` - Verify variable name normalization\n  - Test 5: `test_duplicate_report_generation()` - Verify JSON output format\n  - Test 6: `test_strategies_9_13_duplicates()` - Specific test for known duplicate pair\n  - Purpose: Ensure duplicate detection is accurate and catches all edge cases\n  - _Leverage: pytest framework, ast module for creating test cases_\n  - _Requirements: REQ-2 (All Acceptance Criteria)_\n  - _Prompt: Role: QA Engineer with expertise in Python testing and AST manipulation | Task: Create comprehensive unit tests in tests/analysis/test_duplicate_detector.py for DuplicateDetector class, covering all REQ-2 acceptance criteria. Test Sharpe ratio grouping, AST similarity calculation, variable name normalization, and report generation. Include specific test for strategies 9 and 13 (known duplicates). Use pytest framework and create test strategy files programmatically. | Restrictions: Must test edge cases (empty files, syntax errors, zero Sharpe), do not depend on real strategy files, tests must be isolated | Success: All 6 tests pass, code coverage >90%, test correctly identifies strategies 9 and 13 as duplicates, no false positives on truly different strategies_\n\n### Task 2.4: Manual Review of Duplicate Detection Results\n\n- [ ] 2.4 Review duplicate detection report and confirm findings\n  - Action: Execute `python3 scripts/detect_duplicates.py --validation-results phase2_validated_results_20251101_060315.json --strategy-dir . --output duplicate_report.md`\n  - Purpose: Manual verification that duplicate detection correctly identifies strategies 9 and 13\n  - Review checklist:\n    - Confirm strategies 9 and 13 flagged as duplicates (Sharpe 0.9443348034803672)\n    - Review code diff - should show only variable naming differences\n    - Verify no false positives (strategies with different logic incorrectly flagged)\n    - Check recommendations (strategy 9 KEEP, strategy 13 REMOVE)\n    - Validate similarity score >95% for duplicate pair\n  - If incorrect duplicates flagged: Debug DuplicateDetector logic before proceeding\n  - If correct: Document confirmation and proceed to Phase 3\n  - _Leverage: Duplicate report from Task 2.2_\n  - _Requirements: REQ-2 (Acceptance Criteria 4, 5)_\n  - _Estimated Time: 15-30 minutes (review + documentation)_\n\n---\n\n## Phase 3: Diversity Analysis (REQ-3)\n\n### Task 3.1: Create Diversity Analyzer Module\n\n- [ ] 3.1 Implement DiversityAnalyzer class\n  - File: src/analysis/diversity_analyzer.py (NEW)\n  - Create `DiversityAnalyzer` class with methods:\n    - `analyze_diversity(strategy_files, validation_results, exclude_indices)` - Main entry point\n    - `extract_factors(strategy_path)` - Parse data.get() calls from AST\n    - `calculate_factor_diversity(factor_sets)` - Jaccard similarity matrix\n    - `calculate_return_correlation(strategy_returns)` - Correlation matrix\n    - `calculate_risk_diversity(validation_results, strategy_indices)` - CV of max drawdowns\n  - Calculate overall diversity score (0-100): factor_diversity * 0.5 + (1-avg_corr) * 100 * 0.3 + risk_diversity * 0.2\n  - Return `DiversityReport` dataclass instance\n  - Purpose: Analyze validated strategies for factor diversity, correlation, and risk profile spread\n  - _Leverage: numpy for correlation matrices, pandas for data manipulation, ast for factor extraction_\n  - _Requirements: REQ-3 (Acceptance Criteria 1, 2, 3, 4, 5)_\n  - _Prompt: Role: Data Scientist specializing in portfolio analysis and Python statistical computing | Task: Create DiversityAnalyzer class in new file src/analysis/diversity_analyzer.py with methods to analyze strategy diversity using factor Jaccard similarity, return correlation, and risk profile CV, following REQ-3 acceptance criteria 1-5. Extract factors from strategy code by parsing data.get() calls. Calculate Jaccard similarity matrix for factor sets, correlation matrix for returns (use Sharpe as proxy if returns unavailable), and coefficient of variation for max drawdowns. Combine into overall diversity score (0-100). Return DiversityReport dataclass. | Restrictions: Must handle missing data gracefully, do not execute strategy code, validate all calculations return values in expected ranges | Success: Class calculates all diversity metrics correctly, identifies high correlation (>0.8) and low factor diversity (<0.5) with appropriate warnings, returns comprehensive DiversityReport with recommendation (SUFFICIENT/MARGINAL/INSUFFICIENT)_\n\n### Task 3.2: Create Diversity Analysis Script\n\n- [ ] 3.2 Create standalone diversity analysis script\n  - File: scripts/analyze_diversity.py (NEW)\n  - Command-line script that:\n    - Loads validation results JSON\n    - Filters to validated strategies only\n    - Excludes duplicate strategies if duplicate report provided\n    - Runs DiversityAnalyzer.analyze_diversity()\n    - Generates JSON report and Markdown summary with visualizations\n  - Add arguments: `--validation-results`, `--duplicate-report` (optional), `--strategy-dir`, `--output`\n  - Generate correlation heatmap and factor usage chart (save as PNG)\n  - Purpose: Standalone tool to assess diversity of validated strategies\n  - _Leverage: DiversityAnalyzer from Task 3.1, matplotlib/seaborn for visualization_\n  - _Requirements: REQ-3 (Acceptance Criteria 5)_\n  - _Prompt: Role: Data Engineer specializing in data analysis pipelines and visualization | Task: Create standalone script scripts/analyze_diversity.py that uses DiversityAnalyzer to assess strategy diversity, generating comprehensive report with visualizations, following REQ-3 acceptance criterion 5. Accept --validation-results (JSON), optional --duplicate-report (to exclude duplicates), --strategy-dir, --output arguments. Generate correlation matrix heatmap and factor usage bar chart using matplotlib/seaborn. Output JSON report + Markdown summary with embedded charts. | Restrictions: Must handle missing matplotlib/seaborn gracefully (skip visualizations), validate input files exist, provide clear error messages | Success: Script runs successfully on validation results, excludes duplicates if report provided, generates diversity score (0-100), creates visualizations (correlation heatmap, factor chart), outputs recommendation (SUFFICIENT if score >=60, MARGINAL if 40-60, INSUFFICIENT if <40)_\n\n### Task 3.3: Add Unit Tests for Diversity Analyzer\n\n- [ ] 3.3 Create unit tests for diversity analysis\n  - File: tests/analysis/test_diversity_analyzer.py (NEW)\n  - Test 1: `test_factor_extraction()` - Verify data.get() calls extracted correctly\n  - Test 2: `test_jaccard_similarity()` - Test factor set similarity calculation\n  - Test 3: `test_diversity_score_high()` - Test completely diverse strategies (score ~100)\n  - Test 4: `test_diversity_score_low()` - Test similar strategies (score <40)\n  - Test 5: `test_correlation_warning()` - Verify warning triggered when avg_corr > 0.8\n  - Test 6: `test_risk_diversity_cv()` - Test coefficient of variation calculation\n  - Purpose: Ensure diversity analysis is mathematically correct and catches edge cases\n  - _Leverage: pytest framework, numpy for test data generation_\n  - _Requirements: REQ-3 (All Acceptance Criteria)_\n  - _Prompt: Role: QA Engineer with expertise in statistical testing and Python scientific computing | Task: Create comprehensive unit tests in tests/analysis/test_diversity_analyzer.py for DiversityAnalyzer class, covering all REQ-3 acceptance criteria. Test factor extraction from AST, Jaccard similarity calculation, diversity score computation (0-100 range), correlation warning (>0.8), and risk diversity CV. Create synthetic test data with known diversity properties. Use pytest and numpy. | Restrictions: Must test mathematical correctness, validate output ranges, do not depend on real strategy files, tests must be deterministic | Success: All 6 tests pass, code coverage >90%, diversity score correctly ranges 0-100, high correlation (>0.8) triggers warning flag, recommendations align with score thresholds_\n\n---\n\n## Phase 4: Re-validation Execution (REQ-4)\n\n### Task 4.1: Create Re-validation Script\n\n- [ ] 4.1 Create re-validation script with fixed threshold logic\n  - File: scripts/run_revalidation_with_fixes.py (NEW)\n  - Copy validation logic from run_phase2_with_validation.py\n  - Apply fixes from Tasks 1.1-1.3 (separate thresholds)\n  - Execute all 20 strategies with same parameters as original\n  - Save results to new JSON file: `phase2_validated_results_FIXED_YYYYMMDD_HHMMSS.json`\n  - Generate before/after comparison report (Markdown)\n  - Purpose: Re-execute validation with corrected threshold logic\n  - _Leverage: Fixed Phase2WithValidation logic from Phase 1, existing BacktestExecutor_\n  - _Requirements: REQ-4 (Acceptance Criteria 1, 2, 3)_\n  - _Prompt: Role: Python Backend Developer specializing in validation frameworks and automation | Task: Create re-validation script scripts/run_revalidation_with_fixes.py that re-executes all 20 strategies using FIXED threshold logic from Tasks 1.1-1.3, following REQ-4 acceptance criteria 1-3. Load original results from phase2_validated_results_20251101_060315.json for comparison. Execute strategies with same parameters (timeout 420s, dynamic_threshold 0.8, etc). Save results to timestamped JSON file. Maintain 100% execution success rate. | Restrictions: Must not modify strategy files, use exact same execution parameters as original, do not skip any strategies | Success: All 20 strategies execute successfully (100% rate), results JSON contains bonferroni_threshold=0.5 (not 0.8), approximately 18 strategies show statistically_significant=true (Sharpe > 0.5), 3-4 unique strategies show validation_passed=true (Sharpe > 0.8), execution time <350 seconds total_\n\n### Task 4.2: Generate Comparison Report\n\n- [ ] 4.2 Implement before/after comparison report generator\n  - File: scripts/run_revalidation_with_fixes.py (continue from Task 4.1)\n  - Add `generate_comparison_report(original_results, new_results)` method\n  - Compare:\n    - Threshold values (before: 0.8, after: 0.5 + 0.8)\n    - Statistical significance counts (before: 4, after: ~18)\n    - Validation pass counts (before: 4, after: 3 unique after duplicate removal)\n    - Execution times\n  - Generate Markdown table with side-by-side comparison\n  - Highlight strategies that changed status (false → true for statistically_significant)\n  - Purpose: Document validation framework fix effectiveness\n  - _Leverage: Original and new validation results JSON files_\n  - _Requirements: REQ-4 (Acceptance Criteria 4)_\n  - _Prompt: Role: Technical Writer with Python development skills | Task: Implement comparison report generator in scripts/run_revalidation_with_fixes.py that creates detailed before/after analysis of validation framework fixes, following REQ-4 acceptance criterion 4. Compare threshold values, statistical significance counts, validation pass counts, and execution times between original (phase2_validated_results_20251101_060315.json) and new results. Generate Markdown report with side-by-side tables highlighting changes. Show which strategies changed from statistically_significant=false to true. | Restrictions: Must use actual data from both JSON files, do not fabricate comparison data, maintain markdown formatting | Success: Report clearly shows bonferroni_threshold changed from 0.8 to 0.5, statistical significance count increased from 4 to ~18, validation pass count corrected to 3 unique (after duplicate removal), report is human-readable and highlights key improvements_\n\n### Task 4.3: Add Integration Test for Re-validation\n\n- [ ] 4.3 Create integration test for re-validation script\n  - File: tests/integration/test_revalidation_script.py (NEW)\n  - Test 1: `test_revalidation_execution()` - Run script on 3-strategy subset (pilot)\n  - Test 2: `test_threshold_fix_verification()` - Verify bonferroni_threshold=0.5 in output\n  - Test 3: `test_statistical_significance_count()` - Verify ~18 strategies significant (full 20)\n  - Test 4: `test_comparison_report_generation()` - Verify Markdown report created\n  - Test 5: `test_execution_success_rate()` - Verify 100% success rate maintained\n  - Purpose: Ensure re-validation script works correctly end-to-end\n  - _Leverage: pytest framework, subprocess for script execution, JSON validation_\n  - _Requirements: REQ-4 (All Acceptance Criteria)_\n  - _Prompt: Role: QA Engineer specializing in integration testing and subprocess management | Task: Create integration tests in tests/integration/test_revalidation_script.py for re-validation script, covering all REQ-4 acceptance criteria. Test script execution on subset (3 strategies) and full set (20 strategies), verify threshold fix in JSON output, check statistical significance counts, validate comparison report generation, and confirm 100% execution success rate. Use subprocess to run script, parse JSON output for validation. | Restrictions: Must use test data or subset to avoid long test times, validate JSON schema, do not modify production code during tests | Success: All 5 tests pass, script executes without errors, JSON output validated against schema, comparison report generated correctly, tests complete in <2 minutes for subset_\n\n---\n\n## Phase 5: Decision Framework (REQ-5)\n\n### Task 5.1: Create Decision Framework Module\n\n- [ ] 5.1 Implement DecisionFramework class\n  - File: src/analysis/decision_framework.py (NEW)\n  - Create `DecisionFramework` class with methods:\n    - `evaluate(validation_results, duplicate_report, diversity_report)` - Main decision logic\n    - `check_go_criteria(...)` - Check all GO criteria (unique>=3, diversity>=60, etc.)\n    - `check_conditional_criteria(...)` - Check CONDITIONAL GO criteria (diversity>=40)\n    - `assess_risk(decision, criteria_met)` - Risk level (LOW/MEDIUM/HIGH)\n    - `generate_decision_document(...)` - Markdown report generation\n  - Implement decision tree:\n    - GO: All criteria met (unique>=3, diversity>=60, corr<0.8, validation fixed, exec=100%)\n    - CONDITIONAL GO: Minimal criteria (unique>=3, diversity>=40, corr<0.8, validation fixed, exec=100%)\n    - NO-GO: Any criterion fails\n  - Return `DecisionReport` dataclass instance\n  - Purpose: Automate Phase 3 GO/NO-GO decision based on evidence\n  - _Leverage: Validation, duplicate, and diversity reports from previous phases_\n  - _Requirements: REQ-5 (Acceptance Criteria 1, 2, 3, 4)_\n  - _Prompt: Role: Product Manager with Python development skills and decision analysis expertise | Task: Create DecisionFramework class in new file src/analysis/decision_framework.py that evaluates Phase 3 GO/NO-GO criteria using validation, duplicate, and diversity analysis results, following REQ-5 acceptance criteria 1-4. Implement decision tree with GO (all criteria), CONDITIONAL GO (minimal criteria + mitigation), and NO-GO (any blocker) paths. Check: min 3 unique strategies, diversity score (60 for GO, 40 for CONDITIONAL), avg correlation <0.8, validation framework fixed, 100% execution rate. Return DecisionReport dataclass with decision, rationale, criteria status, risk assessment, and next steps. | Restrictions: Must follow specified decision criteria exactly, do not add subjective judgment, ensure decision is deterministic based on inputs | Success: Class correctly evaluates decision based on criteria, returns GO for ideal case (unique=4, diversity=70), CONDITIONAL GO for marginal case (unique=3, diversity=50), NO-GO for insufficient case (unique=2), decision document clearly explains rationale and next steps_\n\n### Task 5.2: Create Decision Evaluation Script\n\n- [ ] 5.2 Create standalone decision evaluation script\n  - File: scripts/evaluate_phase3_decision.py (NEW)\n  - Command-line script that:\n    - Loads validation results (fixed), duplicate report, diversity report\n    - Runs DecisionFramework.evaluate()\n    - Generates comprehensive decision document (Markdown)\n    - Outputs decision to console (GO/CONDITIONAL GO/NO-GO)\n    - Includes mitigation strategies if CONDITIONAL GO\n    - Lists blocking issues if NO-GO\n  - Add arguments: `--validation-results`, `--duplicate-report`, `--diversity-report`, `--output`\n  - Purpose: Generate final Phase 3 GO/NO-GO decision report\n  - _Leverage: DecisionFramework from Task 5.1, all previous analysis outputs_\n  - _Requirements: REQ-5 (All Acceptance Criteria)_\n  - _Prompt: Role: DevOps Engineer specializing in automation and decision support tools | Task: Create standalone script scripts/evaluate_phase3_decision.py that uses DecisionFramework to generate Phase 3 GO/NO-GO decision, following all REQ-5 acceptance criteria. Accept --validation-results (fixed JSON), --duplicate-report (JSON), --diversity-report (JSON), --output (decision doc path) arguments. Generate comprehensive Markdown decision document including criteria evaluation table, rationale, risk assessment, and next steps. Output decision to console with color coding (green=GO, yellow=CONDITIONAL, red=NO-GO). Include mitigation strategies for CONDITIONAL GO or blocking issues for NO-GO. | Restrictions: Must require all three input reports, validate JSON schemas, do not proceed without complete data, exit with status code (0=GO, 1=CONDITIONAL, 2=NO-GO) | Success: Script generates complete decision document, correctly identifies decision based on inputs, outputs clear console summary, exit code matches decision, mitigation strategies or blockers are actionable and specific_\n\n### Task 5.3: Add Unit Tests for Decision Framework\n\n- [ ] 5.3 Create unit tests for decision framework\n  - File: tests/analysis/test_decision_framework.py (NEW)\n  - Test 1: `test_go_decision()` - All criteria met (unique=4, diversity=70, corr=0.4)\n  - Test 2: `test_conditional_go_decision()` - Minimal criteria (unique=3, diversity=50)\n  - Test 3: `test_no_go_insufficient_strategies()` - unique=2 (below minimum)\n  - Test 4: `test_no_go_low_diversity()` - diversity=30 (below threshold)\n  - Test 5: `test_no_go_high_correlation()` - avg_corr=0.85 (above threshold)\n  - Test 6: `test_risk_assessment()` - Verify LOW/MEDIUM/HIGH risk assignment\n  - Test 7: `test_decision_document_generation()` - Verify Markdown format\n  - Purpose: Ensure decision framework logic is correct and catches all criteria\n  - _Leverage: pytest framework, synthetic test data with known decision outcomes_\n  - _Requirements: REQ-5 (All Acceptance Criteria)_\n  - _Prompt: Role: QA Engineer with expertise in decision logic testing and test data generation | Task: Create comprehensive unit tests in tests/analysis/test_decision_framework.py for DecisionFramework class, covering all REQ-5 acceptance criteria. Test all three decision outcomes (GO, CONDITIONAL GO, NO-GO) with appropriate input data. Verify criteria evaluation logic, risk assessment assignment, decision document generation. Create synthetic validation, duplicate, and diversity reports with known properties. Use pytest framework. | Restrictions: Must test all decision paths, validate decision document format, do not use real production data, tests must be deterministic | Success: All 7 tests pass, code coverage >90%, decision logic correctly handles all criteria combinations, risk assessment aligns with decision (GO=LOW, CONDITIONAL=MEDIUM, NO-GO=HIGH), decision documents are well-formatted and complete_\n\n---\n\n## Phase 6: Integration and Documentation\n\n### Task 6.1: Create Master Workflow Script\n\n- [ ] 6.1 Create end-to-end workflow automation script\n  - File: scripts/run_full_validation_workflow.sh (NEW)\n  - Bash script that orchestrates all steps:\n    1. Run re-validation with fixes (`run_revalidation_with_fixes.py`)\n    2. Run duplicate detection (`detect_duplicates.py`)\n    3. Run diversity analysis (`analyze_diversity.py`)\n    4. Run decision evaluation (`evaluate_phase3_decision.py`)\n  - Add error handling: stop on failure, log all outputs\n  - Generate final summary report combining all analyses\n  - Purpose: One-command workflow from validation fix to Phase 3 decision\n  - _Leverage: All scripts from Phases 1-5_\n  - _Requirements: All REQs_\n  - _Prompt: Role: DevOps Engineer specializing in shell scripting and workflow automation | Task: Create master workflow script scripts/run_full_validation_workflow.sh that orchestrates complete validation fix workflow, running all scripts in sequence (re-validation, duplicate detection, diversity analysis, decision evaluation). Add error handling to stop on any failure, log all command outputs to timestamped log file, generate final summary report combining all analyses. Accept optional --skip-revalidation flag to use existing results. Check all dependencies (Python packages, input files) before execution. | Restrictions: Must check all prerequisites, validate each step succeeded before proceeding, provide clear error messages, do not proceed on partial failures | Success: Script runs all steps successfully end-to-end in <10 minutes, generates complete workflow log, produces final decision document, exits with appropriate status code (0=GO, 1=CONDITIONAL, 2=NO-GO), handles errors gracefully with informative messages_\n\n### Task 6.2: Update Documentation\n\n- [ ] 6.2 Update project documentation with validation framework fixes\n  - Files:\n    - docs/VALIDATION_FRAMEWORK.md (UPDATE)\n    - docs/DIVERSITY_ANALYSIS.md (NEW)\n    - docs/PHASE3_GO_CRITERIA.md (NEW)\n    - CHANGELOG.md (UPDATE)\n    - README.md (UPDATE)\n  - Document:\n    - Threshold logic bug and fix (VALIDATION_FRAMEWORK.md)\n    - Diversity metrics and interpretation (DIVERSITY_ANALYSIS.md)\n    - Decision framework criteria and process (PHASE3_GO_CRITERIA.md)\n    - All changes in CHANGELOG.md (version, date, changes)\n    - Update README.md Phase 3 readiness section\n  - Purpose: Ensure all fixes and analyses are documented for future reference\n  - _Leverage: Markdown templates, existing documentation structure_\n  - _Requirements: All REQs_\n  - _Prompt: Role: Technical Writer with deep understanding of statistical validation | Task: Update project documentation to reflect validation framework fixes and new analysis capabilities. Update docs/VALIDATION_FRAMEWORK.md explaining threshold bug (0.8 vs 0.5) and fix. Create docs/DIVERSITY_ANALYSIS.md documenting diversity metrics (factor Jaccard, correlation, risk CV) and interpretation. Create docs/PHASE3_GO_CRITERIA.md explaining decision framework criteria and thresholds. Update CHANGELOG.md with version entry listing all changes. Update README.md Phase 3 readiness section with new validation statistics. | Restrictions: Must be accurate and technically precise, use clear language for non-experts, maintain consistent formatting with existing docs, include examples where helpful | Success: Documentation is complete and accurate, explains both problem and solution clearly, includes examples and edge cases, CHANGELOG properly versioned, README updated with corrected statistics (15% validation rate, 3 unique strategies)_\n\n### Task 6.3: Final Integration Testing\n\n- [ ] 6.3 Run complete end-to-end validation workflow test\n  - File: tests/integration/test_full_workflow.py (NEW)\n  - Integration test that:\n    - Runs master workflow script on test data (5 strategies subset)\n    - Verifies all intermediate outputs generated (JSON, Markdown reports)\n    - Validates final decision document produced\n    - Checks all files created in expected locations\n    - Verifies workflow completes in <2 minutes for test subset\n  - Purpose: Ensure entire workflow works together seamlessly\n  - _Leverage: Master workflow script from Task 6.1, pytest subprocess execution_\n  - _Requirements: All REQs_\n  - _Prompt: Role: QA Engineer specializing in end-to-end integration testing | Task: Create integration test in tests/integration/test_full_workflow.py that runs complete validation workflow on test dataset (5 strategies) and validates all outputs, covering all requirements. Use subprocess to execute master workflow script (run_full_validation_workflow.sh). Verify all expected files generated (validation results JSON, duplicate report, diversity report, decision document). Parse outputs to validate structure and content. Check workflow completes successfully with appropriate exit code. Use pytest framework. | Restrictions: Must use test subset to keep test time reasonable (<2 min), validate file existence and structure but not detailed content, do not mock script execution (run real scripts) | Success: Test passes when workflow runs successfully end-to-end, all expected outputs generated, decision document contains required sections, workflow completes in <2 minutes for 5-strategy test set, test catches missing dependencies or script failures_\n\n---\n\n## Completion Checklist\n\nAfter all tasks complete, verify:\n\n- [ ] Threshold fix verified: Bonferroni threshold = 0.5, dynamic threshold = 0.8\n- [ ] Re-validation executed: 20/20 strategies, ~18 statistically significant, 3-4 validated\n- [ ] Duplicates identified: Strategies 9 and 13 flagged, recommendations generated\n- [ ] Diversity analyzed: Score calculated, correlation matrix generated, recommendation provided\n- [ ] Decision made: GO/CONDITIONAL GO/NO-GO with rationale and next steps\n- [ ] All tests passing: Unit tests >90% coverage, integration tests pass\n- [ ] Documentation updated: CHANGELOG, README, validation docs\n- [ ] User approval obtained: Decision document reviewed and approved\n\n---\n\n**Total Estimated Time**: 8-13 hours\n**Critical Path**: Tasks 1.1-1.3 → 4.1-4.2 → 5.1-5.2 → 6.1\n**Parallel Work Possible**: Phase 2 (duplicate detection) and Phase 3 (diversity analysis) can run in parallel with Phase 1 (threshold fix)\n\n**Generated**: 2025-11-01\n**Status**: Ready for implementation\n",
  "fileStats": {
    "size": 38465,
    "lines": 399,
    "lastModified": "2025-10-31T23:39:47.705Z"
  },
  "comments": []
}