{
  "id": "snapshot_1761401751081_q8t9h41qc",
  "approvalId": "approval_1761396011817_qoq0gu5lt",
  "approvalTitle": "Review tasks.md for llm-integration-activation (14 tasks, 2-3 days)",
  "version": 2,
  "timestamp": "2025-10-25T14:15:51.081Z",
  "trigger": "approved",
  "status": "pending",
  "content": "# Tasks Document: LLM Integration Activation\n\n## Phase 1: Core LLM Components (Tasks 1-4)\n\n- [ ] 1. Create LLMProviderInterface abstract base class\n  - File: `src/innovation/llm_providers.py`\n  - Define abstract interface for LLM API providers\n  - Implement OpenRouterProvider, GeminiProvider, OpenAIProvider\n  - Add cost estimation method for each provider\n  - Purpose: Provider-agnostic LLM API abstraction\n  - _Leverage: `requests` library for HTTP calls, existing API patterns_\n  - _Requirements: 2.1, 2.2, 2.3, 2.4_\n  - _Prompt: Implement the task for spec llm-integration-activation, first run spec-workflow-guide to get the workflow guide then implement the task: Role: Backend Developer with expertise in API abstraction and Python ABC | Task: Create LLMProviderInterface with 3 concrete implementations (OpenRouter/Gemini/OpenAI) following requirements 2.1-2.4, ensuring consistent interface across providers | Restrictions: Must handle API errors uniformly, implement timeouts (60s), support environment variable API keys | _Leverage: requests library, ABC module | _Requirements: Requirements 2.1-2.4 (Provider abstraction and implementations) | Success: All 3 providers implement interface correctly, API calls work with proper error handling | Instructions: Set task to [-] in tasks.md, mark [x] when provider tests pass_\n\n- [ ] 2. Create PromptBuilder module\n  - File: `src/innovation/prompt_builder.py`\n  - Implement modification prompt construction with champion feedback\n  - Implement creation prompt construction with innovation guidance\n  - Extract success factors from champion code and metrics\n  - Extract failure patterns from failure history JSON\n  - Purpose: Generate effective LLM prompts with context\n  - _Leverage: Existing `failure_patterns.json`, champion tracking from autonomous_loop_\n  - _Requirements: 3.1, 3.2, 3.3_\n  - _Prompt: Implement the task for spec llm-integration-activation, first run spec-workflow-guide to get the workflow guide then implement the task: Role: Prompt Engineer with expertise in LLM instruction design and Python | Task: Create PromptBuilder class following requirements 3.1-3.3, constructing modification and creation prompts with champion feedback and failure patterns | Restrictions: Must include FinLab API constraints in prompts, add few-shot examples, keep prompts under 2000 tokens | _Leverage: artifacts/data/failure_patterns.json, champion code from loop | _Requirements: Requirements 3.1-3.3 (Prompt construction with feedback) | Success: Prompts are well-structured with examples, include relevant feedback, guide LLM effectively | Instructions: Set task to [-], mark [x] when PromptBuilder tests pass_\n\n- [ ] 3. Extend InnovationEngine with feedback loop\n  - File: `src/innovation/innovation_engine.py` (modify existing)\n  - Add `generate_with_feedback()` method using PromptBuilder\n  - Integrate LLMProvider for API calls\n  - Parse LLM response and extract Python code using regex\n  - Handle API errors with retries and fallback signaling\n  - Purpose: Complete LLM-driven innovation capability\n  - _Leverage: Existing InnovationEngine base, new LLMProvider and PromptBuilder_\n  - _Requirements: 1.3, 1.4, 1.5_\n  - _Prompt: Implement the task for spec llm-integration-activation, first run spec-workflow-guide to get the workflow guide then implement the task: Role: ML Engineer with expertise in LLM integration and Python | Task: Extend existing InnovationEngine with feedback-driven generation following requirements 1.3-1.5, integrating PromptBuilder and LLMProvider with robust error handling | Restrictions: Must retry on rate limits (3 attempts), parse code using regex safely, signal fallback on failures | _Leverage: src/innovation/innovation_engine.py, LLMProvider, PromptBuilder | _Requirements: Requirements 1.3-1.5 (Feedback loop, API calls, error handling) | Success: InnovationEngine generates code via LLM, handles errors gracefully, returns valid Python or signals fallback | Instructions: Set task to [-], mark [x] when InnovationEngine feedback tests pass_\n\n- [ ] 4. Create LLMConfig dataclass\n  - File: `src/innovation/llm_config.py`\n  - Define LLM configuration dataclass (provider, model, api_key, innovation_rate, etc.)\n  - Load from `config/learning_system.yaml`\n  - Validate API keys and configuration values\n  - Purpose: Centralized LLM configuration management\n  - _Leverage: Existing config loading patterns from `config/learning_system.yaml`_\n  - _Requirements: 2.1_\n  - _Prompt: Implement the task for spec llm-integration-activation, first run spec-workflow-guide to get the workflow guide then implement the task: Role: DevOps Engineer with expertise in configuration management and Python dataclasses | Task: Create LLMConfig dataclass following requirement 2.1, loading from YAML with validation for API keys and sensible defaults | Restrictions: Must support environment variable substitution for API keys, validate innovation_rate (0.0-1.0), provide defaults | _Leverage: config/learning_system.yaml loading patterns | _Requirements: Requirement 2.1 (LLM configuration) | Success: Config loads from YAML, validates parameters, supports env vars for secrets | Instructions: Set task to [-], mark [x] when LLMConfig tests pass_\n\n## Phase 2: Integration (Tasks 5-6)\n\n- [ ] 5. Integrate LLM into autonomous loop\n  - File: `artifacts/working/modules/autonomous_loop.py` (modify)\n  - Initialize InnovationEngine with configured LLM provider at loop startup\n  - Use LLM for 20% of iterations (iteration % 5 == 0)\n  - Implement fallback to Factor Graph on LLM failures\n  - Track LLM success/failure rates via metrics\n  - Purpose: Activate LLM-driven innovation in production loop\n  - _Leverage: Existing autonomous_loop.py, InnovationEngine, FactorGraph mutation_\n  - _Requirements: 1.1, 1.2, 5.1, 5.2, 5.3, 5.4, 5.5_\n  - _Prompt: Implement the task for spec llm-integration-activation, first run spec-workflow-guide to get the workflow guide then implement the task: Role: Backend Developer with expertise in system integration and error handling | Task: Integrate InnovationEngine into autonomous_loop.py following requirements 1.1-1.2 and 5.1-5.5, routing 20% of iterations to LLM with automatic fallback to Factor Graph on failures | Restrictions: Must maintain 100% iteration success rate via fallback, log all LLM calls and outcomes, disable LLM if auth fails | _Leverage: artifacts/working/modules/autonomous_loop.py, src/innovation/innovation_engine.py, src/mutation/factor_graph.py | _Requirements: Requirements 1.1-1.2 (LLM integration), 5.1-5.5 (Fallback mechanisms) | Success: 20% iterations use LLM, failures fallback to Factor Graph, loop never stalls | Instructions: Set task to [-], mark [x] when integration tests pass with LLM enabled_\n\n- [ ] 6. Add LLM configuration to learning system config\n  - File: `config/learning_system.yaml` (modify)\n  - Add `llm` section with provider selection, API keys, innovation rate\n  - Add provider-specific subsections (openrouter, gemini, openai)\n  - Add generation and fallback settings\n  - Document all configuration options\n  - Purpose: Enable LLM configuration in production\n  - _Leverage: Existing learning_system.yaml structure_\n  - _Requirements: 2.1, 2.2, 2.3, 2.4_\n  - _Prompt: Implement the task for spec llm-integration-activation, first run spec-workflow-guide to get the workflow guide then implement the task: Role: DevOps Engineer with expertise in YAML configuration and secrets management | Task: Extend learning_system.yaml with LLM configuration section following requirements 2.1-2.4, supporting 3 providers with environment variable API keys | Restrictions: Must maintain backward compatibility (default LLM disabled), use ${ENV_VAR} syntax for secrets, document all options | _Leverage: config/learning_system.yaml | _Requirements: Requirements 2.1-2.4 (Provider configuration) | Success: LLM can be configured and disabled via YAML, API keys from env vars, well-documented | Instructions: Set task to [-], mark [x] when config schema validated_\n\n## Phase 3: Prompt Engineering (Tasks 7-8)\n\n- [ ] 7. Create modification prompt template\n  - File: `src/innovation/prompts/modification_template.txt`\n  - Design prompt template for modifying champion strategies\n  - Include placeholders for champion code, metrics, success factors, failure patterns\n  - Add FinLab API constraints and few-shot examples\n  - Purpose: Guide LLM to make effective strategy modifications\n  - _Leverage: Few-shot examples from existing successful strategies_\n  - _Requirements: 4.1, 4.2, 4.3, 4.4_\n  - _Prompt: Implement the task for spec llm-integration-activation, first run spec-workflow-guide to get the workflow guide then implement the task: Role: Prompt Engineer with expertise in LLM instruction design and quantitative finance | Task: Design modification prompt template following requirements 4.1-4.4, including champion context, success factors, failure patterns, and clear constraints | Restrictions: Must include function signature requirements, liquidity constraints (>150M), rebalancing frequency, add 2 few-shot examples | _Leverage: Existing successful strategies for examples | _Requirements: Requirements 4.1-4.4 (Prompt structure and constraints) | Success: Prompt template is clear, includes all placeholders, guides LLM to valid modifications | Instructions: Set task to [-], mark [x] when template reviewed and validated_\n\n- [ ] 8. Create creation prompt template\n  - File: `src/innovation/prompts/creation_template.txt`\n  - Design prompt template for creating novel strategies\n  - Include placeholders for champion approach, failure patterns, innovation directive\n  - Add FinLab API constraints and few-shot examples\n  - Purpose: Guide LLM to create novel viable strategies\n  - _Leverage: Few-shot examples from successful novel strategies_\n  - _Requirements: 4.1, 4.2, 4.3, 4.4_\n  - _Prompt: Implement the task for spec llm-integration-activation, first run spec-workflow-guide to get the workflow guide then implement the task: Role: Prompt Engineer with expertise in creative LLM prompting and strategy design | Task: Design creation prompt template following requirements 4.1-4.4, inspiring novel approaches while maintaining constraints and viability | Restrictions: Must include clear novelty directive, FinLab API constraints, function signature, add 2 few-shot examples of novel strategies | _Leverage: Existing novel successful strategies for examples | _Requirements: Requirements 4.1-4.4 (Prompt structure and constraints) | Success: Prompt template guides LLM to novel but valid strategies, clear constraints included | Instructions: Set task to [-], mark [x] when template reviewed and validated_\n\n## Phase 4: Testing (Tasks 9-12)\n\n- [ ] 9. Write LLMProvider unit tests\n  - File: `tests/innovation/test_llm_providers.py`\n  - Mock API responses for all 3 providers (OpenRouter, Gemini, OpenAI)\n  - Test timeout enforcement (60s)\n  - Test retry logic on rate limits (exponential backoff)\n  - Test error handling (auth errors, network errors, invalid responses)\n  - Coverage target: >85%\n  - _Leverage: `pytest`, `unittest.mock` or `responses` library for HTTP mocking_\n  - _Requirements: 2.1, 2.2, 2.3, 2.4_\n  - _Prompt: Implement the task for spec llm-integration-activation, first run spec-workflow-guide to get the workflow guide then implement the task: Role: QA Engineer with expertise in API testing and mocking | Task: Create unit tests for all 3 LLMProvider implementations following requirements 2.1-2.4, mocking HTTP calls and testing error scenarios | Restrictions: Must mock all API calls (no real LLM calls), test timeout and retry logic, achieve >85% coverage | _Leverage: pytest, unittest.mock or responses library | _Requirements: Requirements 2.1-2.4 (Provider implementations) | Success: All providers tested with mocks, error scenarios covered, >85% coverage | Instructions: Set task to [-], mark [x] when tests pass with >85% coverage_\n\n- [ ] 10. Write PromptBuilder unit tests\n  - File: `tests/innovation/test_prompt_builder.py`\n  - Test modification prompt generation with champion data\n  - Test creation prompt generation with failure patterns\n  - Test success factor extraction from code and metrics\n  - Test failure pattern extraction from JSON\n  - Coverage target: >90%\n  - _Leverage: `pytest`, mock champion data and failure patterns_\n  - _Requirements: 3.1, 3.2, 3.3_\n  - _Prompt: Implement the task for spec llm-integration-activation, first run spec-workflow-guide to get the workflow guide then implement the task: Role: QA Engineer with expertise in unit testing and data transformation | Task: Create unit tests for PromptBuilder following requirements 3.1-3.3, testing prompt generation with various champion and failure data | Restrictions: Must test with realistic champion/failure data, verify prompt structure, achieve >90% coverage | _Leverage: pytest, mock data fixtures | _Requirements: Requirements 3.1-3.3 (Prompt construction) | Success: Prompts generated correctly for all scenarios, success/failure extraction works, >90% coverage | Instructions: Set task to [-], mark [x] when tests pass with >90% coverage_\n\n- [ ] 11. Write InnovationEngine integration tests with LLM\n  - File: `tests/integration/test_llm_innovation.py`\n  - Test 1: Call real LLM API (1 test call), verify code extraction\n  - Test 2: Mock API failure, verify fallback signaling\n  - Test 3: Mock timeout, verify retry and eventual fallback\n  - Test 4: Mock rate limit, verify exponential backoff\n  - _Leverage: Real API for Test 1 (manual/optional), mocks for Tests 2-4_\n  - _Requirements: 1.3, 1.4, 1.5, 5.1, 5.2, 5.3_\n  - _Prompt: Implement the task for spec llm-integration-activation, first run spec-workflow-guide to get the workflow guide then implement the task: Role: Integration Test Engineer with expertise in LLM testing and error scenarios | Task: Create integration tests for InnovationEngine with LLM following requirements 1.3-1.5 and 5.1-5.3, testing real API call and error handling | Restrictions: Limit real API calls to 1 (cost), mock all error scenarios, verify fallback signaling | _Leverage: Real LLM API (1 call), unittest.mock for errors | _Requirements: Requirements 1.3-1.5 (LLM calls), 5.1-5.3 (Error handling and fallback) | Success: Real API call works and returns valid code, error scenarios trigger correct fallbacks | Instructions: Set task to [-], mark [x] when integration tests pass_\n\n- [ ] 12. Write autonomous loop integration tests with LLM\n  - File: `tests/integration/test_autonomous_loop_llm.py`\n  - Run 10 iterations with LLM enabled (innovation_rate=0.20)\n  - Verify ~2 iterations use LLM, ~8 use Factor Graph\n  - Mock some LLM failures, verify automatic fallback works\n  - Verify all 10 iterations complete successfully\n  - _Leverage: Existing loop test patterns, LLM mocks_\n  - _Requirements: 1.1, 1.2, 5.4, 5.5_\n  - _Prompt: Implement the task for spec llm-integration-activation, first run spec-workflow-guide to get the workflow guide then implement the task: Role: Integration Test Engineer with expertise in end-to-end system testing | Task: Create autonomous loop integration tests with LLM following requirements 1.1-1.2 and 5.4-5.5, running 10 iterations and verifying LLM usage and fallback behavior | Restrictions: Must test real loop with LLM mocks, verify iteration success rate 100%, limit to 10 iterations for speed | _Leverage: Existing loop test patterns, LLM provider mocks | _Requirements: Requirements 1.1-1.2 (LLM integration), 5.4-5.5 (Reliability and fallback) | Success: 10 iterations complete with ~20% LLM usage, fallbacks work correctly, 100% success rate | Instructions: Set task to [-], mark [x] when integration test passes_\n\n## Phase 5: Documentation & Deployment (Tasks 13-14)\n\n- [ ] 13. Create user documentation\n  - File: `docs/LLM_INTEGRATION.md`\n  - Document API provider setup (API keys, environment variables)\n  - Document configuration options (provider selection, innovation rate, etc.)\n  - Document monitoring LLM usage (metrics, costs)\n  - Provide troubleshooting guide (auth errors, rate limits, fallback issues)\n  - _Leverage: Existing documentation structure_\n  - _Requirements: All_\n  - _Prompt: Implement the task for spec llm-integration-activation, first run spec-workflow-guide to get the workflow guide then implement the task: Role: Technical Writer with expertise in API documentation and configuration guides | Task: Create comprehensive LLM integration documentation covering API setup, configuration, monitoring, and troubleshooting | Restrictions: Must be clear for both novice and experienced users, include security best practices for API keys, maintain consistent structure | _Leverage: Existing documentation structure and style | _Requirements: All requirements (complete LLM setup and usage) | Success: Documentation is complete, clear, users can set up LLM integration successfully | Instructions: Set task to [-], mark [x] when documentation review passes_\n\n- [ ] 14. Create LLM setup validation script\n  - File: `scripts/validate_llm_setup.sh`\n  - Verify API keys are configured (check environment variables)\n  - Test API connectivity for configured provider\n  - Validate configuration in learning_system.yaml\n  - Provide clear diagnostic messages\n  - _Leverage: Shell scripting, curl for API testing_\n  - _Requirements: All configuration requirements_\n  - _Prompt: Implement the task for spec llm-integration-activation, first run spec-workflow-guide to get the workflow guide then implement the task: Role: DevOps Engineer with expertise in API testing and deployment validation | Task: Create validation script that checks LLM setup (API keys, connectivity, configuration) and provides clear diagnostic output | Restrictions: Must check prerequisites without exposing secrets in logs, provide actionable error messages, be safe to run in CI/CD | _Leverage: bash scripting, curl for API health checks | _Requirements: All configuration requirements | Success: Script validates LLM setup, tests API connectivity, provides clear success/failure messages | Instructions: Set task to [-], mark [x] when script validates working LLM setup_\n\n## Summary\n\n**Total Tasks**: 14\n**Estimated Time**: 2-3 days (full-time)\n\n**Phase Breakdown**:\n- Phase 1 (Core LLM): Tasks 1-4 → 6-8 hours\n- Phase 2 (Integration): Tasks 5-6 → 3-4 hours\n- Phase 3 (Prompts): Tasks 7-8 → 2-3 hours\n- Phase 4 (Testing): Tasks 9-12 → 6-8 hours\n- Phase 5 (Docs): Tasks 13-14 → 2-3 hours\n\n**Dependencies**:\n- Tasks 1-2 can run in parallel\n- Task 3 depends on Tasks 1-2 (needs LLMProvider and PromptBuilder)\n- Task 4 can run in parallel with Tasks 1-3\n- Task 5 depends on Tasks 1-4 complete\n- Task 6 can run in parallel\n- Tasks 7-8 can run in parallel\n- Tasks 9-12 depend on respective implementation tasks\n- Tasks 13-14 depend on all previous tasks\n\n**Critical Path**: 1→3→5→11→14\n",
  "fileStats": {
    "size": 18874,
    "lines": 182,
    "lastModified": "2025-10-25T00:24:47.303Z"
  },
  "comments": []
}