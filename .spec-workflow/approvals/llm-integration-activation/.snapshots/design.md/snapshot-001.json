{
  "id": "snapshot_1761344417837_t4jirhg9k",
  "approvalId": "approval_1761344417779_e49k8cpn9",
  "approvalTitle": "LLM Integration Activation - Design Document",
  "version": 1,
  "timestamp": "2025-10-24T22:20:17.837Z",
  "trigger": "initial",
  "status": "pending",
  "content": "# Design Document: LLM Integration Activation\n\n## Overview\n\nThis design **activates the InnovationEngine** in the autonomous iteration loop, enabling LLM-driven strategy innovation for 20% of iterations with automatic fallback to Factor Graph mutation on failures. The system provides robust error handling, API provider abstraction, and performance feedback to guide LLM improvements.\n\n**Architecture Pattern**: Strategy + Chain of Responsibility - Abstract API provider selection, chain validation → execution → fallback on failure.\n\n## Steering Document Alignment\n\n### Technical Standards (tech.md)\n- **API Abstraction**: Provider-agnostic interface for OpenRouter/Gemini/OpenAI\n- **Error Handling**: Timeout, retry, and fallback mechanisms for all API calls\n- **Logging**: Structured logging of all LLM calls, prompts, responses, failures\n- **Configuration**: LLM settings in `config/learning_system.yaml`\n\n### Project Structure (structure.md)\n- Extend: `artifacts/working/modules/autonomous_loop.py` - Add LLM invocation logic\n- Extend: `src/innovation/innovation_engine.py` - Add feedback loop and prompt engineering\n- New module: `src/innovation/llm_providers.py` - Abstract API provider interface\n- New module: `src/innovation/prompt_builder.py` - Construct prompts with feedback\n- Config: `config/learning_system.yaml` - Add `llm` section\n\n## Code Reuse Analysis\n\n### Existing Components to Leverage\n- **`src/innovation/innovation_engine.py`**: Already implemented (Tasks 3.1-3.3), needs activation\n- **`src/mutation/factor_graph.py`**: Fallback mutation when LLM fails\n- **`src/validation/ast_validator.py`**: Validate LLM-generated code\n- **`artifacts/data/failure_patterns.json`**: Provide failure feedback to LLM\n\n### Integration Points\n- **Autonomous Loop**: `autonomous_loop.py` iteration selection logic\n- **Validation Pipeline**: Reuse existing semantic/AST validators\n- **Champion Tracking**: Pass champion code and metrics to LLM\n- **Failure History**: Load from `failure_patterns.json` for prompts\n\n## Architecture\n\n```mermaid\ngraph TD\n    A[Iteration N] --> B{iteration % 5 == 0?}\n    B -->|Yes| C[InnovationEngine.generate]\n    B -->|No| D[FactorGraphMutation]\n\n    C --> E[PromptBuilder]\n    E -->|Champion + Feedback| F[LLM Provider]\n    F -->|OpenRouter| G[Claude 3.5]\n    F -->|Gemini| H[Gemini 2.0 Flash]\n    F -->|OpenAI| I[GPT-4o]\n\n    G --> J[Parse Response]\n    H --> J\n    I --> J\n\n    J --> K[AST Validation]\n    K -->|Valid| L[Return Strategy]\n    K -->|Invalid| M[Log Error]\n    M --> N[Fallback to Factor Graph]\n    N --> D\n\n    F -.Timeout.-> M\n    F -.API Error.-> M\n\n    style C fill:#e6f3ff\n    style M fill:#ffe6e6\n    style N fill:#ffe6cc\n```\n\n## Components and Interfaces\n\n### Component 1: LLMProviderInterface (Abstract Base)\n- **Purpose:** Abstract API provider to support multiple LLM services\n- **Interfaces:**\n  ```python\n  class LLMProviderInterface(ABC):\n      @abstractmethod\n      def generate(self, prompt: str, max_tokens: int, temperature: float) -> str:\n          \"\"\"Generate code from prompt, returns Python code\"\"\"\n\n      @abstractmethod\n      def get_cost_per_call(self) -> float:\n          \"\"\"Estimate cost in USD for typical call\"\"\"\n  ```\n- **Dependencies:** None (interface only)\n- **Implementations:** `OpenRouterProvider`, `GeminiProvider`, `OpenAIProvider`\n\n### Component 2: PromptBuilder\n- **Purpose:** Construct prompts with champion feedback, failure history, and constraints\n- **Interfaces:**\n  ```python\n  class PromptBuilder:\n      def build_modification_prompt(self, champion_code: str,\n                                    champion_metrics: dict,\n                                    failure_history: list) -> str:\n          \"\"\"Build prompt for modifying champion strategy\"\"\"\n\n      def build_creation_prompt(self, champion_approach: str,\n                                 failure_history: list) -> str:\n          \"\"\"Build prompt for creating novel strategy\"\"\"\n\n      def _extract_success_factors(self, code: str, metrics: dict) -> str:\n          \"\"\"Analyze what makes champion successful\"\"\"\n\n      def _extract_failure_patterns(self, failure_history: list) -> str:\n          \"\"\"Extract common failure modes to avoid\"\"\"\n  ```\n- **Dependencies:** `failure_patterns.json` loader\n- **Reuses:** Template prompts from `src/innovation/prompts/`\n\n### Component 3: InnovationEngineExtension (Extend Existing)\n- **Purpose:** Add feedback loop and provider selection to existing InnovationEngine\n- **Interfaces:**\n  ```python\n  # Extend existing InnovationEngine in src/innovation/innovation_engine.py\n  class InnovationEngine:\n      def __init__(self, provider: LLMProviderInterface, prompt_builder: PromptBuilder):\n          self.provider = provider\n          self.prompt_builder = prompt_builder\n\n      def generate_with_feedback(self, champion_code: str,\n                                  champion_metrics: dict,\n                                  failure_history: list,\n                                  directive: str) -> str:\n          \"\"\"Generate strategy with feedback context\"\"\"\n          if directive == \"modify\":\n              prompt = self.prompt_builder.build_modification_prompt(...)\n          else:  # \"create\"\n              prompt = self.prompt_builder.build_creation_prompt(...)\n\n          response = self.provider.generate(prompt, max_tokens=2000)\n          code = self._extract_code(response)\n          return code\n  ```\n- **Dependencies:** LLMProvider, PromptBuilder\n- **Reuses:** Existing InnovationEngine base implementation\n\n### Component 4: AutonomousLoopIntegration\n- **Purpose:** Integrate LLM calls into iteration loop with fallback logic\n- **Interfaces:**\n  ```python\n  # Extension to autonomous_loop.py\n  def run_iteration(iteration: int, config: dict):\n      use_llm = (iteration % 5 == 0) and config[\"llm\"][\"enabled\"]\n\n      if use_llm:\n          try:\n              code = innovation_engine.generate_with_feedback(\n                  champion_code=champion.code,\n                  champion_metrics=champion.metrics,\n                  failure_history=load_failure_history(),\n                  directive=\"modify\" if random.random() < 0.7 else \"create\"\n              )\n\n              # Validate\n              if not ast_validator.validate(code):\n                  raise ValidationError(\"Invalid syntax\")\n\n          except (TimeoutError, APIError, ValidationError) as e:\n              log_error(f\"LLM failed: {e}, falling back to Factor Graph\")\n              code = factor_graph_mutation(champion.code)\n      else:\n          code = factor_graph_mutation(champion.code)\n\n      return execute_strategy(code)\n  ```\n- **Dependencies:** InnovationEngine, FactorGraphMutation, validators\n- **Reuses:** Existing iteration loop structure\n\n## Data Models\n\n### LLMConfig\n```python\n@dataclass\nclass LLMConfig:\n    enabled: bool = False\n    provider: str = \"openrouter\"  # openrouter, gemini, openai\n    api_key: str = \"\"\n    model_name: str = \"\"\n    innovation_rate: float = 0.20  # 20% of iterations\n    timeout_seconds: int = 60\n    max_tokens: int = 2000\n    temperature: float = 0.7\n    fallback_to_factor_graph: bool = True\n\n    @classmethod\n    def from_yaml(cls, path: str) -> 'LLMConfig':\n        \"\"\"Load from config/learning_system.yaml\"\"\"\n```\n\n### LLMCallMetadata\n```python\n@dataclass\nclass LLMCallMetadata:\n    timestamp: str\n    iteration: int\n    provider: str\n    model: str\n    prompt_length: int\n    response_length: int\n    latency_seconds: float\n    success: bool\n    error_message: Optional[str] = None\n    validation_passed: bool = False\n    cost_usd: float = 0.0\n```\n\n## Error Handling\n\n### Error Scenarios\n\n1. **API Timeout (>60s)**\n   - **Handling:** Raise `TimeoutError`, log, fallback to Factor Graph\n   - **User Impact:** Iteration completes using mutation instead\n   - **Metrics:** Increment `llm_timeout_total`\n\n2. **Invalid API Key / Auth Failure**\n   - **Handling:** Log critical error, disable LLM for remaining iterations\n   - **User Impact:** Warning: \"LLM disabled due to auth failure\"\n   - **Metrics:** Set `llm_enabled=0`, increment `llm_auth_failure_total`\n\n3. **Rate Limit Exceeded**\n   - **Handling:** Exponential backoff (1s, 2s, 4s), max 3 retries\n   - **User Impact:** Slight delay, fallback if retries exhausted\n   - **Metrics:** Increment `llm_rate_limit_total`\n\n4. **Invalid Code Response (Syntax Error)**\n   - **Handling:** Log code snippet, increment `llm_validation_failure_total`, fallback\n   - **User Impact:** Strategy generated via Factor Graph\n   - **Metrics:** Record validation failure reason\n\n5. **Network Error**\n   - **Handling:** Retry once after 2s, fallback if fails\n   - **User Impact:** Slight delay or Factor Graph fallback\n   - **Metrics:** Increment `llm_network_error_total`\n\n6. **High Failure Rate (>50% over 10 iterations)**\n   - **Handling:** Log warning, reduce innovation_rate to 0.1 automatically\n   - **User Impact:** Fewer LLM calls to conserve API quota\n   - **Metrics:** Set `llm_innovation_rate=0.1`\n\n## Testing Strategy\n\n### Unit Testing\n\n**LLMProviders Tests**:\n- Mock API responses for each provider\n- Test timeout enforcement\n- Test retry logic on rate limits\n- **Coverage:** >85%\n\n**PromptBuilder Tests**:\n- Test modification prompt generation\n- Test creation prompt generation\n- Test failure pattern extraction\n- **Coverage:** >90%\n\n### Integration Testing\n\n**LLM API Integration**:\n1. Call real API with test prompt (1 call, manual)\n2. Verify response parsing\n3. Verify code extraction\n\n**Autonomous Loop Integration**:\n1. Run 10 iterations with LLM enabled\n2. Verify 2 iterations use LLM (20%)\n3. Verify fallback works when API mocked to fail\n4. Verify all iterations complete successfully\n\n### End-to-End Testing\n\n**100-Generation LLM Test** (Task 3.5):\n1. Enable LLM with `innovation_rate=0.20`\n2. Run 100 iterations\n3. Verify ~20 use LLM, ~80 use Factor Graph\n4. Verify >80% overall success rate\n5. Analyze LLM-generated vs mutated strategy performance\n\n## Configuration Example\n\n### config/learning_system.yaml\n```yaml\nllm:\n  enabled: true\n  provider: \"openrouter\"  # openrouter, gemini, openai\n  innovation_rate: 0.20   # 20% of iterations\n\n  openrouter:\n    api_key: \"${OPENROUTER_API_KEY}\"\n    model: \"anthropic/claude-3.5-sonnet\"\n    base_url: \"https://openrouter.ai/api/v1\"\n\n  gemini:\n    api_key: \"${GOOGLE_API_KEY}\"\n    model: \"gemini-2.0-flash-thinking-exp\"\n\n  openai:\n    api_key: \"${OPENAI_API_KEY}\"\n    model: \"gpt-4o\"\n\n  generation:\n    timeout_seconds: 60\n    max_tokens: 2000\n    temperature: 0.7\n    max_retries: 3\n\n  fallback:\n    enabled: true\n    auto_reduce_rate_on_failures: true\n    failure_threshold: 0.5  # 50% failures\n    reduced_innovation_rate: 0.1\n\n  prompts:\n    template_dir: \"src/innovation/prompts\"\n    few_shot_examples: 2\n```\n\n## Prompt Engineering Templates\n\n### Modification Prompt Template\n```\nYou are a quantitative trading strategy developer. Your task is to modify an existing strategy to improve its performance.\n\n**Current Champion Strategy:**\n```python\n{champion_code}\n```\n\n**Performance Metrics:**\n- Sharpe Ratio: {sharpe}\n- Max Drawdown: {max_drawdown}\n- Win Rate: {win_rate}\n\n**Success Factors to Preserve:**\n{success_factors}\n\n**Common Failures to Avoid:**\n{failure_patterns}\n\n**Constraints:**\n1. Use FinLab API: data.get('price:收盤價'), data.get('fundamental_features:ROE')\n2. Liquidity requirement: trading_value_mean > 150_000_000\n3. Rebalancing: Weekly Friday (W-FRI)\n4. Return function signature: def strategy(data) -> pd.Series\n\n**Modification Directive:**\nModify this strategy to {target_metric_improvement}. Consider:\n- Adjusting parameters (periods, thresholds)\n- Adding/removing factors\n- Refining entry/exit conditions\n\nReturn ONLY the complete modified Python function, no explanations.\n```\n\n## Performance Considerations\n\n- **LLM Call Latency:** ~5-15s per API call (acceptable for 20% of iterations)\n- **Cost per Iteration:** ~$0.05-0.10 (using cost-effective models)\n- **Fallback Overhead:** <100ms (minimal impact)\n- **Prompt Construction:** <50ms (negligible)\n\n## Future Enhancements (Out of Scope)\n\n- **Structured Innovation:** YAML/JSON specs instead of full code (Phase 2a)\n- **Multi-shot Prompting:** Multiple LLM calls with refinement\n- **Cost Optimization:** Batch API calls, use caching\n- **A/B Testing:** Compare LLM vs Factor Graph performance\n",
  "fileStats": {
    "size": 12290,
    "lines": 369,
    "lastModified": "2025-10-24T22:20:12.564Z"
  },
  "comments": []
}