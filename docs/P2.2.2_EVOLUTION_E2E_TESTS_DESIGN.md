# P2.2.2: Strategy Evolution E2E Tests - TDD Implementation Guide

**Status**: RED Phase (Tests Written, Failing as Expected)
**Created**: 2025-11-15
**Next Phase**: GREEN (Implement fixes to make tests pass)

---

## Overview

This document describes the TDD implementation of P2.2.2 - Strategy Evolution E2E Tests, which test the complete LLM strategy evolution workflow end-to-end.

### Task Requirements

From `.spec-workflow/steering/tasks.md` - **P2.2.2**:
- Create `tests/e2e/test_evolution.py` with complete evolution workflow tests
- Verify `final_sharpe >= target_sharpe`
- Verify `execution_time < 5.0 seconds`
- Test Gate 5 (OOS tolerance ±20%)

---

## Architecture Analysis

### Evolution Workflow Components

The LLM strategy evolution system consists of the following components:

#### 1. **LearningLoop** (`src/learning/learning_loop.py`)
- **Role**: Main orchestrator for the evolution process
- **Responsibilities**:
  - Initialize all Phase 1-5 components
  - Run iteration loop with progress tracking
  - Handle CTRL+C interruption gracefully
  - Support resumption from last iteration
  - Generate summary report

#### 2. **IterationExecutor** (`src/learning/iteration_executor.py`)
- **Role**: Executes single iteration with 10-step process
- **10-Step Process**:
  1. Load recent history (last N iterations)
  2. Generate feedback from history
  3. Decide LLM or Factor Graph (innovation_rate %)
  4. Generate strategy (LLM or Factor Graph)
  5. Execute strategy (BacktestExecutor)
  6. Extract metrics (MetricsExtractor)
  7. Classify success (ErrorClassifier: LEVEL_0-3)
  8. Update champion if better (ChampionTracker)
  9. Create IterationRecord
  10. Return record

#### 3. **ChampionTracker** (`src/learning/champion_tracker.py`)
- **Role**: Track best-performing strategy
- **Responsibilities**:
  - Maintain current champion
  - Update champion when better strategy found
  - Persist champion to JSON file
  - Support both LLM and Factor Graph strategies

#### 4. **FeedbackGenerator** (`src/learning/feedback_generator.py`)
- **Role**: Generate feedback from iteration history
- **Responsibilities**:
  - Analyze recent iteration results
  - Generate constructive feedback for LLM
  - Identify performance patterns

#### 5. **LLMClient** (`src/learning/llm_client.py`)
- **Role**: Handle LLM communication
- **Responsibilities**:
  - Generate strategy code via LLM
  - Handle retries and fallbacks
  - **Mockable** for deterministic testing

---

## Test Design

### Test Structure

Created `tests/e2e/test_evolution.py` with 5 comprehensive tests:

#### 1. `test_evolution_workflow_improves_performance`
**Purpose**: Verify evolution improves strategy performance over iterations

**Given**:
- Configured learning loop with market data
- Mock LLM returning deterministic strategies
- Backtest executor returning improving Sharpe ratios

**When**:
- Evolution runs for 3 iterations

**Then**:
- Final champion should have Sharpe >= min_sharpe_ratio (0.5)
- Execution time should be < 5.0 seconds
- Champion should exist and be valid

**Key Assertions**:
```python
assert champion_sharpe >= min_target_sharpe
assert elapsed_time < max_time
```

#### 2. `test_evolution_respects_oos_tolerance`
**Purpose**: Test Gate 5 - OOS performance within ±20% of IS performance

**Given**:
- Market data split into In-Sample (67%) and Out-of-Sample (33%)
- Mock backtest with different IS/OOS Sharpe ratios
- IS Sharpe = 1.0, OOS Sharpe = 0.85 (15% degradation - acceptable)

**When**:
- Evolution runs on IS data
- Champion tested on OOS data

**Then**:
- OOS performance degradation should be <= 20%
- This validates Gate 5 (OOS tolerance)

**Key Assertions**:
```python
performance_degradation = abs(oos_sharpe - is_sharpe) / is_sharpe
assert performance_degradation <= oos_tolerance  # 0.20 = ±20%
```

#### 3. `test_evolution_handles_llm_failures_gracefully`
**Purpose**: Verify system fallback to Factor Graph when LLM fails

**Given**:
- Mock LLM that fails 40% of the time (calls 2, 4 out of 5)
- Factor Graph available as fallback

**When**:
- Evolution runs for 5 iterations with intermittent LLM failures

**Then**:
- All iterations should complete (no crashes)
- Should have mix of LLM and Factor Graph generations
- System should handle failures gracefully

**Key Assertions**:
```python
assert len(all_records) == config.max_iterations
assert llm_count > 0  # Some LLM successes
assert fg_count > 0   # Some Factor Graph fallbacks
```

#### 4. `test_evolution_tracks_performance_improvement`
**Purpose**: Verify champion tracking shows monotonic improvement

**Given**:
- Mock backtest with progressively better strategies
- Sharpe sequence: 0.6 → 0.8 → 0.9 → 1.2

**When**:
- Evolution runs for 4 iterations

**Then**:
- Final champion should be the best strategy (Sharpe = 1.2)
- Should see performance improvement over time
- Champion should track best performance

**Key Assertions**:
```python
assert final_sharpe >= best_expected_sharpe * 0.95
assert max(sharpe_history) > min(sharpe_history)
```

#### 5. `test_evolution_execution_time_under_limit`
**Purpose**: Verify execution time constraint (< 5.0 seconds)

**Given**:
- Lightweight config with 3 iterations
- Factor Graph only (faster than LLM)
- Fast mock responses

**When**:
- Evolution runs complete workflow

**Then**:
- Total execution time should be < 5.0 seconds
- All iterations should complete
- Performance requirement met

**Key Assertions**:
```python
assert elapsed_time < max_time  # 5.0 seconds
assert len(all_records) == config.max_iterations
```

---

## Mocking Strategy

### Critical Components to Mock

#### 1. **LLM Client**
```python
with patch('src.learning.llm_client.LLMClient') as MockLLMClient:
    mock_llm_instance = MockLLMClient.return_value
    mock_llm_instance.is_enabled.return_value = True

    mock_engine = Mock()
    mock_engine.generate_innovation.return_value = strategy_code
    mock_llm_instance.get_engine.return_value = mock_engine
```

**Why**:
- Avoid actual LLM API calls (expensive, slow, non-deterministic)
- Provide deterministic strategy code for testing
- Control failure scenarios for error recovery tests

#### 2. **Backtest Executor**
```python
with patch('src.backtest.executor.BacktestExecutor.execute') as mock_execute:
    from src.backtest.executor import ExecutionResult
    mock_execute.return_value = ExecutionResult(
        success=True,
        sharpe_ratio=1.2,
        total_return=0.15,
        max_drawdown=-0.20,
        execution_time=0.5
    )
```

**Why**:
- Avoid actual finlab backtest execution (slow, requires real data)
- Control performance metrics for testing different scenarios
- Ensure fast test execution (< 5 seconds)

#### 3. **Finlab Data/Sim**
```python
with patch('src.learning.learning_loop.data') as mock_data, \
     patch('src.learning.learning_loop.sim') as mock_sim:
    # Mock data import
```

**Why**:
- Finlab requires authentication and real market data
- Use test fixture `market_data` instead
- Avoid external dependencies in tests

---

## Test Fixtures Used

### From `tests/e2e/conftest.py`

#### 1. **market_data**
- **Type**: `pd.DataFrame`
- **Structure**: 756 trading days (~3 years), 100 stocks
- **Columns**: `{stock_id}_price`, `{stock_id}_volume`, `{stock_id}_returns`
- **Statistical Properties**:
  - Mean daily return: ~0.04% (10% annualized)
  - Daily volatility: ~1.2% (19% annualized)
  - Realistic Sharpe ratio: ~0.5
  - Cross-sectional correlation: ~0.3

#### 2. **test_environment**
- **Type**: `Dict[str, Any]`
- **Key Settings**:
  - `mock_llm: True` - Mock LLM calls for deterministic tests
  - `mock_backtest: False` - Use real backtest engine for accuracy
  - `timeout_seconds: 60` - Max execution time per test
  - `temp_strategy_dir`, `temp_results_dir` - Temporary paths

#### 3. **validation_thresholds**
- **Type**: `Dict[str, Any]`
- **Key Thresholds**:
  - `min_sharpe_ratio: 0.5` - Minimum acceptable Sharpe
  - `max_execution_time: 5.0` - 5 seconds max for evolution
  - `oos_tolerance: 0.20` - ±20% OOS tolerance (Gate 5)
  - `min_total_return: 0.10` - 10% minimum return

---

## TDD Phases

### Phase 1: RED (Current Status)

**Status**: ✅ Complete

Tests written and failing with expected errors. All 5 tests fail due to:
1. Mocking issues (finlab imports)
2. Missing integration between components
3. Evolution workflow not fully implemented

**Test Execution**:
```bash
$ python3 -m pytest tests/e2e/test_evolution.py -v
============================= test session starts ==============================
...
FAILED tests/e2e/test_evolution.py::TestStrategyEvolution::test_evolution_workflow_improves_performance
FAILED tests/e2e/test_evolution.py::TestStrategyEvolution::test_evolution_respects_oos_tolerance
FAILED tests/e2e/test_evolution.py::TestStrategyEvolution::test_evolution_handles_llm_failures_gracefully
FAILED tests/e2e/test_evolution.py::TestStrategyEvolution::test_evolution_tracks_performance_improvement
FAILED tests/e2e/test_evolution.py::TestStrategyEvolution::test_evolution_execution_time_under_limit
=========================== 5 failed in 3.56s ==============================
```

### Phase 2: GREEN (Next Step)

**Objective**: Make tests pass by implementing necessary fixes

**Required Fixes**:

1. **Fix Mocking Issues**
   - Correct finlab import paths
   - Mock `_initialize_finlab()` properly
   - Handle data/sim initialization

2. **Integration Fixes**
   - Ensure BacktestExecutor properly mocked
   - Verify ExecutionResult structure matches expectations
   - Test champion tracking logic

3. **Error Recovery**
   - Implement LLM → Factor Graph fallback
   - Handle empty/invalid LLM responses
   - Graceful degradation logic

4. **OOS Validation**
   - Implement OOS performance calculation
   - Add OOS tolerance checking
   - Gate 5 validation logic

**Implementation Steps**:
1. Run tests and identify first failure
2. Fix the minimal code to pass that test
3. Refactor if needed (keep tests passing)
4. Move to next failing test
5. Repeat until all tests pass

### Phase 3: REFACTOR (Future)

**Objective**: Clean up code while keeping tests green

**Potential Refactorings**:
1. Extract common test setup into helper methods
2. Simplify mocking strategy
3. Add more descriptive error messages
4. Optimize test execution time
5. Add edge case tests

---

## Configuration Management

### Test Configuration Helper

Created `_create_test_config()` helper method:

```python
def _create_test_config(
    tmpdir: str,
    max_iterations: int,
    innovation_rate: int,
    test_environment: Dict[str, Any]
) -> LearningConfig:
    """Create test configuration for evolution."""
    return LearningConfig(
        max_iterations=max_iterations,
        innovation_rate=innovation_rate,
        # ... other settings
        log_level="ERROR",  # Reduce noise
        log_to_console=False  # Silent tests
    )
```

**Benefits**:
- Centralized test config creation
- Consistent settings across tests
- Easy to modify for different test scenarios
- Uses temporary directories for isolation

---

## Mock Strategy Sequences

### Strategy Evolution Simulation

Created `_generate_mock_strategy_sequence()` to simulate improving strategies:

```python
def _generate_mock_strategy_sequence(self):
    """Generate sequence of mock strategies with improving performance."""
    strategies = [
        # Strategy 1: Basic momentum
        "momentum = close.pct_change(20)",

        # Strategy 2: Momentum + Volume
        "momentum + volume_trend",

        # Strategy 3: Multi-factor with risk management
        "momentum + volatility filter + volume confirmation"
    ]
    for strategy in strategies:
        yield strategy
```

**Purpose**:
- Simulate realistic evolution progression
- Test performance improvement tracking
- Verify champion selection logic

---

## Performance Requirements

### Execution Time Budget

**Total**: < 5.0 seconds for complete evolution workflow

**Breakdown** (for 3 iterations):
- Initialization: < 0.5s
- Per iteration: < 1.5s
  - Load history: < 0.1s
  - Generate feedback: < 0.1s
  - Generate strategy: < 0.2s (mocked)
  - Execute backtest: < 0.5s (mocked)
  - Extract metrics: < 0.1s
  - Classify/update: < 0.1s
  - Save record: < 0.1s

**Optimization Strategies**:
- Mock LLM calls (no network)
- Mock backtest execution (no computation)
- Minimal iterations (3-5 for tests)
- Silent logging (`log_level="ERROR"`)
- Use Factor Graph only when possible (faster)

---

## Validation Thresholds

### Gate 5: OOS Tolerance

**Specification**: ±20% performance degradation acceptable

**Implementation**:
```python
oos_tolerance = validation_thresholds['oos_tolerance']  # 0.20
performance_degradation = abs(oos_sharpe - is_sharpe) / is_sharpe

assert performance_degradation <= oos_tolerance, \
    f"OOS degradation {performance_degradation:.1%} > {oos_tolerance:.1%} tolerance"
```

**Test Cases**:
- ✅ IS=1.0, OOS=0.85: 15% degradation (acceptable)
- ✅ IS=1.0, OOS=0.80: 20% degradation (borderline acceptable)
- ❌ IS=1.0, OOS=0.75: 25% degradation (unacceptable)

---

## Next Steps

### GREEN Phase Implementation

1. **Run Tests and Fix Mocking Issues**
   ```bash
   python3 -m pytest tests/e2e/test_evolution.py::TestStrategyEvolution::test_evolution_workflow_improves_performance -v
   ```

2. **Implement Minimal Fixes**
   - Fix finlab import mocking
   - Ensure BacktestExecutor mocking works
   - Verify ExecutionResult structure

3. **Test Each Component**
   - LearningLoop initialization
   - IterationExecutor.execute_iteration()
   - ChampionTracker.update_champion()
   - FeedbackGenerator.generate_feedback()

4. **Verify Integration**
   - Run all 5 tests
   - Check execution time
   - Verify all assertions pass

5. **Document Results**
   - Update this document with GREEN status
   - Record any issues found
   - Note any deviations from design

---

## Success Criteria

### Definition of Done

- [x] All 5 E2E tests written (RED phase)
- [ ] All 5 E2E tests passing (GREEN phase)
- [ ] Execution time < 5.0 seconds
- [ ] Gate 5 (OOS tolerance ±20%) validated
- [ ] Champion tracking verified
- [ ] Error recovery tested
- [ ] Performance improvement tracked
- [ ] Code reviewed and refactored (REFACTOR phase)

### Quality Gates

1. **Code Coverage**: E2E tests cover complete workflow
2. **Performance**: Tests complete in < 5 seconds
3. **Reliability**: Tests are deterministic (no flaky tests)
4. **Maintainability**: Tests are well-documented and easy to understand
5. **Correctness**: Tests validate actual business requirements

---

## References

### Source Files
- `tests/e2e/test_evolution.py` - E2E evolution tests
- `tests/e2e/conftest.py` - E2E test fixtures
- `tests/e2e/test_infrastructure.py` - Infrastructure validation tests
- `src/learning/learning_loop.py` - Main orchestrator
- `src/learning/iteration_executor.py` - Iteration execution
- `src/learning/champion_tracker.py` - Champion tracking
- `src/learning/learning_config.py` - Configuration management

### Related Documentation
- `.spec-workflow/steering/tasks.md` - P2.2 E2E Testing Tasks
- `.spec-workflow/steering/product.md` - Product requirements
- `.spec-workflow/steering/tech.md` - Technical architecture
- `docs/TDD_IMPLEMENTATION_ROADMAP.md` - TDD methodology guide

---

**Document Version**: 1.0
**Last Updated**: 2025-11-15
**Author**: Claude Code
**Status**: RED Phase Complete, awaiting GREEN Phase implementation
