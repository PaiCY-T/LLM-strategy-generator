"""
Test Task 8 Implementation: Validation Report Generator
Tests the comprehensive validation report generation functionality
"""

import sys
from pathlib import Path
import json
import tempfile

# Add project root to Python path
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from src.validation.validation_report import ValidationReportGenerator


def test_report_generator_initialization():
    """Test ValidationReportGenerator can be initialized."""
    print("=" * 80)
    print("TEST 1: ValidationReportGenerator Initialization")
    print("=" * 80)

    try:
        # Test with default project name
        generator1 = ValidationReportGenerator()
        assert generator1.project_name == "Trading Strategy Validation", \
            "âŒ Default project name not set"
        assert len(generator1.reports) == 0, "âŒ Reports should be empty initially"
        print("âœ… ValidationReportGenerator initialized with default name")

        # Test with custom project name
        generator2 = ValidationReportGenerator(project_name="Custom Strategy Test")
        assert generator2.project_name == "Custom Strategy Test", \
            "âŒ Custom project name not set"
        print("âœ… ValidationReportGenerator initialized with custom name")

        print("\nâœ… ValidationReportGenerator Initialization Test PASSED\n")
        return True

    except Exception as e:
        print(f"âŒ Initialization failed: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_add_strategy_validation():
    """Test adding validation results for a strategy."""
    print("=" * 80)
    print("TEST 2: Add Strategy Validation")
    print("=" * 80)

    try:
        generator = ValidationReportGenerator()

        # Create sample validation results
        out_of_sample_results = {
            'validation_passed': True,
            'consistency': 0.85,
            'degradation_ratio': 0.92,
            'periods_tested': ['training', 'validation', 'test']
        }

        walk_forward_results = {
            'validation_passed': True,
            'mean_sharpe': 1.2,
            'stability_score': 0.35,
            'windows_tested': 4
        }

        baseline_results = {
            'validation_passed': True,
            'strategy_sharpe': 1.5,
            'best_improvement': 0.8
        }

        bootstrap_results = {
            'validation_passed': True,
            'sharpe_ratio': 1.5,
            'ci_lower': 1.1,
            'ci_upper': 1.9,
            'validation_reason': 'CI excludes zero and lower bound â‰¥ 0.5'
        }

        bonferroni_results = {
            'validation_passed': True,
            'sharpe_ratio': 1.5,
            'significance_threshold': 0.5,
            'adjusted_alpha': 0.0025
        }

        # Add strategy validation
        generator.add_strategy_validation(
            strategy_name="TestStrategy_001",
            iteration_num=1,
            out_of_sample_results=out_of_sample_results,
            walk_forward_results=walk_forward_results,
            baseline_results=baseline_results,
            bootstrap_results=bootstrap_results,
            bonferroni_results=bonferroni_results
        )

        assert len(generator.reports) == 1, "âŒ Report not added"
        print("âœ… Strategy validation added successfully")

        # Verify report structure
        report = generator.reports[0]
        assert report['strategy_name'] == "TestStrategy_001", "âŒ Strategy name incorrect"
        assert report['iteration_num'] == 1, "âŒ Iteration number incorrect"
        assert 'timestamp' in report, "âŒ Timestamp missing"
        assert 'validations' in report, "âŒ Validations missing"
        assert 'overall_status' in report, "âŒ Overall status missing"
        print("âœ… Report structure correct")

        # Verify overall status
        overall = report['overall_status']
        assert overall['overall_passed'] == True, "âŒ Should pass all validations"
        assert overall['total_validations'] == 5, f"âŒ Expected 5 validations, got {overall['total_validations']}"
        assert overall['passed_count'] == 5, f"âŒ Expected all 5 to pass, got {overall['passed_count']}"
        assert overall['failed_count'] == 0, f"âŒ Expected 0 failures, got {overall['failed_count']}"
        print(f"âœ… Overall status calculated correctly: {overall['passed_count']}/{overall['total_validations']} passed")

        print("\nâœ… Add Strategy Validation Test PASSED\n")
        return True

    except Exception as e:
        print(f"âŒ Add strategy validation test failed: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_summary_statistics():
    """Test summary statistics generation."""
    print("=" * 80)
    print("TEST 3: Summary Statistics")
    print("=" * 80)

    try:
        generator = ValidationReportGenerator()

        # Add multiple strategies with different pass/fail statuses
        # Strategy 1: All pass
        generator.add_strategy_validation(
            strategy_name="Strategy_Pass",
            iteration_num=1,
            out_of_sample_results={'validation_passed': True},
            walk_forward_results={'validation_passed': True},
            baseline_results={'validation_passed': True}
        )

        # Strategy 2: Some fail
        generator.add_strategy_validation(
            strategy_name="Strategy_Mixed",
            iteration_num=2,
            out_of_sample_results={'validation_passed': True},
            walk_forward_results={'validation_passed': False},
            baseline_results={'validation_passed': True}
        )

        # Strategy 3: All fail
        generator.add_strategy_validation(
            strategy_name="Strategy_Fail",
            iteration_num=3,
            out_of_sample_results={'validation_passed': False},
            walk_forward_results={'validation_passed': False}
        )

        # Generate summary
        summary = generator.generate_summary_statistics()

        assert summary['total_strategies'] == 3, f"âŒ Expected 3 strategies, got {summary['total_strategies']}"
        print(f"âœ… Total strategies: {summary['total_strategies']}")

        assert summary['strategies_passed'] == 1, f"âŒ Expected 1 passed, got {summary['strategies_passed']}"
        print(f"âœ… Strategies passed: {summary['strategies_passed']}")

        assert summary['strategies_failed'] == 2, f"âŒ Expected 2 failed, got {summary['strategies_failed']}"
        print(f"âœ… Strategies failed: {summary['strategies_failed']}")

        assert 'validation_breakdown' in summary, "âŒ Validation breakdown missing"
        print(f"âœ… Validation breakdown generated")

        # Check breakdown
        breakdown = summary['validation_breakdown']
        if 'out_of_sample' in breakdown:
            oos = breakdown['out_of_sample']
            print(f"   Out-of-sample: {oos['passed']}/{oos['total']} ({oos['pass_rate']:.1%})")

        print("\nâœ… Summary Statistics Test PASSED\n")
        return True

    except Exception as e:
        print(f"âŒ Summary statistics test failed: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_json_export():
    """Test JSON export functionality."""
    print("=" * 80)
    print("TEST 4: JSON Export")
    print("=" * 80)

    try:
        generator = ValidationReportGenerator(project_name="JSON Test")

        # Add a simple strategy
        generator.add_strategy_validation(
            strategy_name="JSONTest_Strategy",
            iteration_num=1,
            out_of_sample_results={'validation_passed': True, 'consistency': 0.8}
        )

        # Generate JSON
        json_output = generator.to_json()

        # Verify it's valid JSON
        parsed = json.loads(json_output)
        assert 'project_name' in parsed, "âŒ Project name missing from JSON"
        assert parsed['project_name'] == "JSON Test", "âŒ Project name incorrect in JSON"
        assert 'summary' in parsed, "âŒ Summary missing from JSON"
        assert 'reports' in parsed, "âŒ Reports missing from JSON"
        print("âœ… JSON output structure valid")

        # Test file save
        with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.json') as f:
            temp_path = Path(f.name)

        try:
            generator.save_json(temp_path)
            assert temp_path.exists(), "âŒ JSON file not created"
            print(f"âœ… JSON file saved successfully")

            # Verify content
            with open(temp_path, 'r') as f:
                loaded = json.load(f)
            assert loaded['project_name'] == "JSON Test", "âŒ Loaded JSON content incorrect"
            print("âœ… JSON file content verified")

        finally:
            if temp_path.exists():
                temp_path.unlink()

        print("\nâœ… JSON Export Test PASSED\n")
        return True

    except Exception as e:
        print(f"âŒ JSON export test failed: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_html_generation():
    """Test HTML report generation."""
    print("=" * 80)
    print("TEST 5: HTML Generation")
    print("=" * 80)

    try:
        generator = ValidationReportGenerator(project_name="HTML Test")

        # Add a comprehensive strategy validation
        generator.add_strategy_validation(
            strategy_name="HTMLTest_Strategy",
            iteration_num=1,
            out_of_sample_results={
                'validation_passed': True,
                'consistency': 0.85,
                'degradation_ratio': 0.92,
                'periods_tested': ['training', 'validation', 'test']
            },
            walk_forward_results={
                'validation_passed': True,
                'mean_sharpe': 1.2,
                'stability_score': 0.35,
                'windows_tested': 4
            },
            baseline_results={
                'validation_passed': True,
                'strategy_sharpe': 1.5,
                'best_improvement': 0.8
            }
        )

        # Generate HTML
        html_output = generator.to_html()

        # Verify HTML structure
        assert '<!DOCTYPE html>' in html_output, "âŒ Not valid HTML"
        assert 'HTML Test' in html_output, "âŒ Project name not in HTML"
        assert 'HTMLTest_Strategy' in html_output, "âŒ Strategy name not in HTML"
        assert 'PASSED' in html_output, "âŒ Status not in HTML"
        print("âœ… HTML output structure valid")

        # Test file save
        with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.html') as f:
            temp_path = Path(f.name)

        try:
            generator.save_html(temp_path)
            assert temp_path.exists(), "âŒ HTML file not created"
            print(f"âœ… HTML file saved successfully")

            # Verify it's readable
            with open(temp_path, 'r', encoding='utf-8') as f:
                content = f.read()
            assert 'HTML Test' in content, "âŒ HTML content incorrect"
            print(f"âœ… HTML file content verified ({len(content)} characters)")

        finally:
            if temp_path.exists():
                temp_path.unlink()

        print("\nâœ… HTML Generation Test PASSED\n")
        return True

    except Exception as e:
        print(f"âŒ HTML generation test failed: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_filter_by_status():
    """Test filtering strategies by pass/fail status."""
    print("=" * 80)
    print("TEST 6: Filter by Status")
    print("=" * 80)

    try:
        generator = ValidationReportGenerator()

        # Add passed strategy
        generator.add_strategy_validation(
            strategy_name="PassedStrategy",
            iteration_num=1,
            out_of_sample_results={'validation_passed': True}
        )

        # Add failed strategy
        generator.add_strategy_validation(
            strategy_name="FailedStrategy",
            iteration_num=2,
            out_of_sample_results={'validation_passed': False}
        )

        # Get passed strategies
        passed_strategies = generator.get_strategies_by_status(passed=True)
        assert len(passed_strategies) == 1, f"âŒ Expected 1 passed strategy, got {len(passed_strategies)}"
        assert passed_strategies[0]['strategy_name'] == "PassedStrategy", "âŒ Wrong passed strategy"
        print(f"âœ… Found {len(passed_strategies)} passed strategy")

        # Get failed strategies
        failed_strategies = generator.get_strategies_by_status(passed=False)
        assert len(failed_strategies) == 1, f"âŒ Expected 1 failed strategy, got {len(failed_strategies)}"
        assert failed_strategies[0]['strategy_name'] == "FailedStrategy", "âŒ Wrong failed strategy"
        print(f"âœ… Found {len(failed_strategies)} failed strategy")

        print("\nâœ… Filter by Status Test PASSED\n")
        return True

    except Exception as e:
        print(f"âŒ Filter by status test failed: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_clear_reports():
    """Test clearing all reports."""
    print("=" * 80)
    print("TEST 7: Clear Reports")
    print("=" * 80)

    try:
        generator = ValidationReportGenerator()

        # Add some strategies
        generator.add_strategy_validation(
            strategy_name="Strategy1",
            iteration_num=1,
            out_of_sample_results={'validation_passed': True}
        )
        generator.add_strategy_validation(
            strategy_name="Strategy2",
            iteration_num=2,
            out_of_sample_results={'validation_passed': False}
        )

        assert len(generator.reports) == 2, "âŒ Reports not added"
        print(f"âœ… Added 2 strategies")

        # Clear reports
        generator.clear()

        assert len(generator.reports) == 0, "âŒ Reports not cleared"
        print(f"âœ… Reports cleared successfully")

        print("\nâœ… Clear Reports Test PASSED\n")
        return True

    except Exception as e:
        print(f"âŒ Clear reports test failed: {e}")
        import traceback
        traceback.print_exc()
        return False


def main():
    """Run all tests."""
    print("\n")
    print("=" * 80)
    print(" Task 8 Implementation Validation Test Suite")
    print("=" * 80)
    print("\n")

    tests = [
        test_report_generator_initialization,
        test_add_strategy_validation,
        test_summary_statistics,
        test_json_export,
        test_html_generation,
        test_filter_by_status,
        test_clear_reports,
    ]

    passed = 0
    failed = 0

    for test in tests:
        try:
            if test():
                passed += 1
            else:
                failed += 1
        except Exception as e:
            print(f"âŒ Test {test.__name__} crashed: {e}")
            import traceback
            traceback.print_exc()
            failed += 1

    print("=" * 80)
    if failed == 0:
        print(" âœ… ALL TESTS PASSED - Task 8 Implementation Complete")
    else:
        print(f" âš ï¸  SOME TESTS FAILED - {passed} passed, {failed} failed")
    print("=" * 80)
    print("\n")
    print("Summary:")
    print("  âœ… Task 8: Validation report generator implemented")
    print("     - ValidationReportGenerator class")
    print("     - Aggregates all validation results (Tasks 3-7)")
    print("     - JSON export with summary statistics")
    print("     - HTML report with comprehensive visualizations")
    print("     - Strategy filtering by pass/fail status")
    print("\n")
    print("Phase 2 Validation Framework Integration:")
    print("  âœ… Wave 1 (P0 Critical): Tasks 0, 1, 2 COMPLETE")
    print("  âœ… Wave 2 (P1 High): Tasks 3, 4, 5 COMPLETE")
    print("  âœ… Wave 3 (P2 Statistical): Tasks 6, 7 COMPLETE")
    print("  âœ… Wave 4 (P2 Reporting): Task 8 COMPLETE")
    print("\n")
    print("  ğŸ‰ ALL 9 TASKS COMPLETE! ğŸ‰")
    print("\n")
    print("Next Steps:")
    print("  1. Update STATUS.md to mark Task 8 complete")
    print("  2. Update src/validation/__init__.py exports")
    print("  3. Create final completion summary")
    print("  4. Re-run 20-strategy dataset with full validation")
    print("\n")

    return 0 if failed == 0 else 1


if __name__ == "__main__":
    sys.exit(main())
