# Monitoring Configuration
# Comprehensive monitoring settings for the FinLab resource monitoring system
# Controls Prometheus metrics, Grafana dashboards, and alert thresholds

# === CORE MONITORING SETTINGS ===
monitoring:
  # Master switch for all monitoring features
  # Set to false to disable monitoring entirely (not recommended for production)
  enabled: ${MONITORING_ENABLED:true}

  # === PROMETHEUS CONFIGURATION ===
  prometheus:
    # HTTP server port for /metrics endpoint
    # Prometheus scrapes this endpoint to collect metrics
    # Default: 8000 (standard Prometheus client port)
    port: ${PROMETHEUS_PORT:8000}

    # Metrics endpoint path
    # Full URL: http://localhost:8000/metrics
    metrics_path: "/metrics"

    # Scrape interval: How often Prometheus collects metrics (seconds)
    # Lower = more granular data, higher storage
    # Higher = less storage, coarser resolution
    # Recommended: 5s for production, 10s for dev/test
    scrape_interval: ${PROMETHEUS_SCRAPE_INTERVAL:5}

    # Metrics retention period (days)
    # How long Prometheus stores historical metrics
    # Default: 30 days (balance between storage and historical analysis)
    retention_days: 30

# === RESOURCE MONITORING ===
resource_monitor:
  # Enable system resource tracking (CPU, memory, disk)
  enabled: ${RESOURCE_MONITOR_ENABLED:true}

  # Collection interval: How often to sample system resources (seconds)
  # Lower = detect issues faster, higher CPU overhead
  # Higher = less overhead, slower issue detection
  # Default: 5s (balance between responsiveness and overhead)
  collection_interval: ${RESOURCE_COLLECTION_INTERVAL:5}

  # Resource collection timeout (seconds)
  # Max time to wait for psutil queries before giving up
  # Prevents hanging on slow I/O operations
  timeout: 2

  # Enable detailed logging of resource metrics
  # Set to true for debugging resource issues
  log_metrics: false

# === DIVERSITY MONITORING ===
diversity_monitor:
  # Enable population diversity and champion staleness tracking
  enabled: ${DIVERSITY_MONITOR_ENABLED:true}

  # Diversity collapse detection window (iterations)
  # Alert triggers if diversity stays below threshold for this many consecutive iterations
  # Example: 5 iterations at diversity <0.1 = collapse detected
  # Lower = faster detection but more false positives
  # Higher = fewer false positives but slower detection
  diversity_collapse_window: 5

  # Diversity collapse threshold (0.0-1.0)
  # Alert if diversity falls below this value for consecutive iterations
  # 0.0 = no diversity (all strategies identical)
  # 1.0 = maximum diversity (all strategies unique)
  # Default: 0.1 (indicates severe diversity loss)
  diversity_collapse_threshold: 0.1

  # Champion staleness threshold (iterations)
  # Alert if champion hasn't updated in this many iterations
  # Indicates learning stagnation or overly strict anti-churn settings
  # Default: 20 iterations (based on typical update frequency of 10-20%)
  champion_staleness_threshold: 20

  # Enable detailed logging of diversity metrics
  log_metrics: false

# === CONTAINER MONITORING ===
container_monitor:
  # Enable Docker container resource tracking and cleanup
  enabled: ${CONTAINER_MONITOR_ENABLED:true}

  # Stats collection interval: How often to query container stats (seconds)
  # Lower = more accurate resource tracking, higher Docker API load
  # Higher = less load, coarser resource data
  # Default: 30s (containers are long-lived, less frequent checks acceptable)
  stats_collection_interval: ${CONTAINER_STATS_INTERVAL:30}

  # Docker stats query timeout (seconds)
  # Prevent hanging on unresponsive containers
  # If timeout occurs, use cached stats from previous query
  stats_timeout: 2

  # Orphaned container scan interval (seconds)
  # How often to search for containers with status=exited
  # Lower = faster cleanup, higher Docker API load
  # Higher = slower cleanup, less load
  # Default: 30s (orphans are rare, infrequent scans acceptable)
  orphan_scan_interval: 30

  # Automatic orphaned container cleanup
  # If true, automatically remove containers with label=finlab-sandbox and status=exited
  # If false, only alert on orphans (manual cleanup required)
  # Default: true (prevent resource leaks)
  auto_cleanup_orphans: true

  # Container label filter for orphan detection
  # Only containers with this label are considered "ours" for cleanup
  # Prevents accidentally removing unrelated containers
  container_label: "finlab-sandbox"

  # Enable detailed logging of container operations
  log_operations: false

# === ALERT CONFIGURATION ===
alerts:
  # Master switch for all alerting
  # Set to false to disable alert evaluation (metrics still collected)
  enabled: ${ALERTS_ENABLED:true}

  # Alert evaluation interval (seconds)
  # How often to check alert conditions
  # Lower = faster alert response, higher CPU overhead
  # Higher = less overhead, slower alerts
  # Default: 10s (balance between responsiveness and performance)
  evaluation_interval: ${ALERT_EVALUATION_INTERVAL:10}

  # Alert suppression window (seconds)
  # Don't re-alert for the same condition within this time window
  # Prevents alert fatigue from flapping conditions
  # Default: 300s (5 minutes)
  suppression_window: 300

  # === MEMORY ALERT ===
  memory:
    # Memory usage threshold (percent of system memory)
    # Alert if system memory usage exceeds this percentage
    # Example: 80% on 16GB system = alert at 12.8GB used
    # Lower = earlier warnings but more false positives
    # Higher = fewer false positives but less warning time
    # Default: 80% (standard production threshold)
    threshold_percent: ${MEMORY_THRESHOLD:80.0}

    # Alert severity level
    # Options: info, warning, critical
    # Affects logging level and notification routing
    severity: warning

  # === CPU ALERT ===
  cpu:
    # CPU usage threshold (percent of total CPU)
    # Alert if system CPU usage exceeds this percentage
    # Default: 90% (high threshold, indicates CPU exhaustion)
    threshold_percent: ${CPU_THRESHOLD:90.0}

    # Alert severity level
    severity: warning

  # === DIVERSITY COLLAPSE ALERT ===
  diversity_collapse:
    # Threshold and window defined in diversity_monitor section
    # Alert triggers if diversity < threshold for consecutive iterations
    # Severity: critical (indicates learning system failure)
    severity: critical

  # === CHAMPION STALENESS ALERT ===
  champion_staleness:
    # Threshold defined in diversity_monitor section
    # Alert if champion unchanged for threshold iterations
    # Severity: warning (may indicate overly conservative anti-churn)
    severity: warning

  # === SUCCESS RATE ALERT ===
  success_rate:
    # Success rate threshold (percent of successful backtests)
    # Alert if success rate falls below this over window
    # Example: <20% success over 10 iterations = system degradation
    # Default: 20% (indicates severe execution issues)
    threshold_percent: ${SUCCESS_RATE_THRESHOLD:20.0}

    # Rolling window for success rate calculation (iterations)
    # Calculate success rate over last N iterations
    # Lower = faster detection but more volatility
    # Higher = smoother but slower detection
    # Default: 10 iterations
    window_iterations: 10

    # Alert severity level
    severity: critical

  # === ORPHANED CONTAINERS ALERT ===
  orphaned_containers:
    # Orphaned container count threshold
    # Alert if orphaned container count exceeds this value
    # Default: 3 (indicates repeated cleanup failures)
    threshold: ${ORPHANED_THRESHOLD:3}

    # Alert severity level
    severity: warning

  # === EXECUTION TIME ALERT ===
  execution_time:
    # Execution time threshold (seconds per iteration)
    # Alert if iteration takes longer than this
    # Indicates performance degradation or resource contention
    # Default: 300s (5 minutes per iteration)
    threshold_seconds: ${EXECUTION_TIME_THRESHOLD:300}

    # Alert severity level
    severity: info

# === METRICS EXPORT ===
export:
  # Final metrics JSON export path
  # Written at end of iteration loop for post-analysis
  # Relative to project root
  final_metrics_path: "${FINAL_METRICS_PATH:metrics_final.json}"

  # Final Prometheus snapshot export path
  # Text dump of all metrics at end of run
  # Useful for debugging and archival
  prometheus_snapshot_path: "${PROMETHEUS_SNAPSHOT_PATH:metrics_final.txt}"

  # Metrics export format
  # Options: json, prometheus, both
  # json = structured JSON for parsing
  # prometheus = text format for ingestion
  # both = export both formats
  format: both

  # Include timestamps in exports
  include_timestamps: true

  # Include metric metadata (help text, type)
  include_metadata: true

# === GRAFANA DASHBOARD ===
grafana:
  # Dashboard configuration file path
  # JSON template for importing into Grafana
  # Relative to project root
  dashboard_path: "config/grafana_dashboard.json"

  # Prometheus datasource URL
  # Grafana queries this Prometheus instance for metrics
  # Must match Prometheus server configuration
  datasource_url: "${PROMETHEUS_URL:http://localhost:9090}"

  # Dashboard refresh interval
  # How often Grafana polls Prometheus for new data
  # Options: 5s, 10s, 30s, 1m, 5m
  # Lower = more real-time, higher Prometheus load
  # Default: 5s (matches scrape interval)
  refresh_interval: "5s"

  # Enable annotations for champion update events
  # Shows vertical lines on graphs when champion changes
  enable_annotations: true

  # Default time range for dashboard
  # Options: 5m, 15m, 1h, 6h, 12h, 24h, 7d
  # Default: 1h (good for monitoring active runs)
  default_time_range: "1h"

# === PERFORMANCE TUNING ===
performance:
  # Maximum metrics collection overhead (percent of iteration time)
  # If metrics collection exceeds this, log warning and reduce frequency
  # Default: 1% (negligible impact on iteration performance)
  max_overhead_percent: 1.0

  # Background thread priority
  # Options: low, normal, high
  # low = less impact on iteration loop, slower metrics
  # high = faster metrics, may impact loop performance
  # Default: low (metrics are not time-critical)
  thread_priority: low

  # Metrics buffer size (number of samples)
  # In-memory buffer before flushing to Prometheus
  # Larger = less frequent writes, higher memory usage
  # Smaller = more frequent writes, lower memory
  # Default: 1000 samples (~5000s at 5s interval)
  buffer_size: 1000

# === LOGGING INTEGRATION ===
logging:
  # Enable monitoring-specific logging
  # Logs all monitoring events (alerts, collection, cleanup)
  enabled: true

  # Log level for monitoring events
  # Options: DEBUG, INFO, WARNING, ERROR, CRITICAL
  # DEBUG = verbose, all events logged
  # INFO = standard, major events only
  # WARNING = errors and alerts only
  # Default: INFO
  level: "${MONITORING_LOG_LEVEL:INFO}"

  # Log format
  # Options: json, text
  # json = structured logging for parsing
  # text = human-readable for console
  # Default: json (matches existing json_logger.py)
  format: json

  # Include metric values in logs
  # If true, log actual metric values with events
  # Useful for debugging but increases log volume
  include_values: false

# === NOTIFICATION CHANNELS (Future Enhancement) ===
# Note: Currently logs only, notification integration planned for future
notifications:
  # Enable external notifications (email, Slack, PagerDuty)
  # Currently unsupported, reserved for future implementation
  enabled: false

  # Notification channels
  # channels:
  #   - type: email
  #     recipients: ["ops@example.com"]
  #     severity_filter: [critical, warning]
  #
  #   - type: slack
  #     webhook_url: "https://hooks.slack.com/..."
  #     channel: "#alerts"
  #     severity_filter: [critical]
  #
  #   - type: pagerduty
  #     integration_key: "..."
  #     severity_filter: [critical]

# === FEATURE FLAGS ===
features:
  # Enable experimental features
  enable_experimental: false

  # Enable advanced metrics (histograms, summaries)
  # Increases memory usage but provides percentile data
  enable_advanced_metrics: false

  # Enable metric cardinality limits
  # Prevents unbounded label values (e.g., container_id)
  # Default: true (prevent metric explosion)
  enable_cardinality_limits: true

  # Maximum unique label values per metric
  # Prevents memory exhaustion from high-cardinality labels
  # Default: 1000 (sufficient for typical container counts)
  max_cardinality: 1000
