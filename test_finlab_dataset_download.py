#!/usr/bin/env python3
"""
FinLab è³‡æ–™é›†ä¸‹è¼‰èˆ‡å¿«å–é©—è­‰æ¸¬è©¦
===================================

ç›®çš„ï¼š
1. é€ä¸€å¾ FinLab ä¸‹è¼‰ CSV ä¸­åˆ—å‡ºçš„ 307 å€‹è³‡æ–™é›†
2. é©—è­‰æ¯å€‹è³‡æ–™é›†éƒ½å¯ä»¥æ­£å¸¸ä¸‹è¼‰
3. æ¸¬è©¦ DataCache ç³»çµ±æ˜¯å¦èƒ½ä½œç‚ºå¿«å–ä½¿ç”¨
4. ç”Ÿæˆè©³ç´°çš„ä¸‹è¼‰å ±å‘Šå’Œçµ±è¨ˆ

ä½¿ç”¨ç¾æœ‰çµ„ä»¶ï¼š
- src/templates/data_cache.py - è³‡æ–™å¿«å–ç³»çµ±
- src/backtest/finlab_authenticator.py - Session é©—è­‰
"""

import csv
import sys
import time
import json
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Tuple, Any

# Add src to path
sys.path.insert(0, str(Path(__file__).parent / 'src'))

from templates.data_cache import DataCache
from backtest.finlab_authenticator import verify_finlab_session


def load_csv_datasets(csv_path: str) -> List[Dict[str, str]]:
    """
    å¾ CSV è¼‰å…¥æ‰€æœ‰è³‡æ–™é›†å®šç¾©

    Returns:
        List of dict with keys: è³‡æ–™é›†åç¨±, ä¸‹è¼‰æ–¹å¼åŠkey, æ•¸æ“šé¡å‹
    """
    datasets = []

    with open(csv_path, 'r', encoding='utf-8-sig') as f:
        reader = csv.DictReader(f)
        for row in reader:
            datasets.append({
                'åç¨±': row['è³‡æ–™é›†åç¨±'],
                'key': row['ä¸‹è¼‰æ–¹å¼åŠkey'],
                'é¡å‹': row['æ•¸æ“šé¡å‹']
            })

    return datasets


def extract_finlab_key(download_key: str) -> Tuple[str, str]:
    """
    å¾ä¸‹è¼‰æ–¹å¼æå– FinLab data.get() çš„ key

    Args:
        download_key: CSV ä¸­çš„ä¸‹è¼‰æ–¹å¼

    Returns:
        (key_type, finlab_key)
        key_type: 'etl', 'data.get', 'other'
        finlab_key: å¯¦éš›çš„ key (e.g., 'price:æ”¶ç›¤åƒ¹')

    Examples:
        'etl:adj_close' â†’ ('etl', 'etl:adj_close')
        "data.get('price:æ”¶ç›¤åƒ¹')" â†’ ('data.get', 'price:æ”¶ç›¤åƒ¹')
    """
    if download_key.startswith('etl:'):
        return 'etl', download_key

    elif download_key.startswith('data.get('):
        # Extract key from data.get('key') format
        # data.get('price:æ”¶ç›¤åƒ¹') â†’ price:æ”¶ç›¤åƒ¹
        key = download_key.split("'")[1] if "'" in download_key else None
        if key:
            return 'data.get', key
        else:
            return 'other', download_key

    else:
        return 'other', download_key


def test_dataset_download(cache: DataCache, key: str, name: str,
                          verbose: bool = True) -> Dict[str, Any]:
    """
    æ¸¬è©¦å–®ä¸€è³‡æ–™é›†ä¸‹è¼‰

    Returns:
        çµæœå­—å…¸åŒ…å«ï¼š
        - success: bool
        - key: str
        - name: str
        - shape: tuple or None
        - error: str or None
        - load_time: float
        - cache_hit: bool (ç¬¬äºŒæ¬¡æ¸¬è©¦æ™‚æ˜¯å¦å¾å¿«å–è®€å–)
    """
    result = {
        'success': False,
        'key': key,
        'name': name,
        'shape': None,
        'error': None,
        'load_time': 0.0,
        'cache_hit': False
    }

    try:
        # ç¬¬ä¸€æ¬¡ä¸‹è¼‰
        start_time = time.time()
        data = cache.get(key, verbose=False)
        load_time = time.time() - start_time

        if data is None:
            result['error'] = "data.get() returned None"
            if verbose:
                print(f"  âŒ {name}: returned None")
            return result

        # æª¢æŸ¥ shape
        if hasattr(data, 'shape'):
            result['shape'] = data.shape
            result['load_time'] = load_time
            result['success'] = True

            if verbose:
                print(f"  âœ… {name}: {data.shape} ({load_time:.2f}s)")
        else:
            result['error'] = f"No shape attribute (type: {type(data).__name__})"
            if verbose:
                print(f"  âš ï¸  {name}: No shape (type: {type(data).__name__})")

        # æ¸¬è©¦å¿«å– - ç¬¬äºŒæ¬¡è®€å–æ‡‰è©²æ¥µå¿«
        cache_start = time.time()
        cached_data = cache.get(key, verbose=False)
        cache_time = time.time() - cache_start

        # å¿«å–å‘½ä¸­æ‡‰è©² <0.01s
        result['cache_hit'] = cache_time < 0.01

        if verbose and result['cache_hit']:
            print(f"     ğŸ’¾ Cache hit! ({cache_time*1000:.1f}ms)")

        return result

    except Exception as e:
        result['error'] = f"{type(e).__name__}: {str(e)}"
        if verbose:
            print(f"  âŒ {name}: {result['error']}")
        return result


def main():
    csv_path = "/mnt/c/Users/jnpi/ML4T/epic-finlab-data-downloader/example/finlab_database_cleaned.csv"
    output_dir = Path("experiments/finlab_data_verification")
    output_dir.mkdir(parents=True, exist_ok=True)

    print("=" * 80)
    print("FinLab è³‡æ–™é›†ä¸‹è¼‰èˆ‡å¿«å–é©—è­‰æ¸¬è©¦")
    print("=" * 80)
    print(f"é–‹å§‹æ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")

    # Step 1: é©—è­‰ FinLab session
    print("Step 1: é©—è­‰ FinLab Session")
    print("-" * 80)
    status = verify_finlab_session(verbose=False)

    if not status.is_authenticated:
        print("âŒ FinLab session æœªæ­£ç¢ºé©—è­‰ï¼")
        print(f"éŒ¯èª¤: {status.error_message}")
        for diag in status.diagnostics:
            print(f"  {diag}")
        return 1

    print("âœ… FinLab session é©—è­‰æˆåŠŸï¼\n")

    # Step 2: è¼‰å…¥è³‡æ–™é›†æ¸…å–®
    print("Step 2: è¼‰å…¥ CSV è³‡æ–™é›†æ¸…å–®")
    print("-" * 80)
    datasets = load_csv_datasets(csv_path)
    print(f"âœ… è¼‰å…¥ {len(datasets)} å€‹è³‡æ–™é›†å®šç¾©\n")

    # Step 3: å–å¾—å¿«å–å¯¦ä¾‹
    print("Step 3: åˆå§‹åŒ– DataCache")
    print("-" * 80)
    cache = DataCache.get_instance()
    print(f"âœ… DataCache å¯¦ä¾‹å·²å°±ç·’\n")

    # Step 4: é€ä¸€æ¸¬è©¦ä¸‹è¼‰
    print("Step 4: é€ä¸€ä¸‹è¼‰ä¸¦é©—è­‰è³‡æ–™é›†")
    print("-" * 80)
    print(f"é–‹å§‹ä¸‹è¼‰ {len(datasets)} å€‹è³‡æ–™é›†...\n")

    results = []
    success_count = 0
    fail_count = 0
    skip_count = 0

    for i, dataset in enumerate(datasets, 1):
        name = dataset['åç¨±']
        download_key = dataset['key']
        data_type = dataset['é¡å‹']

        print(f"[{i}/{len(datasets)}] {name}")

        # æå– FinLab key
        key_type, finlab_key = extract_finlab_key(download_key)

        if key_type == 'other':
            print(f"  â­ï¸  è·³é (ç„¡æ³•è§£æçš„æ ¼å¼: {download_key})")
            skip_count += 1
            results.append({
                'index': i,
                'name': name,
                'key': download_key,
                'type': data_type,
                'key_type': key_type,
                'success': False,
                'skipped': True,
                'error': 'Unsupported format'
            })
            continue

        # æ¸¬è©¦ä¸‹è¼‰
        result = test_dataset_download(cache, finlab_key, name, verbose=True)
        result['index'] = i
        result['type'] = data_type
        result['key_type'] = key_type
        result['download_key'] = download_key

        if result['success']:
            success_count += 1
        else:
            fail_count += 1

        results.append(result)

        # æ¯10å€‹è³‡æ–™é›†é¡¯ç¤ºé€²åº¦
        if i % 10 == 0:
            print(f"\né€²åº¦: {i}/{len(datasets)} ({i/len(datasets)*100:.1f}%)")
            print(f"æˆåŠŸ: {success_count}, å¤±æ•—: {fail_count}, è·³é: {skip_count}\n")

    # Step 5: çµ±è¨ˆèˆ‡å ±å‘Š
    print("\n" + "=" * 80)
    print("æ¸¬è©¦å®Œæˆ - çµ±è¨ˆå ±å‘Š")
    print("=" * 80)

    cache_stats = cache.get_stats()

    print(f"\nè³‡æ–™é›†ä¸‹è¼‰çµ±è¨ˆ:")
    print(f"  ç¸½æ•¸:   {len(datasets)}")
    print(f"  æˆåŠŸ:   {success_count} ({success_count/len(datasets)*100:.1f}%)")
    print(f"  å¤±æ•—:   {fail_count} ({fail_count/len(datasets)*100:.1f}%)")
    print(f"  è·³é:   {skip_count} ({skip_count/len(datasets)*100:.1f}%)")

    print(f"\nDataCache çµ±è¨ˆ:")
    print(f"  å¿«å–å¤§å°:     {cache_stats['cache_size']} å€‹è³‡æ–™é›†")
    print(f"  å¿«å–å‘½ä¸­ç‡:   {cache_stats['hit_rate']:.2%}")
    print(f"  ç¸½è«‹æ±‚æ¬¡æ•¸:   {cache_stats['total_requests']}")
    print(f"  å¹³å‡è¼‰å…¥æ™‚é–“: {cache_stats['avg_load_time']:.2f}ç§’")
    print(f"  ç¸½è¼‰å…¥æ™‚é–“:   {cache_stats['total_load_time']:.2f}ç§’")

    # å¤±æ•—çš„è³‡æ–™é›†
    if fail_count > 0:
        print(f"\nå¤±æ•—çš„è³‡æ–™é›† ({fail_count}):")
        for result in results:
            if not result['success'] and not result.get('skipped', False):
                print(f"  âŒ {result['name']}")
                print(f"     Key: {result['key']}")
                print(f"     éŒ¯èª¤: {result['error']}")

    # Step 6: å„²å­˜çµæœ
    print(f"\nStep 6: å„²å­˜æ¸¬è©¦çµæœ")
    print("-" * 80)

    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')

    # è©³ç´°çµæœ JSON
    results_file = output_dir / f"dataset_download_results_{timestamp}.json"
    with open(results_file, 'w', encoding='utf-8') as f:
        json.dump({
            'timestamp': datetime.now().isoformat(),
            'total_datasets': len(datasets),
            'success_count': success_count,
            'fail_count': fail_count,
            'skip_count': skip_count,
            'cache_stats': cache_stats,
            'results': results
        }, f, indent=2, ensure_ascii=False)

    print(f"âœ… è©³ç´°çµæœå·²å„²å­˜: {results_file}")

    # ç°¡è¦çµ±è¨ˆ TXT
    summary_file = output_dir / f"dataset_download_summary_{timestamp}.txt"
    with open(summary_file, 'w', encoding='utf-8') as f:
        f.write("=" * 80 + "\n")
        f.write("FinLab è³‡æ–™é›†ä¸‹è¼‰é©—è­‰ - æ‘˜è¦å ±å‘Š\n")
        f.write("=" * 80 + "\n\n")
        f.write(f"æ¸¬è©¦æ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"ç¸½è³‡æ–™é›†: {len(datasets)}\n")
        f.write(f"æˆåŠŸä¸‹è¼‰: {success_count} ({success_count/len(datasets)*100:.1f}%)\n")
        f.write(f"ä¸‹è¼‰å¤±æ•—: {fail_count} ({fail_count/len(datasets)*100:.1f}%)\n")
        f.write(f"è·³éé …ç›®: {skip_count} ({skip_count/len(datasets)*100:.1f}%)\n\n")
        f.write(f"DataCache æ€§èƒ½:\n")
        f.write(f"  å¿«å–å¤§å°: {cache_stats['cache_size']}\n")
        f.write(f"  å‘½ä¸­ç‡: {cache_stats['hit_rate']:.2%}\n")
        f.write(f"  å¹³å‡è¼‰å…¥æ™‚é–“: {cache_stats['avg_load_time']:.2f}s\n\n")

        if fail_count > 0:
            f.write(f"å¤±æ•—çš„è³‡æ–™é›†:\n")
            for result in results:
                if not result['success'] and not result.get('skipped', False):
                    f.write(f"  - {result['name']} ({result['key']})\n")
                    f.write(f"    éŒ¯èª¤: {result['error']}\n")

    print(f"âœ… æ‘˜è¦å ±å‘Šå·²å„²å­˜: {summary_file}")

    print("\n" + "=" * 80)
    print("æ¸¬è©¦å®Œæˆï¼")
    print("=" * 80)

    # çµè«–
    print("\nğŸ“Š çµè«–:")
    if success_count >= len(datasets) * 0.95:
        print("  âœ… å„ªç§€ï¼95%+ è³‡æ–™é›†å¯æ­£å¸¸ä¸‹è¼‰")
    elif success_count >= len(datasets) * 0.80:
        print("  âš ï¸  è‰¯å¥½ï¼Œä½†æœ‰éƒ¨åˆ†è³‡æ–™é›†ç„¡æ³•ä¸‹è¼‰")
    else:
        print("  âŒ è­¦å‘Šï¼šå¤§é‡è³‡æ–™é›†ç„¡æ³•ä¸‹è¼‰")

    if cache_stats['hit_rate'] > 0.3:
        print("  âœ… DataCache ç³»çµ±é‹ä½œæ­£å¸¸ï¼Œå¯ä½œç‚ºå¿«å–ä½¿ç”¨")
    else:
        print("  âš ï¸  DataCache å¿«å–å‘½ä¸­ç‡åä½")

    return 0 if success_count >= len(datasets) * 0.80 else 1


if __name__ == "__main__":
    sys.exit(main())
