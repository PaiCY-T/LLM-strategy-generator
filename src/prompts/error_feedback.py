"""
Error Feedback Loop for LLM Config Generation Validation - Task 24.3

This module provides error feedback and retry mechanisms for invalid YAML configurations
generated by LLMs. It integrates with SchemaValidator to provide actionable feedback
and enable LLMs to retry config generation with corrected errors.

Key Features:
- Convert validation errors to human-readable feedback
- Generate retry prompts with original YAML and error details
- Automatic retry loop with max retry limits
- Error history tracking for debugging
- Integration with SchemaValidator from Layer 3

Usage:
    from src.prompts.error_feedback import ErrorFeedbackLoop
    from src.execution.schema_validator import SchemaValidator

    # Initialize feedback loop with max retries
    loop = ErrorFeedbackLoop(max_retries=3)
    validator = SchemaValidator()

    # Define LLM generate function
    def llm_generate(prompt: str) -> str:
        # Call your LLM API with prompt
        return llm_api.generate(prompt)

    # Validate with automatic retry on errors
    success, validated_config, error_history = loop.validate_and_retry(
        yaml_str,
        validator,
        llm_generate
    )

    if success:
        print(f"Validation successful: {validated_config}")
    else:
        print(f"Validation failed after {len(error_history)} attempts")
        for i, error in enumerate(error_history, 1):
            print(f"  Attempt {i}: {error}")

Integration:
    - Layer 3: Uses SchemaValidator and ValidationError
    - Layer 2: Compatible with prompt_formatter functions
    - LLM APIs: Provides retry prompts for correction

Architecture:
    format_validation_errors(): Convert errors to readable feedback
    generate_retry_prompt(): Create retry prompt with errors
    ErrorFeedbackLoop: Orchestrate validation and retry workflow

Thread Safety:
    - ErrorFeedbackLoop instances are independent
    - validate_and_retry() is stateless - no side effects
"""

from typing import List, Dict, Optional, Tuple, Callable
import yaml

from src.execution.schema_validator import (
    SchemaValidator,
    ValidationError,
    ValidationSeverity
)


def format_field_errors(errors: List['FieldError']) -> str:
    """
    Convert FieldError objects to human-readable feedback for LLM.

    Formats field validation errors with line numbers, error types, and suggestions
    to help LLM understand and fix code-level validation failures.

    Args:
        errors: List of FieldError objects from Layer 2 FieldValidator
               Can be empty list (returns empty string)

    Returns:
        Formatted error message string with line numbers and suggestions
        Empty string if no errors

    Example:
        >>> from src.validation.validation_result import FieldError
        >>> errors = [
        ...     FieldError(
        ...         line=10,
        ...         column=15,
        ...         field_name='price:成交量',
        ...         error_type='invalid_field',
        ...         message='Invalid field name',
        ...         suggestion='Did you mean "price:成交金額"?'
        ...     )
        ... ]
        >>> print(format_field_errors(errors))
        1. **Line 10**: Invalid field name
           Field: price:成交量
           → Suggestion: Did you mean "price:成交金額"?
    """
    if not errors:
        return ""

    output_parts = []

    for i, error in enumerate(errors, 1):
        error_lines = [f"{i}. **Line {error.line}**: {error.message}"]

        # Add field name
        if error.field_name:
            error_lines.append(f"   Field: {error.field_name}")

        # Add suggestion if available
        if error.suggestion:
            error_lines.append(f"   → Suggestion: {error.suggestion}")

        output_parts.append("\n".join(error_lines))
        output_parts.append("")

    return "\n".join(output_parts).rstrip()


def generate_retry_prompt_for_code(
    original_code: str,
    field_errors: List['FieldError'],
    attempt_number: int
) -> str:
    """
    Generate retry prompt for LLM with original code and field validation errors.

    Creates a structured prompt for code validation failures (Layer 2), showing
    the LLM what field names were invalid and guiding it to generate corrected code.

    Args:
        original_code: The Python code that failed validation
        field_errors: List of FieldError objects explaining field validation failures
        attempt_number: Current retry attempt number (1-based)

    Returns:
        Formatted retry prompt string ready for LLM input

    Example:
        >>> from src.validation.validation_result import FieldError
        >>> code = "def strategy(data):\\n    return data.get('bad_field') > 100"
        >>> errors = [
        ...     FieldError(
        ...         line=2,
        ...         column=25,
        ...         field_name='bad_field',
        ...         error_type='invalid_field',
        ...         message='Invalid field name',
        ...         suggestion='Use valid field from manifest'
        ...     )
        ... ]
        >>> prompt = generate_retry_prompt_for_code(code, errors, attempt_number=1)
        >>> "Invalid field name" in prompt
        True
        >>> "Attempt 1" in prompt
        True

    Prompt Structure:
        ## Code Validation Failed - Retry Required

        **Attempt: {attempt_number}**

        Your previous code had the following validation errors:

        {formatted_errors}

        ## Your Original Code (with errors):
        ```python
        {original_code}
        ```

        ## Instructions:
        Please fix the validation errors and generate corrected code.
        Use only valid field names listed in the reference above.
    """
    # Format field errors
    formatted_errors = format_field_errors(field_errors)

    # Build retry prompt
    prompt = f"""## Code Validation Failed - Retry Required

**Attempt: {attempt_number}**

Your previous code had the following validation errors:

{formatted_errors}

## Your Original Code (with errors):
```python
{original_code.strip()}
```

## Instructions:
Please fix the validation errors and generate corrected code.
Use only valid field names listed in the reference above.
"""

    return prompt


def format_validation_errors(errors: List[ValidationError]) -> str:
    """
    Convert ValidationError objects to human-readable feedback for LLM.

    Formats errors grouped by severity (ERROR, WARNING, INFO) with clear
    structure including field paths, line numbers, and actionable suggestions.

    Args:
        errors: List of ValidationError objects from SchemaValidator
               Can be empty list (returns empty string)

    Returns:
        Formatted error message string grouped by severity
        Empty string if no errors

    Example:
        >>> errors = [
        ...     ValidationError(
        ...         severity=ValidationSeverity.ERROR,
        ...         message="Missing required key: 'name'",
        ...         field_path="<root>",
        ...         suggestion="Add 'name' to the top level of your YAML"
        ...     )
        ... ]
        >>> print(format_validation_errors(errors))
        === ERRORS (1) ===

        1. Missing required key: 'name'
           Field: <root>
           Suggestion: Add 'name' to the top level of your YAML

    Format:
        === ERRORS (N) ===
        1. Error message
           Field: field_path
           Line: line_number (if available)
           Suggestion: suggestion (if available)

        === WARNINGS (N) ===
        ...

        === INFO (N) ===
        ...
    """
    if not errors:
        return ""

    # Group errors by severity
    errors_by_severity = {
        ValidationSeverity.ERROR: [],
        ValidationSeverity.WARNING: [],
        ValidationSeverity.INFO: []
    }

    for error in errors:
        errors_by_severity[error.severity].append(error)

    # Format output
    output_parts = []

    # Process in order: ERROR, WARNING, INFO
    severity_labels = {
        ValidationSeverity.ERROR: "ERRORS",
        ValidationSeverity.WARNING: "WARNINGS",
        ValidationSeverity.INFO: "INFO"
    }

    for severity in [ValidationSeverity.ERROR, ValidationSeverity.WARNING, ValidationSeverity.INFO]:
        severity_errors = errors_by_severity[severity]
        if not severity_errors:
            continue

        # Add section header
        severity_label = severity_labels[severity]
        section_title = f"=== {severity_label} ({len(severity_errors)}) ==="
        output_parts.append(section_title)
        output_parts.append("")

        # Format each error
        for i, error in enumerate(severity_errors, 1):
            error_lines = [f"{i}. {error.message}"]

            # Add field path
            if error.field_path:
                error_lines.append(f"   Field: {error.field_path}")

            # Add line number if available
            if error.line_number is not None:
                error_lines.append(f"   Line: {error.line_number}")

            # Add suggestion if available
            if error.suggestion:
                error_lines.append(f"   Suggestion: {error.suggestion}")

            output_parts.append("\n".join(error_lines))
            output_parts.append("")

    return "\n".join(output_parts).rstrip()


def generate_retry_prompt(
    original_yaml: str,
    validation_errors: List[ValidationError],
    attempt_number: int
) -> str:
    """
    Generate retry prompt for LLM with original YAML and validation errors.

    Creates a structured prompt that shows the LLM what went wrong and guides
    it to generate corrected YAML. Includes attempt number and remaining attempts.

    Args:
        original_yaml: The YAML string that failed validation
        validation_errors: List of ValidationError objects explaining failures
        attempt_number: Current retry attempt number (1-based)

    Returns:
        Formatted retry prompt string ready for LLM input

    Example:
        >>> original_yaml = "name: Test\\ntype: invalid_type"
        >>> errors = [
        ...     ValidationError(
        ...         severity=ValidationSeverity.ERROR,
        ...         message="Invalid strategy type: 'invalid_type'",
        ...         field_path="type",
        ...         suggestion="Valid types are: factor_graph, llm_generated, hybrid"
        ...     )
        ... ]
        >>> prompt = generate_retry_prompt(original_yaml, errors, attempt_number=1)
        >>> "Invalid strategy type" in prompt
        True
        >>> "Attempt: 1" in prompt
        True

    Prompt Structure:
        ## YAML Validation Failed - Retry Required

        **Attempt: {attempt_number}**

        Your previous YAML configuration had the following errors:

        {formatted_errors}

        ## Your Original YAML:
        ```yaml
        {original_yaml}
        ```

        ## Instructions:
        Please fix the errors above and generate a corrected YAML configuration.
        Follow the suggestions provided and ensure all required fields are present.

        Return ONLY valid YAML. No explanations or comments.
    """
    # Format validation errors
    formatted_errors = format_validation_errors(validation_errors)

    # Build retry prompt
    prompt = f"""## YAML Validation Failed - Retry Required

**Attempt: {attempt_number}**

Your previous YAML configuration had the following errors:

{formatted_errors}

## Your Original YAML:
```yaml
{original_yaml.strip()}
```

## Instructions:
Please fix the errors above and generate a corrected YAML configuration.
Follow the suggestions provided and ensure all required fields are present.

Return ONLY valid YAML. No explanations or comments.
"""

    return prompt


class ErrorFeedbackLoop:
    """
    Orchestrates validation and retry workflow for LLM-generated YAML configs.

    Validates YAML configurations using SchemaValidator and provides feedback
    to LLM for retry when validation fails. Enforces maximum retry limits and
    tracks error history for debugging.

    Attributes:
        max_retries: Maximum number of retry attempts (default: 3)

    Example:
        >>> from src.execution.schema_validator import SchemaValidator
        >>> loop = ErrorFeedbackLoop(max_retries=3)
        >>> validator = SchemaValidator()
        >>>
        >>> def mock_llm(prompt: str) -> str:
        ...     return "name: Test\\ntype: factor_graph\\n..."
        >>>
        >>> success, config, history = loop.validate_and_retry(
        ...     "invalid_yaml",
        ...     validator,
        ...     mock_llm
        ... )

    Thread Safety:
        - Each ErrorFeedbackLoop instance is independent
        - validate_and_retry() is stateless with no side effects
        - Safe for concurrent usage across multiple threads
    """

    def __init__(self, max_retries: int = 3):
        """
        Initialize ErrorFeedbackLoop with retry configuration.

        Args:
            max_retries: Maximum number of retry attempts
                        Default: 3
                        Range: 1-10 recommended
        """
        self.max_retries = max_retries

    def validate_and_retry(
        self,
        yaml_str: str,
        validator: SchemaValidator,
        llm_generate_func: Callable[[str], str]
    ) -> Tuple[bool, Optional[Dict], List[str]]:
        """
        Validate YAML config and retry with feedback if validation fails.

        Performs the following workflow:
        1. Parse YAML string to dict
        2. Validate dict using SchemaValidator
        3. If valid: return success with validated config
        4. If invalid: generate retry prompt with errors
        5. Call LLM generate function with retry prompt
        6. Repeat steps 1-5 up to max_retries times
        7. Return failure if max retries exceeded

        Args:
            yaml_str: YAML configuration string to validate
            validator: SchemaValidator instance for validation
            llm_generate_func: Callable that takes retry prompt and returns
                              corrected YAML string
                              Signature: (prompt: str) -> str

        Returns:
            Tuple of (success, validated_config, error_history):
                - success: True if validation succeeded, False otherwise
                - validated_config: Parsed dict if valid, None if invalid
                - error_history: List of error summaries from all attempts

        Example:
            >>> loop = ErrorFeedbackLoop(max_retries=2)
            >>> validator = SchemaValidator()
            >>>
            >>> # Valid YAML - succeeds on first attempt
            >>> valid_yaml = "name: Test\\ntype: factor_graph\\n..."
            >>> success, config, history = loop.validate_and_retry(
            ...     valid_yaml,
            ...     validator,
            ...     lambda p: p
            ... )
            >>> assert success is True
            >>> assert config is not None
            >>> assert len(history) == 0

            >>> # Invalid YAML - retries with feedback
            >>> invalid_yaml = "name: Test\\ntype: invalid"
            >>> def mock_llm(prompt):
            ...     return "name: Test\\ntype: factor_graph\\n..."
            >>> success, config, history = loop.validate_and_retry(
            ...     invalid_yaml,
            ...     validator,
            ...     mock_llm
            ... )
            >>> assert success is True  # Fixed on retry
            >>> assert len(history) >= 1  # Tracked first error
        """
        error_history = []
        current_yaml = yaml_str
        attempt_number = 0

        while attempt_number <= self.max_retries:
            attempt_number += 1

            try:
                # Parse YAML
                yaml_dict = yaml.safe_load(current_yaml)

                # Validate using SchemaValidator
                validation_errors = validator.validate(yaml_dict)

                if not validation_errors:
                    # Success! Return validated config
                    return True, yaml_dict, error_history

                # Validation failed - record errors
                error_summary = self._format_error_summary(validation_errors)
                error_history.append(error_summary)

                # Check if we've exceeded max retries
                if attempt_number > self.max_retries:
                    # Max retries exceeded
                    return False, None, error_history

                # Generate retry prompt
                retry_prompt = generate_retry_prompt(
                    current_yaml,
                    validation_errors,
                    attempt_number
                )

                # Call LLM to get corrected YAML
                current_yaml = llm_generate_func(retry_prompt)

            except yaml.YAMLError as e:
                # YAML parse error
                error_summary = f"YAML parse error: {str(e)}"
                error_history.append(error_summary)

                # Check if we've exceeded max retries
                if attempt_number > self.max_retries:
                    return False, None, error_history

                # Create synthetic validation error for YAML parse failure
                parse_error = ValidationError(
                    severity=ValidationSeverity.ERROR,
                    message=f"Invalid YAML syntax: {str(e)}",
                    field_path="<yaml>",
                    suggestion="Ensure proper YAML indentation and syntax"
                )

                # Generate retry prompt
                retry_prompt = generate_retry_prompt(
                    current_yaml,
                    [parse_error],
                    attempt_number
                )

                # Call LLM to get corrected YAML
                current_yaml = llm_generate_func(retry_prompt)

            except Exception as e:
                # Unexpected error - fail immediately
                error_summary = f"Unexpected error: {str(e)}"
                error_history.append(error_summary)
                return False, None, error_history

        # Should not reach here, but safety fallback
        return False, None, error_history

    def _format_error_summary(self, errors: List[ValidationError]) -> str:
        """
        Format validation errors into concise summary for error history.

        Args:
            errors: List of ValidationError objects

        Returns:
            Concise error summary string
        """
        if not errors:
            return "No errors"

        # Count by severity
        error_count = sum(1 for e in errors if e.severity == ValidationSeverity.ERROR)
        warning_count = sum(1 for e in errors if e.severity == ValidationSeverity.WARNING)

        # Get first error message as summary
        first_error = next((e for e in errors if e.severity == ValidationSeverity.ERROR), errors[0])
        summary = f"{error_count} error(s), {warning_count} warning(s): {first_error.message}"

        return summary
